[
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "I am working on Hyper-heuristics for Combinatorial Operation (CO) problem.\nIn this conversation, I will introduce the problem and then framework we have built now, you just remember this.\nIn next conversation, I will describe the challenges I'm encountering and explore how we can collaborate to resolve them.\n\nCurrently, I am working on max_cut problem:\nMaxCut is a classical combinatorial optimization problem where the goal is to partition the vertices of a graph into two disjoint subsets such that the number (or total weight) of edges between the two sets is maximized.\n\nTo support different heuristic algorithms, I build the Solution and Operator framework.\nThe Solution is designed as:\nclass Solution:\n    \"\"\"The solution for the MaxCut problem.\nTwo sets of vertices representing the partition of the graph into two subsets.\"\"\"\n    def __init__(self, set_a: set[int], set_b: set[int]):\n        self.set_a = set_a\n        self.set_b = set_b\nOperator servers as a mechanism to modify solution, which enables the application of heuristic algorithms.\u00a0\nTo support heuristic algorithm, we have build the following operators:\nclass InsertNodeOperator(BaseOperator):\n    \"\"\"Insert a node into one of the sets for the MaxCut solution.\"\"\"\n    def __init__(self, node: int, target_set: str):\n        self.node = node\n        self.target_set = target_set\n        assert target_set in ['A', 'B']\n    def run(self, solution: Solution) -> Solution:\n        new_set_a = set(solution.set_a)\n        new_set_b = set(solution.set_b)\n        if self.target_set == 'A':\n            assert self.node not in solution.set_b\n            new_set_a.add(self.node)\n        elif self.target_set == 'B':\n            assert self.node not in solution.set_a\n            new_set_b.add(self.node)\n        return Solution(new_set_a, new_set_b)\nclass InsertEdgeOperator(BaseOperator):\n    \"\"\"Insert an edge into the MaxCut solution with node_1 in set A and node_2 in set B.\"\"\"\n    def __init__(self, node_1: int, node_2: int):\n        self.node_1 = node_1\n        self.node_2 = node_2\n    def run(self, solution: Solution) -> Solution:\n        new_set_a = set(solution.set_a)\n        new_set_b = set(solution.set_b)\n        assert self.node_1 not in solution.set_b\n        assert self.node_2 not in solution.set_a\n        new_set_a.add(self.node_1)\n        new_set_b.add(self.node_2)\n        return Solution(new_set_a, new_set_b)\nclass SwapOperator(BaseOperator):\n    \"\"\"Swap a list of nodes from origin set to the opposite set in the MaxCut solution.\"\"\"\n    def __init__(self, nodes: list[int]):\n        self.nodes = nodes\n    def run(self, solution: Solution) -> Solution:\n        new_set_a = set(solution.set_a)\n        new_set_b = set(solution.set_b)\n        for node in self.nodes:\n            if node in solution.set_a:\n                assert node not in solution.set_b\n                new_set_a.remove(node)\n                new_set_b.add(node)\n            elif node in solution.set_b:\n                assert node not in solution.set_a\n                new_set_b.remove(node)\n                new_set_a.add(node)\n        return Solution(new_set_a, new_set_b)\nclass DeleteOperator(BaseOperator):\n    \"\"\"Delete a node from both sets in the MaxCut solution.\"\"\"\n    def __init__(self, node: int):\n        self.node = node\n    def run(self, solution: Solution) -> Solution:\n        new_set_a = set(solution.set_a)\n        new_set_b = set(solution.set_b)\n        if self.node in solution.set_a:\n            new_set_a.remove(self.node)\n        elif self.node in solution.set_b:\n            new_set_b.remove(self.node)\n        return Solution(new_set_a, new_set_b)\n\nIn pursuit of augmenting our heuristic algorithmic suite, we require the following standardized heuristic function signature:\ndef heuristic(global_data: dict, state_data: dict, algorithm_data: dict, get_state_data_function: call, **kwargs) -> tuple[TargetOperatorType, dict]:\nThe inputs are:\nglobal_data (dict): The global data dict containing the global instance data with:\n    - \"node_num\" (int): The total number of vertices in the graph.\n    - \"weight_matrix\" (numpy.ndarray): A 2D array representing the weight between nodes.\nstate_data (dict): The state data dict containing the solution state data with:\n    - \"current_solution\" (Solution): An instance of the Solution class representing the current solution.\n    - \"set_a_count\" (int): The number of nodes in set A of the current partition.\n    - \"set_b_count\" (int): The number of nodes in set B of the current partition.\n    - \"selected_nodes\" (set[int]): The set of selected nodes.\n    - \"selected_num\" (int): The number of nodes have been selected.\n    - \"unselected_nodes\" (set[int]): The set of unselected nodes.\n    - \"unselected_num\" (int): The number of nodes have not been selected.\n    - \"current_cut_value\" (int or float): The total weight of edges between set A and set B in the current solution.\n    - \"validation_solution\" (callable): def validation_solution(solution: Solution) -> bool: function to check whether new solution is valid.\nalgorithm_data(dict): Algorithm data contains the data that necessary for some algorithms.\nget_state_data_function(callable): The get_state_data_function is the function that receives the new solution as input and return the state dictionary for new solution. It will not modify the origin solution.\nOther hyper-parameters in kwargs.\nThe outputs includes the operator that must be an instance of a predefined target operator type and updated algorithm dict, which contains new information for future work for both this or other algorithm.\n\nPlease consider is max_cut a combinatorial operation problem and can be solved by heuristics?\nIf you think this problem is not a combinatorial operation problem or the solution, operator, data are incorrect, response ***is_cop:no*** to me, or response ***is_cop:yes***.\n\nThe response format is very important. For better communication, please respond to me in this format:\n***is_cop:yes/no***\nEnsure there is no other content inside the ***, and analysis outside *** is welcome.\nIf you have no information to provide, simply respond with ***None***."
            }
        ]
    },
    {
        "role": "assistant",
        "content": [
            {
                "type": "text",
                "text": "***is_cop:yes***"
            }
        ]
    },
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "In this instance, I have developed a heuristic function, although its performance has not reached a satisfactory level. My goal is to learn from case studies to improve and optimize this heuristic. To achieve this, I will provide the following:\n1. The heuristic function code.\n2. Test data for evaluation.\n3. Negative solution from heuristic function.\n4. Positive solution from external, which is better than negative solution.\n\nThe function most_weight_neighbors_d31b is the heuristic function:\nfrom src.problems.max_cut.components import Solution, InsertNodeOperator, SwapOperator\nimport numpy as np\n\ndef most_weight_neighbors_d31b(global_data: dict, state_data: dict, algorithm_data: dict, get_state_data_function: callable, scaling_factor: float = 0.5, swap_frequency: int = 5, **kwargs) -> tuple[InsertNodeOperator, dict]:\n    \"\"\"The most_weight_neighbors_d31b heuristic selects an unselected node based on both immediate and future impacts \n    on the cut value and periodically considers swaps to further improve the solution.\n\n    Args:\n        global_data (dict): The global data dict containing the following items:\n            - \"weight_matrix\" (numpy.ndarray): A 2D array representing the weight between nodes.\n        state_data (dict): The state dictionary containing the current state information:\n            - \"current_solution\" (Solution): The current partition of the graph into sets A and B.\n            - \"unselected_nodes\" (set[int]): The set of nodes that have not yet been selected.\n            - \"current_cut_value\" (float): The current cut value of the solution.\n        algorithm_data (dict): The algorithm dictionary for the current algorithm only. In this algorithm, the following items are necessary:\n            - \"sorted_nodes\" (list of tuples): A sorted list of (node, future_impact) in descending order.\n            - \"operation_count\" (int): The number of operations performed so far.\n        get_state_data_function (callable): The function receives the new solution as input and returns the state dictionary for the new solution. It does not modify the original solution.\n        scaling_factor (float, optional): A hyperparameter to scale the future impact of nodes. Defaults to 0.5.\n        swap_frequency (int, optional): Frequency (in terms of operations) at which swap operations are considered. Defaults to 5.\n\n    Returns:\n        InsertNodeOperator or SwapOperator: The operator to modify the solution.\n        dict: Updated algorithm data with the sorted list of nodes and operation count.\n    \"\"\"\n\n    # Extract necessary data\n    weight_matrix = global_data[\"weight_matrix\"]\n    current_solution = state_data[\"current_solution\"]\n    unselected_nodes = state_data[\"unselected_nodes\"]\n    set_a, set_b = current_solution.set_a, current_solution.set_b\n    operation_count = algorithm_data.get(\"operation_count\", 0)\n\n    # Step 1: Perform a swap operation periodically\n    if operation_count % swap_frequency == 0 and set_a and set_b:\n        weight_to_a = weight_matrix[:, list(set_a)].sum(axis=1)\n        weight_to_b = weight_matrix[:, list(set_b)].sum(axis=1)\n        best_increase = float(\"-inf\")\n        best_pair = None\n\n        # Evaluate all possible swaps\n        for i in set_a:\n            for j in set_b:\n                delta = weight_to_a[i] - weight_to_a[j] + weight_to_b[j] - weight_to_b[i]\n                if weight_matrix[i, j] != 0:  # Adjust for the edge between i and j if it exists\n                    delta += 2 * weight_matrix[i, j]\n                if delta > best_increase:\n                    best_increase = delta\n                    best_pair = (i, j)\n\n        # If a beneficial swap is found, return the SwapOperator\n        if best_pair:\n            return SwapOperator(nodes=list(best_pair)), {\"operation_count\": operation_count + 1}\n\n    # Step 2: Sort unselected nodes based on future impact if not already sorted\n    if \"sorted_nodes\" not in algorithm_data or not algorithm_data[\"sorted_nodes\"]:\n        # Calculate the future impact for each unselected node\n        sorted_nodes = sorted(\n            [(node, sum(abs(weight_matrix[node][other]) for other in range(len(weight_matrix)))) for node in unselected_nodes],\n            key=lambda x: x[1],\n            reverse=True\n        )\n        algorithm_data[\"sorted_nodes\"] = sorted_nodes\n    else:\n        # Filter out nodes that have been selected since the last run\n        sorted_nodes = [\n            (node, future_impact) for (node, future_impact) in algorithm_data[\"sorted_nodes\"]\n            if node in unselected_nodes\n        ]\n\n    # Step 3: Select the best unselected node based on both immediate and future impacts\n    if not sorted_nodes:\n        return None, {}\n\n    # Extract the best node and its future impact\n    best_node, future_impact = sorted_nodes.pop(0)\n\n    # Calculate the potential increase in cut value for adding the node to each set\n    potential_increase_a = sum(weight_matrix[best_node][other] for other in set_b)\n    potential_increase_b = sum(weight_matrix[best_node][other] for other in set_a)\n\n    # Adjust the potential increases by adding the scaled future impact\n    adjusted_increase_a = potential_increase_a + scaling_factor * future_impact\n    adjusted_increase_b = potential_increase_b + scaling_factor * future_impact\n\n    # Choose the set that gives the maximum adjusted increase in cut value\n    target_set = \"A\" if adjusted_increase_a >= adjusted_increase_b else \"B\"\n\n    # Create the operator to insert the selected node into the chosen set\n    operator = InsertNodeOperator(best_node, target_set)\n\n    # Return the operator and the updated algorithm data\n    return operator, {\"sorted_nodes\": sorted_nodes, \"operation_count\": operation_count + 1}\n\nThe global data for this problem:\nnode_num:5\nweight_matrix:[[0.0, 0.0, 0.0, -1.0, 0.0], [0.0, 0.0, -1.0, 1.0, 1.0], [0.0, -1.0, 0.0, -1.0, 1.0], [-1.0, 1.0, -1.0, 0.0, -1.0], [0.0, 1.0, 1.0, -1.0, 0.0]]\nTo aid your analysis, I extracted the following features from this data:\naverage_node_degree:2.8\nedge_density:0.7\naverage_edge_weight:-0.14285714285714285\nmax_edge_weight:1.0\nmin_edge_weight:-1.0\nstandard_deviation_edge_weight:0.989743318610787\nweighted_degree_distribution:[-1.0, 1.0, -1.0, -2.0, 1.0]\n\nNegative solution from most_weight_neighbors_d31b:\nset_a: 0,1,2,3\nset_b: 4\nThe result is 1.0.\nThe trajectory to get this solution:\noperation_id\theuristic\toperator(parameter)\tsolution_after_operation\n0\tmost_weight_neighbors_d31b\tInsertNodeOperator(node=3, target_set='A')\tset_a: 3\\nset_b: \\n\n1\tmost_weight_neighbors_d31b\tInsertNodeOperator(node=1, target_set='B')\tset_a: 3\\nset_b: 1\\n\n2\tmost_weight_neighbors_d31b\tInsertNodeOperator(node=2, target_set='A')\tset_a: 2,3\\nset_b: 1\\n\n3\tmost_weight_neighbors_d31b\tInsertNodeOperator(node=4, target_set='A')\tset_a: 2,3,4\\nset_b: 1\\n\n4\tmost_weight_neighbors_d31b\tInsertNodeOperator(node=0, target_set='A')\tset_a: 0,2,3,4\\nset_b: 1\\n\n5\tmost_weight_neighbors_d31b\tSwapOperator(nodes=[4, 1])\tset_a: 0,1,2,3\\nset_b: 4\\n\n\nPositive solution from external:\nset_a: 0,3,4\nset_b: 1,2\nThe result is 2.0.\nThe trajectory to get this solution:\noperation_id\theuristic\toperator(parameter)\tsolution_after_operation\n0\tmost_weight_neighbors_d31b\tInsertNodeOperator(node=3, target_set='A')\tset_a: 3\\nset_b: \\n\n1\tmost_weight_neighbors_d31b\tInsertNodeOperator(node=1, target_set='B')\tset_a: 3\\nset_b: 1\\n\n2\trandom_5c59\tInsertNodeOperator(node=2, target_set='B')\tset_a: 3\\nset_b: 1,2\\n\n3\tmost_weight_neighbors_d31b\tInsertNodeOperator(node=4, target_set='A')\tset_a: 3,4\\nset_b: 1,2\\n\n4\tmost_weight_neighbors_d31b\tInsertNodeOperator(node=0, target_set='A')\tset_a: 0,3,4\\nset_b: 1,2\\n\n\nPlease based on the data and solution, help to pick out the bottleneck operations in negative solution, which may cause the poor performance\nPlease note:\n1. Some operations, although they appear different, are essentially the same.\n2. Some operations may lead to solutions that look different but are essentially the same.\n3. Some operations may cause changes to the solution but do not affect the final cost; these are not considered bottlenecks.\n4. When an operation A is performed poorly, leading to a series of subsequent operations experiencing issues, we consider the first operation A to be a bottleneck.\n5. If there is only one item, there is no need to force multiple answers.\n6. Try to propose a better operator than origin one and you can learn from positive solution or others.\n\nThe response format is very important. For better communication, please respond to me in this format:\n***bottleneck_operations:\nnegative_operator_id; proposed_better_operator; reason\nnegative_operator_id; proposed_better_operator; reason\n....\n***\nEnsure there is no other content inside the ***, and analysis outside *** is welcome.\nIf you have no information to provide, simply respond with ***None***."
            }
        ]
    },
    {
        "role": "assistant",
        "content": [
            {
                "type": "text",
                "text": "***bottleneck_operations:\n3; InsertNodeOperator(node=4, target_set='B'); The node 4 should have been placed in set B to increase the cut value, as seen in the positive solution.\n5; None; The swap operation did not improve the cut value as intended, indicating a prior misplacement of nodes.\n***"
            }
        ]
    },
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "Now focus on this operator InsertNodeOperator(node=4, target_set='A') in step 3.\n\nThe state before InsertNodeOperator(node=4, target_set='A') is:\ncurrent_solution:\nset_a: 2,3\nset_b: 1\n\nset_a_count:2\nset_b_count:1\nselected_nodes:{1, 2, 3}\nselected_num:3\nunselected_nodes:{0, 4}\nunselected_num:2\ncurrent_cut_value:0.0\nTo aid your analysis, I extracted the following features from this state:\naverage_node_degree:2.8\nedge_density:0.7\naverage_edge_weight:-0.14285714285714285\nmax_edge_weight:1.0\nmin_edge_weight:-1.0\nstandard_deviation_edge_weight:0.989743318610787\nweighted_degree_distribution:[-1.0, 1.0, -1.0, -2.0, 1.0]\n\nYou believe that  InsertNodeOperator(node=4, target_set='B') is better with reason  The node 4 should have been placed in set B to increase the cut value, as seen in the positive solution.. So we hope to update most_weight_neighbors_d31b to output  InsertNodeOperator(node=4, target_set='B') in this state.\nTry to provide suggestion to update the code using mathematical analysis.\n\nPlease provide suggestion in following one or more aspects:\n1. Set the hyper-parameters or update the hyper-parameters' value for better control.\n2. Add more logic to this function for better control.\n3. Limit the application scope of certain logic with an if-check.\n4. Consider providing different output operators, such as operation type, operation subject, operation location, operation time, etc.\n5. Also consider learning from other heuristics:\nfrom src.problems.max_cut.components import *\nimport numpy as np\n\ndef highest_weight_edge_ca02(global_data: dict, state_data: dict, algorithm_data: dict, get_state_data_function: callable, **kwargs) -> tuple[InsertNodeOperator, dict]:\n    \"\"\" Heuristic algorithm to maximize cut value by evaluating the impact of placing each unselected node into set A or B.\n\n    Args:\n        global_data (dict): The global data dict containing the global data. In this algorithm, the following items are necessary:\n            - \"weight_matrix\" (numpy.ndarray): A 2D array representing the weight between nodes.\n        state_data (dict): The state dictionary containing the current state information. In this algorithm, the following items are necessary:\n            - \"current_solution\" (Solution): The current solution of the MaxCut problem.\n            - \"unselected_nodes\" (set[int]): The set of unselected nodes.\n            - \"set_a\" (set[int]): The set of nodes in partition A.\n            - \"set_b\" (set[int]): The set of nodes in partition B.\n        algorithm_data (dict): Not used in this algorithm.\n        get_state_data_function (callable): Not used in this algorithm.\n\n    Kwargs:\n        k (int, optional): Frequency of applying swap operations. Default is 3.\n\n    Returns:\n        InsertNodeOperator: An operator that places an unselected node into one of the sets (A or B) to maximize the cut value.\n        dict: Empty dictionary as no algorithm data is updated.\n    \"\"\"\n    # Extract necessary information\n    weight_matrix = global_data[\"weight_matrix\"]\n    current_solution = state_data[\"current_solution\"]\n    unselected_nodes = state_data[\"unselected_nodes\"]\n    set_a = state_data[\"current_solution\"].set_a\n    set_b = state_data[\"current_solution\"].set_b\n    k = kwargs.get(\"k\", 3)  # Frequency of swap operations, default is 3\n    \n    # If there are no unselected nodes, return None\n    if not unselected_nodes:\n        return None, {}\n\n    # Initialize variables for the best node and its placement\n    best_node = None\n    best_target_set = None\n    max_delta = float('-inf')\n\n    # Evaluate each unselected node\n    for node in unselected_nodes:\n        # Calculate delta for placing the node in set A\n        delta_a = sum(weight_matrix[node][other] for other in set_b) - sum(weight_matrix[node][other] for other in set_a)\n\n        # Calculate delta for placing the node in set B\n        delta_b = sum(weight_matrix[node][other] for other in set_a) - sum(weight_matrix[node][other] for other in set_b)\n\n        # Determine the best placement for the current node\n        if delta_b > delta_a and delta_b > max_delta:\n            best_node = node\n            best_target_set = 'B'\n            max_delta = delta_b\n        elif delta_a >= delta_b and delta_a > max_delta:\n            best_node = node\n            best_target_set = 'A'\n            max_delta = delta_a\n\n    # If no valid node is found, return None\n    if best_node is None:\n        return None, {}\n\n    # Perform swap operation periodically (every k operations)\n    if len(set_a) + len(set_b) % k == 0:\n        best_swap_delta = float('-inf')\n        best_swap_pair = None\n\n        # Precompute weights to sets\n        weight_to_a = weight_matrix[:, list(set_a)].sum(axis=1)\n        weight_to_b = weight_matrix[:, list(set_b)].sum(axis=1)\n\n        # Evaluate all possible swaps\n        for i in set_a:\n            for j in set_b:\n                # Calculate delta for swapping nodes i and j\n                delta = weight_to_a[i] - weight_to_a[j] + weight_to_b[j] - weight_to_b[i]\n                if weight_matrix[i, j] != 0:\n                    delta += 2 * weight_matrix[i, j]\n\n                # Check if the swap improves the cut value\n                if delta > best_swap_delta:\n                    best_swap_delta = delta\n                    best_swap_pair = (i, j)\n\n        # If a beneficial swap is found, execute it\n        if best_swap_pair and best_swap_delta > 0:\n            i, j = best_swap_pair\n            return SwapOperator(nodes=[i, j]), {}\n\n    # Return the best node insertion operator\n    return InsertNodeOperator(node=best_node, target_set=best_target_set), {}\nfrom src.problems.max_cut.components import Solution, InsertNodeOperator, SwapOperator\nimport numpy as np\n\ndef most_weight_neighbors_d31b(global_data: dict, state_data: dict, algorithm_data: dict, get_state_data_function: callable, scaling_factor: float = 0.5, swap_frequency: int = 5, **kwargs) -> tuple[InsertNodeOperator, dict]:\n    \"\"\"The most_weight_neighbors_d31b heuristic selects an unselected node based on both immediate and future impacts \n    on the cut value and periodically considers swaps to further improve the solution.\n\n    Args:\n        global_data (dict): The global data dict containing the following items:\n            - \"weight_matrix\" (numpy.ndarray): A 2D array representing the weight between nodes.\n        state_data (dict): The state dictionary containing the current state information:\n            - \"current_solution\" (Solution): The current partition of the graph into sets A and B.\n            - \"unselected_nodes\" (set[int]): The set of nodes that have not yet been selected.\n            - \"current_cut_value\" (float): The current cut value of the solution.\n        algorithm_data (dict): The algorithm dictionary for the current algorithm only. In this algorithm, the following items are necessary:\n            - \"sorted_nodes\" (list of tuples): A sorted list of (node, future_impact) in descending order.\n            - \"operation_count\" (int): The number of operations performed so far.\n        get_state_data_function (callable): The function receives the new solution as input and returns the state dictionary for the new solution. It does not modify the original solution.\n        scaling_factor (float, optional): A hyperparameter to scale the future impact of nodes. Defaults to 0.5.\n        swap_frequency (int, optional): Frequency (in terms of operations) at which swap operations are considered. Defaults to 5.\n\n    Returns:\n        InsertNodeOperator or SwapOperator: The operator to modify the solution.\n        dict: Updated algorithm data with the sorted list of nodes and operation count.\n    \"\"\"\n\n    # Extract necessary data\n    weight_matrix = global_data[\"weight_matrix\"]\n    current_solution = state_data[\"current_solution\"]\n    unselected_nodes = state_data[\"unselected_nodes\"]\n    set_a, set_b = current_solution.set_a, current_solution.set_b\n    operation_count = algorithm_data.get(\"operation_count\", 0)\n\n    # Step 1: Perform a swap operation periodically\n    if operation_count % swap_frequency == 0 and set_a and set_b:\n        weight_to_a = weight_matrix[:, list(set_a)].sum(axis=1)\n        weight_to_b = weight_matrix[:, list(set_b)].sum(axis=1)\n        best_increase = float(\"-inf\")\n        best_pair = None\n\n        # Evaluate all possible swaps\n        for i in set_a:\n            for j in set_b:\n                delta = weight_to_a[i] - weight_to_a[j] + weight_to_b[j] - weight_to_b[i]\n                if weight_matrix[i, j] != 0:  # Adjust for the edge between i and j if it exists\n                    delta += 2 * weight_matrix[i, j]\n                if delta > best_increase:\n                    best_increase = delta\n                    best_pair = (i, j)\n\n        # If a beneficial swap is found, return the SwapOperator\n        if best_pair:\n            return SwapOperator(nodes=list(best_pair)), {\"operation_count\": operation_count + 1}\n\n    # Step 2: Sort unselected nodes based on future impact if not already sorted\n    if \"sorted_nodes\" not in algorithm_data or not algorithm_data[\"sorted_nodes\"]:\n        # Calculate the future impact for each unselected node\n        sorted_nodes = sorted(\n            [(node, sum(abs(weight_matrix[node][other]) for other in range(len(weight_matrix)))) for node in unselected_nodes],\n            key=lambda x: x[1],\n            reverse=True\n        )\n        algorithm_data[\"sorted_nodes\"] = sorted_nodes\n    else:\n        # Filter out nodes that have been selected since the last run\n        sorted_nodes = [\n            (node, future_impact) for (node, future_impact) in algorithm_data[\"sorted_nodes\"]\n            if node in unselected_nodes\n        ]\n\n    # Step 3: Select the best unselected node based on both immediate and future impacts\n    if not sorted_nodes:\n        return None, {}\n\n    # Extract the best node and its future impact\n    best_node, future_impact = sorted_nodes.pop(0)\n\n    # Calculate the potential increase in cut value for adding the node to each set\n    potential_increase_a = sum(weight_matrix[best_node][other] for other in set_b)\n    potential_increase_b = sum(weight_matrix[best_node][other] for other in set_a)\n\n    # Adjust the potential increases by adding the scaled future impact\n    adjusted_increase_a = potential_increase_a + scaling_factor * future_impact\n    adjusted_increase_b = potential_increase_b + scaling_factor * future_impact\n\n    # Choose the set that gives the maximum adjusted increase in cut value\n    target_set = \"A\" if adjusted_increase_a >= adjusted_increase_b else \"B\"\n\n    # Create the operator to insert the selected node into the chosen set\n    operator = InsertNodeOperator(best_node, target_set)\n\n    # Return the operator and the updated algorithm data\n    return operator, {\"sorted_nodes\": sorted_nodes, \"operation_count\": operation_count + 1}\nfrom src.problems.max_cut.components import Solution, InsertNodeOperator, SwapOperator\nimport numpy as np\n\ndef balanced_cut_c0e6(global_data: dict, state_data: dict, algorithm_data: dict, get_state_data_function: callable, **kwargs) -> tuple[InsertNodeOperator, dict]:\n    \"\"\"Balanced Cut heuristic for the Max Cut problem with improvements to ensure balanced partitions, \n    future impact consideration, and periodic swap operations.\n\n    Args:\n        global_data (dict): The global data dict containing the following items:\n            - weight_matrix (numpy.ndarray): A 2D array representing the weight between nodes.\n        state_data (dict): The state dictionary containing the current state information:\n            - current_solution (Solution): The current partition of the graph into sets A and B.\n            - unselected_nodes (set[int]): The set of nodes that have not yet been selected.\n            - set_a (set[int]): The set of nodes in partition A.\n            - set_b (set[int]): The set of nodes in partition B.\n            - current_cut_value (float): The current cut value of the solution.\n        algorithm_data (dict): The algorithm dictionary for the current algorithm only. In this algorithm, the following items are necessary:\n            - operation_count (int): The number of operations performed so far. Default is 0.\n            - sorted_nodes (list of tuples): A sorted list of (node, future_impact) in descending order. Default is empty.\n        get_state_data_function (callable): The function receives the new solution as input and returns the state dictionary for the new solution. It does not modify the original solution.\n        kwargs (dict): Hyperparameters for the heuristic:\n            - scaling_factor (float, optional): A hyperparameter to scale the future impact of nodes. Defaults to 0.5.\n            - swap_frequency (int, optional): Frequency (in terms of operations) at which swap operations are considered. Defaults to 5.\n\n    Returns:\n        InsertNodeOperator or SwapOperator: The operator to modify the solution.\n        dict: Updated algorithm data with the sorted list of nodes and operation count.\n    \"\"\"\n    # Extract necessary data\n    weight_matrix = global_data[\"weight_matrix\"]\n    current_solution = state_data[\"current_solution\"]\n    unselected_nodes = state_data[\"unselected_nodes\"]\n    set_a, set_b = current_solution.set_a, current_solution.set_b\n    operation_count = algorithm_data.get(\"operation_count\", 0)\n    sorted_nodes = algorithm_data.get(\"sorted_nodes\", [])\n\n    # Hyperparameters\n    scaling_factor = kwargs.get(\"scaling_factor\", 0.5)\n    swap_frequency = kwargs.get(\"swap_frequency\", 5)\n\n    # Step 1: Perform a swap operation periodically\n    if operation_count % swap_frequency == 0 and set_a and set_b:\n        weight_to_a = weight_matrix[:, list(set_a)].sum(axis=1)\n        weight_to_b = weight_matrix[:, list(set_b)].sum(axis=1)\n        best_increase = float(\"-inf\")\n        best_pair = None\n\n        # Evaluate all possible swaps\n        for i in set_a:\n            for j in set_b:\n                delta = weight_to_a[i] - weight_to_a[j] + weight_to_b[j] - weight_to_b[i]\n                if weight_matrix[i, j] != 0:  # Adjust for the edge between i and j if it exists\n                    delta += 2 * weight_matrix[i, j]\n                if delta > best_increase:\n                    best_increase = delta\n                    best_pair = (i, j)\n\n        # If a beneficial swap is found, return the SwapOperator\n        if best_pair:\n            return SwapOperator(nodes=list(best_pair)), {\"operation_count\": operation_count + 1}\n\n    # Step 2: Sort unselected nodes based on future impact if not already sorted\n    if not sorted_nodes:\n        # Calculate the future impact for each unselected node\n        sorted_nodes = sorted(\n            [(node, sum(abs(weight_matrix[node][other]) for other in range(len(weight_matrix)))) for node in unselected_nodes],\n            key=lambda x: x[1],\n            reverse=True\n        )\n    else:\n        # Filter out nodes that have been selected since the last run\n        sorted_nodes = [\n            (node, future_impact) for (node, future_impact) in sorted_nodes\n            if node in unselected_nodes\n        ]\n\n    # Step 3: Select the best unselected node based on both immediate and future impacts\n    if not sorted_nodes:\n        return None, {}\n\n    # Extract the best node and its future impact\n    best_node, future_impact = sorted_nodes.pop(0)\n\n    # Calculate the potential increase in cut value for adding the node to each set\n    potential_increase_a = sum(weight_matrix[best_node][other] for other in set_b)\n    potential_increase_b = sum(weight_matrix[best_node][other] for other in set_a)\n\n    # Adjust the potential increases by adding the scaled future impact\n    adjusted_increase_a = potential_increase_a + scaling_factor * future_impact\n    adjusted_increase_b = potential_increase_b + scaling_factor * future_impact\n\n    # Choose the set that gives the maximum adjusted increase in cut value\n    target_set = \"A\" if adjusted_increase_a >= adjusted_increase_b else \"B\"\n\n    # Create the operator to insert the selected node into the chosen set\n    operator = InsertNodeOperator(best_node, target_set)\n\n    # Return the operator and the updated algorithm data\n    return operator, {\"sorted_nodes\": sorted_nodes, \"operation_count\": operation_count + 1}\n\nTo make sure the suggestion is useful, please check in following aspects:\n1. Is this suggestion clear and achievable? For example, choosing operation A brings greater benefits in the form of rebates is not a clear and achievable suggestion because we do not know how to measure future benefits.\n2. Will the updated suggestion make the function output  InsertNodeOperator(node=4, target_set='B') in this step?\n3. If we run the updated function on this data from beginning to end, will the result be better than set_a: 0,3,4\nset_b: 1,2 (2.0)?\n4. Is this a commonly useful update for all instances or states?\n\nThe response format is very important. For better communication, please respond to me in this format:\n***suggestion: xxx\n***\nEnsure there is no other content inside the ***, and analysis outside *** is welcome.\nIf you have no information to provide, simply respond with ***None***."
            }
        ]
    },
    {
        "role": "assistant",
        "content": [
            {
                "type": "text",
                "text": "***suggestion:\n1. Adjust the scaling_factor to a higher value, such as 1.0, to give more weight to future impacts, potentially improving decision-making for InsertNodeOperator.\n2. Add logic to consider negative weights more critically, potentially through an additional condition that prioritizes nodes that balance negative and positive impacts.\n3. Introduce an if-check to ensure nodes with negative weighted degrees are directed to the set that minimizes the negative impact.\n4. Incorporate logic from highest_weight_edge_ca02 to evaluate the potential delta for each node and choose based on the highest positive impact to the cut value.\n***"
            }
        ]
    },
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "Based on previous discuss, we get the heuristic most_weight_neighbors:\nNow, based on these suggestions:\n1. Adjust the scaling_factor to a higher value, such as 1.0, to give more weight to future impacts, potentially improving decision-making for InsertNodeOperator.\n2. Add logic to consider negative weights more critically, potentially through an additional condition that prioritizes nodes that balance negative and positive impacts.\n3. Introduce an if-check to ensure nodes with negative weighted degrees are directed to the set that minimizes the negative impact.\n4. Incorporate logic from highest_weight_edge_ca02 to evaluate the potential delta for each node and choose based on the highest positive impact to the cut value.\nUpdate the most_weight_neighbors_d31b.\n\nNow please implement it in python function most_weight_neighbors_3e17.\n\nTo implement this heuristic function in python, please note:\n1. We can assume that Solution and all Operators are imported.\n2. The operator type must be defined previously, do not create a new one in your code.\n3. Never modify the data in global, state and algorithm data.\n4. All hyper parameters in kwargs should be set a default value.\n5. Any reasonable partial solution may be used as input, such as an empty solution.\n6. Comments in the code are very important. They must clearly explain which hyper-parameters are required by the algorithm (i.e., the parameters in **kwargs), how the algorithm proceeds, and under what circumstances it will not return any operator or will return an empty operator. We hope that people can understand the principles and workflow of the algorithm clearly just by reading the comments, without needing to look at the code.\n7. The name of function must be most_weight_neighbors_3e17. The TargetOperatorType should be replaced by real returned operator type.\n8. No any omissions or placeholders, I'm just going to use the code.\n9. For the algorithm to update the algorithm_data, do not modify directly \"algorithm_data[\"abc\"] = 123\", we should return operator, {\"abc\": 123}.For algorithm that do not update them, just return operator, {}\n10. For the circumstances that algorithm return empty operator, please return None, {}.\n11. Do not forget to check whether the content is empty when calculating the min/max/average/std/..., such as when using min([distance[node][other] for other in nodes if other != node]).\n12. Make the result must be valid.\n\nFor max_cut problem, take about following:\n1. Make the result must be valid, such as: no re-inserted nodes, never insert a node in set A, which are already in set B, etc.\n2. The graph is undirected, meaning each edge connecting two nodes is treated as a single, bidirectional edge.\n\n\nThe problem will work on following data.\nPlease try to understand the data and generate based on them, especially setting the default value for hyper-parameters:\nAll data is possible\n\nThe response format is very important. For better communication, please respond to me in this format:\n***python_code:\nfrom src.problems.max_cut.components import *\nimport other_library (if necessary)\ndef most_weight_neighbors_3e17(global_data: dict, state_data: dict, algorithm_data: dict, get_state_data_function: callable, **kwargs) -> tuple[TargetOperatorType, dict]:\n    \"\"\" Description for this heuristic algorithm.\n\n    Args:\n        global_data (dict): The global data dict containing the global data. In this algorithm, the following items are necessary:\n            - necessary_key (value type): description\n            - necessary_key (value type): description\n            ...\n        state_data (dict): The state dictionary containing the current state information. In this algorithm, the following items are necessary:\n            - necessary_key (value type): description\n            - necessary_key (value type): description\n            ...\n        (Optional and can be omitted if no algorithm data) algorithm_data (dict): The algorithm dictionary for current algorithm only. In this algorithm, the following items are necessary:\n            - necessary_key (value type): description\n            - necessary_key (value type): description\n            ...\n        (Optional and can be omitted if no used) get_state_data_function (callable): The function receives the new solution as input and return the state dictionary for new solution, and it will not modify the origin solution.\n        (Optional and can be omitted if no hyper parameters data) introduction for hyper parameters in kwargs if used.\n\n    Returns:\n        Description of this operator.\n        Description of updated algorithm data.\n    \"\"\"\n    xxx\nEnsure there is no other content inside the ***, and analysis outside *** is welcome.\nIf you have no information to provide, simply respond with ***None***.\n"
            }
        ]
    },
    {
        "role": "assistant",
        "content": [
            {
                "type": "text",
                "text": "***python_code:\nfrom src.problems.max_cut.components import Solution, InsertNodeOperator, SwapOperator\nimport numpy as np\n\ndef most_weight_neighbors_3e17(global_data: dict, state_data: dict, algorithm_data: dict, get_state_data_function: callable, scaling_factor: float = 1.0, swap_frequency: int = 5, **kwargs) -> tuple[InsertNodeOperator, dict]:\n    \"\"\" Heuristic algorithm to optimize the MaxCut problem by considering immediate and future impacts of nodes, handling negative weights, and periodic swap operations.\n\n    Args:\n        global_data (dict): The global data dict containing the global data. In this algorithm, the following items are necessary:\n            - \"weight_matrix\" (numpy.ndarray): A 2D array representing the weight between nodes.\n        state_data (dict): The state dictionary containing the current state information. In this algorithm, the following items are necessary:\n            - \"current_solution\" (Solution): The current solution of the MaxCut problem.\n            - \"unselected_nodes\" (set[int]): The set of unselected nodes.\n        algorithm_data (dict): The algorithm dictionary for the current algorithm only. In this algorithm, the following items are necessary:\n            - \"operation_count\" (int): The number of operations performed so far. Default is 0.\n            - \"sorted_nodes\" (list of tuples): A sorted list of (node, future_impact) in descending order. Default is empty.\n        get_state_data_function (callable): The function receives the new solution as input and returns the state dictionary for the new solution. It does not modify the original solution.\n        kwargs: \n            - scaling_factor (float, optional): A hyperparameter to scale the future impact of nodes. Defaults to 1.0.\n            - swap_frequency (int, optional): Frequency (in terms of operations) at which swap operations are considered. Defaults to 5.\n\n    Returns:\n        InsertNodeOperator or SwapOperator: The operator to modify the solution.\n        dict: Updated algorithm data with the sorted list of nodes and operation count.\n    \"\"\"\n    \n    # Extract necessary data\n    weight_matrix = global_data[\"weight_matrix\"]\n    current_solution = state_data[\"current_solution\"]\n    unselected_nodes = state_data[\"unselected_nodes\"]\n    set_a, set_b = current_solution.set_a, current_solution.set_b\n    operation_count = algorithm_data.get(\"operation_count\", 0)\n    sorted_nodes = algorithm_data.get(\"sorted_nodes\", [])\n\n    # Step 1: Perform a swap operation periodically\n    if operation_count % swap_frequency == 0 and set_a and set_b:\n        weight_to_a = weight_matrix[:, list(set_a)].sum(axis=1)\n        weight_to_b = weight_matrix[:, list(set_b)].sum(axis=1)\n        best_increase = float(\"-inf\")\n        best_pair = None\n\n        # Evaluate all possible swaps\n        for i in set_a:\n            for j in set_b:\n                delta = weight_to_a[i] - weight_to_a[j] + weight_to_b[j] - weight_to_b[i]\n                if weight_matrix[i, j] != 0:  # Adjust for the edge between i and j if it exists\n                    delta += 2 * weight_matrix[i, j]\n                if delta > best_increase:\n                    best_increase = delta\n                    best_pair = (i, j)\n\n        # If a beneficial swap is found, return the SwapOperator\n        if best_pair:\n            return SwapOperator(nodes=list(best_pair)), {\"operation_count\": operation_count + 1}\n\n    # Step 2: Sort unselected nodes based on future impact if not already sorted\n    if not sorted_nodes:\n        # Calculate the future impact for each unselected node\n        sorted_nodes = sorted(\n            [(node, sum(abs(weight_matrix[node][other]) for other in range(len(weight_matrix)))) for node in unselected_nodes],\n            key=lambda x: x[1],\n            reverse=True\n        )\n    else:\n        # Filter out nodes that have been selected since the last run\n        sorted_nodes = [\n            (node, future_impact) for (node, future_impact) in sorted_nodes\n            if node in unselected_nodes\n        ]\n\n    # Step 3: Select the best unselected node based on both immediate and future impacts\n    if not sorted_nodes:\n        return None, {}\n\n    # Extract the best node and its future impact\n    best_node, future_impact = sorted_nodes.pop(0)\n\n    # Calculate the potential increase in cut value for adding the node to each set\n    potential_increase_a = sum(weight_matrix[best_node][other] for other in set_b)\n    potential_increase_b = sum(weight_matrix[best_node][other] for other in set_a)\n\n    # Adjust the potential increases by adding the scaled future impact\n    adjusted_increase_a = potential_increase_a + scaling_factor * future_impact\n    adjusted_increase_b = potential_increase_b + scaling_factor * future_impact\n\n    # Consider negative weights critically and choose the set that minimizes negative impact\n    if sum(weight_matrix[best_node]) < 0:\n        target_set = \"B\" if adjusted_increase_b >= adjusted_increase_a else \"A\"\n    else:\n        # Choose the set that gives the maximum adjusted increase in cut value\n        target_set = \"A\" if adjusted_increase_a >= adjusted_increase_b else \"B\"\n\n    # Create the operator to insert the selected node into the chosen set\n    operator = InsertNodeOperator(best_node, target_set)\n\n    # Return the operator and the updated algorithm data\n    return operator, {\"sorted_nodes\": sorted_nodes, \"operation_count\": operation_count + 1}\n***"
            }
        ]
    },
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "To ensure the stable of heuristics, we adjust the code to:\nfrom src.problems.max_cut.components import Solution, InsertNodeOperator, SwapOperator\nimport numpy as np\n\ndef most_weight_neighbors_3e17(global_data: dict, state_data: dict, algorithm_data: dict, get_state_data_function: callable, scaling_factor: float = 1.0, swap_frequency: int = 5, **kwargs) -> tuple[InsertNodeOperator, dict]:\n    \"\"\" Heuristic algorithm to optimize the MaxCut problem by considering immediate and future impacts of nodes, handling negative weights, and periodic swap operations.\n\n    Args:\n        global_data (dict): The global data dict containing the global data. In this algorithm, the following items are necessary:\n            - \"weight_matrix\" (numpy.ndarray): A 2D array representing the weight between nodes.\n        state_data (dict): The state dictionary containing the current state information. In this algorithm, the following items are necessary:\n            - \"current_solution\" (Solution): The current solution of the MaxCut problem.\n            - \"unselected_nodes\" (set[int]): The set of unselected nodes.\n        algorithm_data (dict): The algorithm dictionary for the current algorithm only. In this algorithm, the following items are necessary:\n            - \"operation_count\" (int): The number of operations performed so far. Default is 0.\n            - \"sorted_nodes\" (list of tuples): A sorted list of (node, future_impact) in descending order. Default is empty.\n        get_state_data_function (callable): The function receives the new solution as input and returns the state dictionary for the new solution. It does not modify the original solution.\n        kwargs: \n            - scaling_factor (float, optional): A hyperparameter to scale the future impact of nodes. Defaults to 1.0.\n            - swap_frequency (int, optional): Frequency (in terms of operations) at which swap operations are considered. Defaults to 5.\n\n    Returns:\n        InsertNodeOperator or SwapOperator: The operator to modify the solution.\n        dict: Updated algorithm data with the sorted list of nodes and operation count.\n    \"\"\"\n    \n    # Extract necessary data\n    weight_matrix = global_data[\"weight_matrix\"]\n    current_solution = state_data[\"current_solution\"]\n    unselected_nodes = state_data[\"unselected_nodes\"]\n    set_a, set_b = current_solution.set_a, current_solution.set_b\n    operation_count = algorithm_data.get(\"operation_count\", 0)\n    sorted_nodes = algorithm_data.get(\"sorted_nodes\", [])\n\n    # Step 1: Perform a swap operation periodically\n    if operation_count % swap_frequency == 0 and set_a and set_b:\n        weight_to_a = weight_matrix[:, list(set_a)].sum(axis=1)\n        weight_to_b = weight_matrix[:, list(set_b)].sum(axis=1)\n        best_increase = float(\"-inf\")\n        best_pair = None\n\n        # Evaluate all possible swaps\n        for i in set_a:\n            for j in set_b:\n                delta = weight_to_a[i] - weight_to_a[j] + weight_to_b[j] - weight_to_b[i]\n                if weight_matrix[i, j] != 0:  # Adjust for the edge between i and j if it exists\n                    delta += 2 * weight_matrix[i, j]\n                if delta > best_increase:\n                    best_increase = delta\n                    best_pair = (i, j)\n\n        # If a beneficial swap is found, return the SwapOperator\n        if best_pair and best_increase > 0:\n            return SwapOperator(nodes=list(best_pair)), {\"operation_count\": operation_count + 1}\n\n    # Step 2: Sort unselected nodes based on future impact if not already sorted\n    if not sorted_nodes:\n        # Calculate the future impact for each unselected node\n        sorted_nodes = sorted(\n            [(node, sum(abs(weight_matrix[node][other]) for other in range(len(weight_matrix)))) for node in unselected_nodes],\n            key=lambda x: x[1],\n            reverse=True\n        )\n    else:\n        # Filter out nodes that have been selected since the last run\n        sorted_nodes = [\n            (node, future_impact) for (node, future_impact) in sorted_nodes\n            if node in unselected_nodes\n        ]\n\n    # Step 3: Select the best unselected node based on both immediate and future impacts\n    if not sorted_nodes:\n        return None, {}\n\n    # Extract the best node and its future impact\n    best_node, future_impact = sorted_nodes.pop(0)\n\n    # Calculate the potential increase in cut value for adding the node to each set\n    potential_increase_a = sum(weight_matrix[best_node][other] for other in set_b)\n    potential_increase_b = sum(weight_matrix[best_node][other] for other in set_a)\n\n    # Adjust the potential increases by adding the scaled future impact\n    adjusted_increase_a = potential_increase_a + scaling_factor * future_impact\n    adjusted_increase_b = potential_increase_b + scaling_factor * future_impact\n\n    # Consider negative weights critically and choose the set that maximizes the cut value\n    if sum(weight_matrix[best_node]) < 0:\n        target_set = \"A\" if adjusted_increase_a > adjusted_increase_b else \"B\"\n    else:\n        # Choose the set that gives the maximum adjusted increase in cut value\n        target_set = \"A\" if adjusted_increase_a >= adjusted_increase_b else \"B\"\n\n    # Create the operator to insert the selected node into the chosen set\n    operator = InsertNodeOperator(best_node, target_set)\n\n    # Return the operator and the updated algorithm data\n    return operator, {\"sorted_nodes\": sorted_nodes, \"operation_count\": operation_count + 1}"
            }
        ]
    },
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "To assess the performance of the evolved heuristics, I have tested both the original heuristic most_weight_neighbors_d31b and the evolved heuristic most_weight_neighbors_3e17.\nThe following table presents a comparative analysis of these heuristics. The first 8 columns represent the features of the testing data, while the remaining columns show the results.\nA higher result indicates better performance, and the improvement ratio (in brackets, positive means to improve and negative means decline) reflects the enhancement relative to most_weight_neighbors_d31b.\n\ndata_name\taverage_node_degree\tedge_density\taverage_edge_weight\tmax_edge_weight\tmin_edge_weight\tstandard_deviation_edge_weight\tweighted_degree_distribution\tmost_weight_neighbors_d31b\tmost_weight_neighbors_3e17\ng9.mc\t47.94\t0.06\t-0.003337505214851898\t1.0\t-1.0\t0.9999944305139609\t\"[-10.  -3. -13.  -4.  -6.   2.  -9.   3.  -1.  -6.   4.   3.   1.   1.\n   5.  -7. -12.  -4. -16.   1.   8.  12. -11.   0.  -3.   3.  -4.  -2.\n  -2.   7.  -8.  11.  -6.  -4.   0.   5.  -2.  10.  -8.   3.   8.   4.\n   0.   8. -16. -13.  -4.  -5.  -5.  11.   1.   9.  -1.  -3. -11.   6.\n  -2.  10.  -4.  11.  -9.   4.   5.  13.   6. -18.  14.  -6.   7.  -2.\n   9.   1.  -6.   4.   3.  -1.   3.  -6.   2.   3. -12.  -8.  -8. -10.\n  -5.  -2.  13. -11.  -2.   1.   4. -12.   2. -10.   1.   7. -11.   6.\n  -4.   5.   0.  -3.  -7. -13.  -5.   1.   1.  14.   4.   3.   1.  -7.\n  -4.  -5.  -6.  -4.   0.  -6.   2.  -4.  -4.  13.   9.  -6.   3.   3.\n  -3.  -2.   7. -10. -10.   1.  -9.   1.  -1.  10.  -1.  -2.   3.  -6.\n   4.   1.   3.  -8.   4.   0.  -6.   2. -20.  19.   1.  -1.  -5.   6.\n   7.   2.   9.  11.  -3.   7.  -2.  -6.  -6.  -1.   4.   5.   5.   6.\n  -5.  -3.  -2.   0.  -6.   3.   9. -15.  -8.  15.  -5.   6.  -5.   4.\n   0.  -3.   6.  10.   3. -12.  -6.   6.   1.   1.  -9.  -3.  -3.   1.\n   3.  -2.   0.  -3.  -4.   1.  -4.   7. -11.   4. -10.   1.   0.  -6.\n  -1.  17. -11.  12.   5.  11.   1.  -5.   6. -13.   9.  -7.  -8.   5.\n  -2.   3.  -3.   1.   4.  -9.   5.   4.  -2. -13.  -2.  -6.  14.  -5.\n  -8.   1. -21.  -3.  -2.  -2.   2.  -5.   1.  -1.   5.  -9.  -1. -13.\n   1.  -5.   5.  -2.   0.   0.   9.  -9.   8.   8.  -1.   4.  -1. -13.\n  -1.   3.  -9.   0.  -2. -10. -11.  -1.   2.   9.  -1.  -2.   2.  11.\n   4.  -9.  10.  18.  -6.   2.  -3.  -2.  -6.  -4.  10.   1.   3.   0.\n   4.   2.   0.   2.  -9.  -1.   2.  -8.  -3. -12.   7.  -1.   1.   7.\n   5.   2.  -2.  -2.   2.  -4.  -4.   0.  -2.  -1.  -5.   1.  11.  10.\n   3.  -6.  -4.   3.  -3.  -4.   1.  -3.   0.  -5.  -4.  -4.  -6.  -9.\n  -5.  11.  -1.   7.   0.   7.   3.  -1.  -2.  -4.  -3.   1.  -3.   0.\n   5.   1.   5. -12.  -8.  12.   1.   3.   1.   2.  11.  -2.  10.   1.\n  -6.  -1.  16.  10.  -8.  10.  -3.  -4.  -2.  13.  -6.  -4.   1. -10.\n   2.   2.  -2.   3.  -9.   8.   0.   4.  -2.  -1.  -4.   5.   0.   8.\n  -7.   1. -11.   0.   2.   5.   1.   0.  -8.  -7.   1.  -6.   6. -15.\n  -1.  -1. -15.  12.  18.  -5.  -6.   3.  13. -12.   0.  13.   2.   7.\n  -8.  -4. -20.  -4.  -7.  -5.  17.  -8.   0.  -1.   2.  -3.  -5.  13.\n   0.  -5.  -3.  -1.   6.   2.   2.  -6.  -3.   5. -12.   7.   5.   9.\n   6.  -1.   3.   1. -12.  -8.  -8.  -1.  -2.  -6.  -4.  -5.  -3.   6.\n  -1.   7.  14.   2.  -2.   8.  -4.   7.   0.   1.  -6.   7.  -5.   3.\n -15.   7.   2.   2. -12. -11.   0.  -1.   7.   0.  -5.   7.  -3.  -2.\n   8.   3.   1.  -6.   2.   4.  17.   9.  -3.  11.  -6.  -1.  -3.  11.\n  -5.  -9.  -3.  -4.  -2.  -4.   9.   9.   1.  -3.   1.  -4. -10.  -3.\n -12.  -1.  -8.  -3.   6. -11.   3.  -3.  10.  -1.   2.   2.   0.  -9.\n  -6.   6.  -4. -10.  -1.   5.  -6.  -6.  -6.  -4. -10.   8.  -4.   6.\n   1.  -7.   0.  26.   6.  -7.  -3.   1.   0.  -3.   6.   1.  11.   7.\n  -2.   5.  -9.  -6.  13.  -2.   3.   3.   2.   0.  -2.   7.  14.   3.\n  11.  -1.  -1.  -4.  -6.  -1.   4.  -5.  -5.   1.   1.  -1.  10.   4.\n   4.   0.  -4. -10.   3.   2.  -4.  -2.   0.  17.   3. -15.   4.   6.\n   7.  -6.  -2.   2.  -5. -14.  -6.   1.   9.   2. -18.   6. -14.  -7.\n  11.  11.  -1. -13.  -1.  -4.   1.   7.  -4.   1.  -4.  -1.   4.   2.\n -11.  12.  -7.   5.  -3.  -6.  -6.   7.  -4.   1.  -3.  -7.  10.   8.\n   0.   4.  -5.   6.  -2.  10.  -8.  -1.   2.   2.  -2.  -7.  -1.   2.\n   5.   7.   3.  10.   4.  -3.  -7.   0.  16.  -6.  -6.  -4.  -2.  -6.\n   3.   9.  -8.  -8.  -3.  -4.   0.  -3.   0.   2.  -2.   6.  -4.  -3.\n   1.   7.  -2.   4.  -2.   0.  -6.   8.  -1.   2.  -1.  12.  14.  -6.\n  -1.  14.   7.  -5.   4.   3.  -3.   3.  -3.  -9.   4. -11.   5.  -3.\n   3.   0.  -7.  -4.  14.  -7.  -3.   3.  -3.   7.  -4. -11.  -3.   2.\n  -3.  -8.   2.  -1.  -9.   6.  -1.  -4.   3.  -7.  -5.  17.   1.  -2.\n  15.  -1.  -3.  -2. -10.  -4.   1.   9.   3.  -3.   5.   4.  -4.   0.\n  -2.   9.   2.  -1.   9. -13.  -2.   3.  -4.  -4.  -1.  -6.   2.   4.\n  20.  -1. -11.   8.   1.   6. -10.   5.  10.   3.  -1.   5.   1.   8.\n  -4.   1.  13.  -3.   6.  -3.   1.   9.  -4.   7.   0.  -5.  -6.  -3.\n   8.   9.]\"\t1859.0(0.0)\t1843.0(-0.01)\ng10.mc\t47.94\t0.06\t-0.008343763037129746\t1.0\t-1.0\t0.9999651902033291\t\"[  6.  -3.  -7.  -8.  12.  -3.  -1.   6.   2.   3.   1.  -9.   1.   7.\n -11.   2.   4.   6. -10.   5.  -3.   7.   7.   5.   5.   7.  -9.   4.\n   3.  -2.  -5.  -1.  10.  15.  -6.  -7.  -1.   1.  10.   3.   4. -10.\n  -1.  -2.   3.  -5.   9.   1.   4.  -1.  11.   4.  -3.   4.  -9.  -2.\n  -7.   0.   3.   5.  -6.  -4.   3.   7.  -2.  -7.   1.  -5.   3.   4.\n   9.  -1.  -2.  -1.  -4.  -4. -10.   8.   1.  -6.  -1.   0.   2. -13.\n  11.   8.  -2.   2. -11. -14.   0.   5.  -5.  -8.  -3.  -2.  10.  -1.\n   2.   1.  -4.  -7.  -8.  -5.  -5.   7.   5. -12.  12.   4.   1.  -8.\n   2.  -3.  -3.   5.  13.   2.   0.  13.   4. -14. -10.  -9.   0.   0.\n -12.  -6.  -9.  -9.  10.  -3.  -2. -16.  -5.   2.   8.   9.   4.  -5.\n   5.  16.   1. -11.   7.   0.  10.  -1.  16.   5.   6.   2.  -6.  -7.\n   6.  10.   2.   1. -11.   6.  -3.   6.   3.   5.  -2.   0. -12.  -3.\n -12.  -6.   5.   3.  -6.  -9.  -5.  -8.  -4.   2.   2.   4.  -2.   6.\n -16.  -1.   8.  -5.   2.  -4.   0.  -6.   5.  13. -11.   2.  -4.   3.\n  -2.  14.  -7.  -4.   3.   4.   7.   1.   2.  12.  -9.   7.  -4.  -5.\n  -9.   3. -12.   3.   0.   3.  -3.   1.  -7.  -5.   1.   4.  -5.   3.\n   2.  10.   3.   2.  -7.   7.  10.   2.  -3.   2.  -1.   2.  -5. -10.\n  -2.   4.  10.  18.   0.  -4.   4.   0.   0.   2.  -3.  -5.  -6.   0.\n  -1.   5.  16.  13.  -5. -13.   3.  -7.  -6.  -2.  -1. -10.  -5.  -8.\n  -8.  -3.  -1.  -7.   5.  -1.  -2.   2.   8.   3.   1.   1.  11.   4.\n  -4.   3.  11.  10.  17.   9.   6.   2.  -2.   3.   2.  -5.   4.  -2.\n  -5.  -3.   5.  -5.   3.  -2.  -3.  -2. -12.   5.  -1. -20.  -2.  -7.\n -19.  -6.  -3.  -8.  -8.  -1.   4.  -2. -11. -11. -10.   5.  -3.   3.\n   5.   2.   0.  -6.   4.   6.   4.  -3.  12.  11.  -6.   1.   8.   5.\n   3.   8.  -9.  -9.   2.  -8.   4.  -1.  -7.  -2.   2.  -9.  -7.   3.\n  11.  10.  -7.  -2.  -6.   0.  10.  -3.   1.   3.   4.  16. -12.   5.\n -13.  -6. -10.  -6.  -1.  -5. -11.  -7.   0.  -1. -11.  11.  -1.   6.\n   7. -18. -14.  -4.   6.   3.  -1.  -2.   1. -10.  -5.  -9.  -4.  12.\n   2.   0.   0.  -2.  18.   1.  -8.   2.  16.  -3.  -3.  -5.  10.  -9.\n   2.   0.   1.  -5.   4.  -1.  -9.  -4.  -7.  10.  15.   7.  -2.  -9.\n   5.   2.  -6.  -5. -12.  12.   9.   3.  -4.  -2.   1.   2. -10.   6.\n  -7.   9.   0.  -1.   0.  -4.   5.  -9.   2.  -8.   3.   3.  -8.   7.\n  -4.  -1.  -4.   0.   0.   5.  -4.  11.   4.   2. -15.   1.  -9.  -1.\n  -5.  11.   5.   1.  -6.  18.   1.  -6.  -6.   7.  -8.  -3.   3.  -1.\n  -1. -11.   1.  -4.  -7.  -3.  -6. -10.   6.   5.  11.  -7.  11.   2.\n  -9. -12.   0.  11.   3.   5.  13.   4.   6.   4.  -5.  -7.   1.  -7.\n  10.   0. -16.  -6.  -4.   2.   7.  -4. -10.   0. -10.   4.   5.  -6.\n   5.  -3.   3.   4.  -1.  -5.  10.  -3.  -3.  -5.  16.   4.   0.  -1.\n   7.   5. -10.  -3.  -2.  -4.   7.   3.   4.   0.   2.   0.   2.   9.\n   0.   4.  -3.  -7.  -2.   3.   3.  -4.  -2.  -4.   6.  -1.  -4.  -1.\n   5.  13.  -9.  -2.  16.  -5.  -3.   3.   4. -22.   6. -10.  -9.  -7.\n   4.  -6.   0.   4.  -8.  -7.  -5.   3.   4. -12.  -3.  -8.  -6.   8.\n  -9.   0.   5.   2. -12.   3.  -3.   2.  -4.  -5.  -9.  -1.   2.  16.\n -17.  -2.  -2.  -2.   4.   2.  -7.  -8.   7. -11. -10. -14.   4.   3.\n   2.   7.   0. -10.  -3.  -1.   7.  18. -13.   2.  -2.  -6.  -4.   3.\n  -3.   2.   3.  -1. -10.  -3.  -2. -15.   5.  -8.  -3. -10.   1.  -7.\n   0.   3.   3.   5.  -6.   6.  -2.   1.   2.  -4.   8.   4.  -3.   3.\n  -1.  -8.  -1.  -5.  -1.   7.   6.  11.   2.   3.  -5.  -3.  -1.  -2.\n  -1.  -5.  -4.  -1.   4.  10.  -6.  -2.  -7.  15.   0.  -5.  -6.  -6.\n   9.   0.  -7.   4.   5. -10.  10.   8.   6.   3.  -1.   3.  -1.  12.\n  -5.   4. -10.  12.  -2.  -1.   1.  -3.   5.  -4.  -1.  -2. -18.  -1.\n  -6.  -8.   2.   1.   3.   8.   3.  -7.  -4.  -3.  -6.   7.   2.   3.\n   2. -15.   5.  -1.   1.   5.  11.   2.  -1.   1.   0.   8. -17.   8.\n   7.  -3.  -7.   0.   5.   0.  -6.  -7.  -5. -13.  -5.  11.  -8.   0.\n  -2.  -9.  -7.  -9.   2.   4.  13.   7.   6.  -4.  10. -11.  -2. -12.\n  -4.   1.   9. -18.   0.   9.   4. -11.  -6.  -5.  -9.   2.  -6.  -2.\n   2.  -8.  14.  -1.  -1.   9.  -3.  -5.   3.  -4.   0.   1.  10.   4.\n   4.  -5.]\"\t1714.0(0.0)\t1808.0(0.05)\ng2.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[42. 46. 46. 45. 59. 47. 50. 58. 48. 54. 44. 36. 49. 41. 42. 46. 49. 45.\n 50. 42. 45. 47. 53. 39. 38. 61. 54. 52. 55. 52. 52. 44. 57. 46. 53. 57.\n 39. 52. 48. 54. 45. 57. 38. 45. 36. 46. 41. 46. 58. 50. 43. 56. 47. 41.\n 51. 58. 43. 54. 40. 41. 40. 42. 49. 47. 50. 66. 44. 54. 61. 50. 61. 59.\n 48. 52. 52. 50. 44. 47. 49. 49. 50. 45. 43. 44. 52. 45. 51. 47. 45. 53.\n 42. 44. 50. 47. 50. 47. 57. 40. 53. 50. 64. 66. 49. 57. 46. 45. 47. 43.\n 48. 43. 52. 51. 45. 50. 56. 61. 49. 44. 47. 45. 39. 57. 49. 45. 60. 39.\n 48. 51. 45. 43. 48. 47. 48. 57. 45. 47. 52. 53. 48. 49. 38. 40. 54. 52.\n 45. 47. 48. 52. 42. 48. 49. 50. 49. 52. 52. 48. 49. 48. 53. 41. 48. 50.\n 42. 55. 54. 46. 42. 42. 50. 56. 45. 52. 45. 65. 54. 43. 44. 45. 55. 48.\n 49. 43. 51. 45. 37. 42. 45. 59. 46. 52. 45. 42. 34. 55. 52. 41. 52. 52.\n 40. 51. 53. 55. 51. 52. 52. 40. 43. 51. 42. 43. 49. 47. 45. 63. 50. 65.\n 28. 49. 47. 50. 35. 55. 39. 38. 38. 49. 48. 46. 60. 47. 51. 59. 56. 41.\n 55. 41. 53. 47. 38. 43. 44. 46. 46. 45. 31. 34. 52. 38. 44. 54. 48. 45.\n 45. 43. 51. 44. 51. 58. 41. 41. 50. 54. 41. 47. 40. 48. 61. 54. 40. 36.\n 36. 45. 40. 37. 56. 42. 58. 51. 49. 54. 41. 48. 43. 42. 56. 55. 53. 41.\n 62. 48. 52. 45. 51. 48. 58. 46. 44. 51. 40. 39. 51. 54. 58. 43. 46. 47.\n 48. 49. 53. 50. 52. 55. 58. 62. 47. 53. 51. 51. 45. 54. 38. 54. 43. 55.\n 47. 40. 43. 48. 55. 45. 47. 41. 61. 52. 43. 48. 45. 57. 48. 40. 47. 57.\n 44. 47. 46. 41. 43. 48. 57. 57. 41. 54. 56. 45. 58. 42. 51. 47. 48. 53.\n 63. 52. 41. 35. 44. 48. 56. 44. 65. 54. 46. 43. 50. 52. 36. 39. 41. 38.\n 47. 48. 47. 46. 43. 40. 46. 49. 51. 50. 46. 44. 54. 51. 36. 47. 48. 43.\n 43. 44. 44. 39. 43. 30. 49. 51. 48. 53. 43. 43. 43. 41. 51. 47. 56. 41.\n 57. 52. 54. 65. 56. 49. 50. 47. 41. 37. 43. 48. 42. 56. 36. 48. 46. 60.\n 41. 49. 42. 41. 54. 52. 55. 43. 46. 55. 53. 57. 53. 43. 55. 54. 42. 59.\n 29. 42. 58. 40. 52. 50. 60. 53. 43. 37. 43. 45. 50. 46. 36. 45. 49. 39.\n 46. 48. 44. 53. 36. 49. 53. 45. 45. 55. 50. 40. 36. 55. 36. 58. 48. 45.\n 46. 39. 50. 53. 46. 52. 44. 46. 65. 40. 61. 60. 48. 52. 51. 44. 50. 49.\n 41. 39. 43. 48. 49. 52. 42. 54. 46. 42. 53. 43. 57. 40. 48. 56. 49. 38.\n 59. 50. 54. 44. 47. 53. 42. 55. 55. 51. 39. 52. 54. 47. 52. 39. 57. 49.\n 48. 45. 36. 50. 44. 42. 42. 53. 54. 39. 45. 55. 52. 34. 57. 53. 52. 43.\n 62. 54. 48. 56. 50. 53. 40. 33. 43. 45. 47. 48. 39. 56. 39. 50. 52. 43.\n 52. 61. 54. 46. 45. 47. 49. 41. 53. 58. 39. 47. 40. 54. 45. 53. 51. 54.\n 44. 58. 54. 49. 51. 45. 59. 45. 49. 54. 48. 37. 58. 37. 49. 39. 53. 42.\n 46. 50. 50. 41. 53. 44. 40. 43. 38. 42. 40. 44. 59. 51. 56. 40. 45. 45.\n 58. 43. 48. 56. 49. 55. 44. 46. 50. 41. 47. 41. 46. 47. 47. 58. 49. 42.\n 49. 45. 49. 48. 43. 64. 52. 46. 47. 54. 49. 51. 43. 54. 47. 57. 45. 37.\n 50. 44. 41. 60. 49. 46. 41. 50. 41. 33. 48. 45. 46. 53. 50. 57. 56. 47.\n 59. 40. 48. 49. 42. 47. 36. 42. 46. 45. 42. 54. 57. 49. 44. 54. 54. 52.\n 50. 50. 39. 36. 55. 41. 48. 55. 46. 52. 37. 42. 58. 52. 47. 58. 55. 34.\n 42. 49. 46. 52. 39. 44. 48. 52. 56. 39. 41. 60. 49. 42. 44. 50. 47. 60.\n 45. 54. 34. 53. 40. 40. 52. 49. 54. 44. 47. 43. 45. 51. 33. 55. 47. 56.\n 44. 47. 51. 50. 48. 46. 54. 47. 37. 47. 49. 46. 45. 52. 38. 49. 56. 47.\n 44. 63. 40. 61. 46. 46. 51. 63. 62. 45. 45. 37. 50. 52. 48. 42. 51. 51.\n 51. 49. 45. 53. 35. 44. 33. 48.]\"\t11420.0(0.0)\t11433.0(0.0)\ng4.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[44. 57. 41. 54. 44. 38. 45. 47. 45. 44. 48. 43. 51. 61. 45. 57. 44. 42.\n 56. 41. 60. 58. 49. 44. 37. 47. 48. 36. 56. 49. 50. 61. 46. 56. 44. 53.\n 36. 46. 54. 47. 50. 46. 42. 44. 54. 45. 50. 65. 37. 49. 41. 39. 33. 51.\n 45. 52. 60. 40. 48. 53. 35. 44. 39. 53. 44. 52. 50. 48. 47. 44. 51. 55.\n 40. 44. 53. 51. 43. 46. 46. 61. 48. 48. 54. 46. 49. 50. 61. 51. 40. 45.\n 46. 42. 48. 40. 45. 43. 53. 48. 48. 53. 44. 53. 39. 47. 49. 43. 45. 44.\n 46. 45. 37. 51. 58. 47. 44. 42. 32. 50. 48. 42. 40. 65. 57. 62. 43. 53.\n 45. 52. 47. 56. 46. 47. 51. 51. 55. 46. 49. 48. 35. 50. 50. 61. 39. 54.\n 42. 44. 58. 48. 50. 53. 57. 63. 57. 42. 39. 46. 53. 59. 55. 39. 58. 46.\n 36. 33. 48. 51. 61. 48. 47. 53. 56. 46. 60. 53. 45. 51. 46. 55. 45. 40.\n 41. 60. 50. 31. 44. 56. 49. 46. 50. 58. 57. 49. 41. 39. 47. 51. 55. 48.\n 52. 51. 52. 39. 32. 57. 43. 34. 52. 49. 50. 46. 49. 43. 51. 54. 33. 45.\n 57. 57. 54. 47. 45. 45. 58. 59. 46. 41. 41. 53. 44. 59. 43. 42. 50. 47.\n 50. 42. 46. 59. 54. 57. 45. 45. 42. 44. 52. 57. 57. 59. 37. 57. 47. 45.\n 51. 37. 51. 38. 68. 40. 53. 51. 62. 46. 55. 46. 55. 35. 41. 53. 45. 46.\n 44. 50. 61. 41. 46. 55. 39. 48. 52. 45. 40. 41. 52. 56. 56. 48. 49. 46.\n 50. 50. 56. 45. 49. 38. 46. 50. 52. 42. 39. 45. 42. 58. 47. 54. 41. 37.\n 49. 43. 53. 38. 46. 52. 46. 46. 52. 46. 44. 43. 55. 51. 53. 46. 49. 50.\n 42. 45. 41. 48. 45. 55. 38. 51. 52. 44. 48. 29. 53. 59. 45. 45. 48. 57.\n 37. 51. 36. 40. 55. 37. 39. 54. 33. 57. 49. 48. 48. 54. 53. 43. 49. 34.\n 45. 50. 50. 51. 50. 53. 40. 48. 60. 46. 47. 44. 42. 51. 48. 46. 49. 52.\n 44. 54. 44. 49. 47. 56. 46. 52. 56. 63. 48. 53. 50. 50. 43. 49. 51. 60.\n 44. 49. 51. 50. 52. 51. 37. 48. 48. 57. 41. 59. 51. 50. 56. 45. 40. 43.\n 49. 52. 38. 51. 56. 35. 58. 52. 64. 64. 55. 61. 41. 40. 38. 53. 46. 55.\n 51. 41. 44. 43. 53. 39. 42. 48. 42. 46. 39. 51. 40. 47. 53. 51. 44. 39.\n 51. 47. 48. 48. 40. 55. 52. 46. 52. 55. 55. 46. 53. 59. 48. 38. 42. 54.\n 48. 51. 48. 47. 48. 55. 47. 39. 51. 49. 40. 52. 52. 59. 48. 41. 51. 42.\n 61. 47. 47. 50. 42. 47. 47. 44. 40. 40. 53. 45. 49. 41. 50. 45. 59. 57.\n 45. 41. 47. 54. 34. 62. 35. 51. 43. 53. 41. 48. 52. 51. 42. 45. 42. 55.\n 44. 55. 57. 33. 50. 43. 50. 48. 48. 53. 38. 54. 48. 48. 43. 51. 52. 52.\n 48. 42. 42. 62. 42. 42. 51. 59. 40. 42. 62. 55. 59. 45. 46. 45. 40. 45.\n 55. 49. 38. 57. 39. 54. 39. 60. 47. 53. 50. 56. 46. 41. 52. 37. 47. 45.\n 49. 50. 42. 45. 38. 35. 45. 57. 43. 41. 48. 46. 52. 56. 52. 52. 51. 52.\n 38. 48. 36. 49. 59. 57. 54. 46. 45. 40. 58. 54. 59. 48. 58. 43. 43. 62.\n 56. 44. 40. 49. 37. 49. 57. 55. 47. 54. 37. 47. 44. 49. 48. 53. 38. 46.\n 49. 42. 41. 51. 43. 50. 52. 45. 52. 37. 45. 35. 52. 46. 48. 52. 35. 42.\n 46. 42. 62. 41. 56. 46. 46. 55. 45. 42. 55. 39. 59. 54. 60. 41. 45. 46.\n 50. 44. 48. 58. 52. 48. 51. 57. 42. 44. 51. 48. 40. 49. 52. 46. 42. 46.\n 52. 55. 55. 43. 44. 42. 40. 44. 42. 56. 59. 42. 57. 50. 50. 54. 49. 38.\n 53. 47. 40. 41. 45. 51. 51. 47. 42. 35. 41. 43. 43. 38. 45. 36. 54. 53.\n 39. 39. 59. 51. 48. 47. 49. 44. 47. 56. 38. 47. 61. 48. 49. 44. 59. 43.\n 39. 41. 57. 52. 47. 43. 49. 38. 48. 48. 45. 51. 43. 45. 55. 50. 54. 42.\n 52. 49. 46. 39. 57. 55. 40. 49. 58. 52. 51. 44. 48. 38. 62. 43. 51. 40.\n 47. 40. 46. 51. 52. 41. 37. 39. 43. 40. 46. 35. 53. 51. 46. 45. 49. 39.\n 52. 53. 42. 51. 54. 75. 48. 47.]\"\t11384.0(0.0)\t11377.0(-0.0)\ng7.mc\t47.94\t0.06\t-0.007822277847309137\t1.0\t-1.0\t0.9999694055166285\t\"[ -2.   6.   0.  -9.  -5.   5. -14.  -4.  -2.  -2.   2.  -6.   7.   3.\n   2.  -2.   7.   3.   0. -10.   5.  -3.  11.   7.  -6.  19.  -2.   0.\n -11.  -2.   4.  -6.  -5.   0.  -5. -13.  -7.   4. -12.   4.   9.  -7.\n  -4.   3.  -4.   2. -11.  -4. -16.  -6.   1.  -8.   5.   3. -15. -18.\n   5.  -2. -18.   5.   6.   4.   1.   1.  10.   4.  10. -12.   1.   2.\n  -3.  -7.  -4.   2. -10. -14.   4.  -9.  -3.   7.   6.   5.   3.   2.\n   2.   3.   3.  -7.   3.  -9.  12.   6.   0.  -1.  -2.   3.  -3.   4.\n   3.  -4.  -8.  -8. -13.  -7.  -4. -11.  -9.  -3. -12.  -7.  -6.   3.\n   7.   0.  12.   1.  -3.  12.   7.  -1.   7.  -1.   9.  -5.   4.  -5.\n   2.  15.   9.  -7.  16.  -1.   2.  -5.   9.  13.   2.   1.   4.  -9.\n  -2.   6. -10.  -6.  -5.  13.  20.   2.   2.   4.   1. -12.  -1.   8.\n  -8.  -4. -11.  -6.   3.   7.   8.   4.  -4.  -1.  -2.  -6.  14.   2.\n  -8.   0.   7.   2.  -7.  11.  -2.   5. -10.  -3. -17. -14.  -3.  -5.\n  -9.   9.  -7.  -8.  -7.  -1.  -4.  -8.  -1.  -2.  -2. -11. -16.   5.\n  -8. -12.   0.  -5.   1.  -3. -13.   0.   2. -14.   5.   3.   0.  -3.\n  -3.   1.  -1.  -9.  16.   3.  -2.  -1.   5.   0.   7.   1.  -1.   6.\n   2.  -9.   6.   0. -10.  -7.  -3.   7.   8.  -7.  11.   1.  -1.  -5.\n   2.  -5.  -2.  -6.   6.   1.  -1.   4.   0. -10.   8.  10.  -2.  -3.\n   9. -11.  -1.  -4.  -5.   4.   1.  -7.   2.  -2.  -5.  -1.  -8.  -6.\n  -1.   4.   8.   2.   0.   1.   2. -11.   6.   6.   6.   3.  -1.  14.\n  -5.   8.   3.  -6.   2.  -3.   9.   5.  -2. -10.   6.   3.   3. -12.\n   2.   0.   6.   7.   8.  19. -11.  -4. -10.  -7.   8.  -7.  -2.  11.\n  -3.  12.   6.   5. -10.   8.  -1.  -1.   7.  -3.   7.  -6.  -4.   2.\n  11.  -1.  -1.  -4.  -9.  -2.  -5.   7.  11.  -1.  -3.  -6.  -1.  -4.\n   3.  -7.  16.  -4. -11.   7.  14.  -9.  12.  -1.   3.  -6.  -5.  -9.\n -15.   0. -18.  13.   4.  10.  -5.  -3.   4.  11.  -3.  -2.   3.   1.\n   4.  12.  -6.   8.   5.  10.  -8.  -1.   6.  -8.   0.  -5.  -3.   2.\n   7.   2.  -1.   2.   7.  -2.   0.  -1. -15.  -6.  -2.   0.  10.  -7.\n   8.  -5.   0.  -1.   3.   4.  -2.   5.  13.   0.   9.   7. -10.   3.\n   1.   5.  -5.  -3.  -3.  -7.  -2.   5.  11.  10.  -4.   9.  -4. -17.\n   2.   3.   1.   1.  -3.   0.   0.  -4.   2.  -4.   0.   4.   1.  -3.\n  -6. -13.  -4.  -2.  -7.   3.   8.  13.   7.  -1. -13.  13.  -9.   2.\n   2. -11.  -1.   4.   2.   4.   0.   4.  -8.  -7.  -5.  -7.   9.   5.\n   6.  -8.   0.  15.  11.  -1.   4.   6.   2.   9.   0.  -3.   5. -11.\n   5.   1. -10.  10.   0.   7.   0.  -4.   2.  -7.  -4.   3.  10.   3.\n   6. -14. -12.   6.  -3.   2.  -1.   6.   4.  -4.   5.  -2.   0.  -7.\n   3.   3.   9.   8. -15.  -6.  -2.  10.   2.   0.  15. -13. -13.  -8.\n   4. -16.   5.   0.   1.  10.  10. -12.   1.   5.   2.   3.  -5.  -3.\n -13.   4.  -6.  -7.   0.   7.  -9.  -7.  -2.   1.   2.   2.   4.   2.\n   2.  -9.  14.  11.  -7.   9.   8.  -2. -11.  -7.  -4.   1.  -4.   0.\n  -4.   2.  -4.  -3.   8.  -3.  -5.   3.   3.  16.  -3.  10.  -1.   6.\n  12.  11.  -6.   5.  -6.  -6.   5. -11.  13.   1.  -9. -14.   3. -15.\n -12.  -8.   1.  -7.   7. -10. -12.  -8.   4.  -7. -15.  -1. -11.  -1.\n  -1.  14.   2.  -1. -14.   3. -11.  -3.  -3.  -6. -10.   4.   4.  -3.\n  -3. -10.  -2.   5.  -4.  -8.  12.   4.   3.  -5.  -2.  -4.  -5.  -9.\n  -6.   9.  -8.  -4.   7.  -7.   6.   6.   6.  -3. -11.   7.  -4.  -9.\n  -3.   8. -11.  14.   3.   5.   5.  -2.  -1.  12. -10. -14.   3.  14.\n   1.   1.   5.   8.  -3.  -9.   7.   7. -18. -14.  15.  -6.   1.  -4.\n  -5.  -2.   7.   9.  10. -11.  -2.   7.  -4.  -7.  -2.   3.  -7.   4.\n   8.  -7.  -6.  -3.   2. -10.  -2.   1.   2.  -4.   3.  -7.  -2. -10.\n  -2.  -4.  -4.  -4.   9.   6.  13.  -1.   4.  -7.   0.  12.   1.   0.\n  -4.   4.  -3.  -2.  -5.  -2.   0.  17.   6. -12.  -1.   0.  -6.   4.\n   0. -13.  -3.   6.  -3.   6.   6.  -6.   9. -12.  -9.   2.  -2.  -1.\n -12.   4.  -6.   3.   4.   0.  -7.  -1.  -1.   7.   5.  -7.  -7.   8.\n  14. -11.   3.  -2.  -6.  -4.  -4.   7.   7.   1.   5.   4. -11.  -8.\n   4.   7. -12.  -5.   4.   5. -10.  17.  -6.  -2.   1.  -9.  -2. -11.\n  -3.  -3.   4.  -2.  -2.   8.   5.  13.  -1.  -7.   5.   9.  -5.   2.\n   3.   0.]\"\t1806.0(0.0)\t1818.0(0.01)\ng8.mc\t47.94\t0.06\t-0.008865248226950355\t1.0\t-1.0\t0.9999607029148068\t\"[ -5.   7.  -3.   1.  -9.   4.   9.  -6.  -2.   0.   0.  11.   3.   2.\n  -2.  -5.  13.  -1.  -9.   3.   4.   1.   2.   3.   2.  -1.  -1.   9.\n   3.  -8.  -1.   8.  -4.   7.  -2.   5.  -1.   5.  -1.   0.  -1. -10.\n  -5. -11.  -4.  -4.  13.  15.   3.   4. -10.   1. -17.  -7.   4. -11.\n   6.   0.  12.  -4.  -4. -28.  -2.  -8.  -4.  12.  -2. -15.  -9.   1.\n  -7.   9.  -7.  -6.   6. -18.   5.  -2.  -3.   6.   3.   2.  -1.   3.\n   8.  12.  -2.  -5.  15.   4.  -7.  16.   1.   6.  -2.  -5. -11.  -5.\n -12.  -4.  -5.  -3.   0.   1.   6.  -1.   9.   6.  -8.   0.  -9.   6.\n   3.   4. -12.  -6.  -1.   8.  -1.   1.  -3.   3.   5.   3.   5.   3.\n  -5.   6.   2.   2.  17.  -4.  -6.   3.  -6.   4. -11.  -9.  -5.  -1.\n   2.  -1.  -8.   1.   7.   4.   2.   2.  -4.  -3.  10.  20.  -3.  -4.\n  -4.   1.   7.  11.   2. -12.  -2.   1.   6.  -1.   0.  -8.   9.  -2.\n   2.  -3.  13.  -8.   3.   4.   1.   7.   0.  -8.  -1.   8.   4.   1.\n  -1.   4.  -6.  10.   1.   9.   3.   0. -12.  -7.   0.   6.  -5.   0.\n   6.   5. -11.  10.  11. -13.   0.  -1.   9.   9.   1.   7.  -9.  10.\n   3.  -7.  -6.  -1.   2.  -1.  -5.   2.  -2.   3.   5.   2.   2.   2.\n -15.   1. -21.  -4.  -3.   4.   2.  -5.  -6.   1.  -6.   5.  -9.  -1.\n  -1.  -8.   3.  -2.  -2. -11.  -4.  -9. -12. -12.  -6.  -3.  -1. -13.\n  -4.   5.  -3.   1.   5.  -1.  -7.  -5.  -8.   8.  11.  13.  -9.   8.\n  -3.  -5.   4.  -2.   6.   3.   3.   1.  -9.  -8. -12.  -7.   7. -13.\n   0.   2.  11.  11.   2.  -2.  10.  -4.  -6.  15.   1.  21.  -7.   3.\n  -6.   0.  -6.   6.   8.  -5.  -5. -14.  -3. -11.  11.   9.   5.   4.\n   3.   8.  11.   1.   0.  -3.  -4. -13.   1.   2.  14.   4.  -5.   5.\n   0.   5.  -4.  -8.   9.  -5.   0.   2.  -3.   7.  15.   4.   4.   8.\n   4.  -3.   5. -11.   0.   8.   1.   4.   8.   3.  -5. -14.   0. -12.\n   4.   2.  -3.  -1.   2.  -1.  -3.  -2.  -5.   4.  -6.  11.   0.   4.\n   9.   1.   0.  -2.  -6.   3.  -3.  -3.   8.   8.  12.  -9.  -4. -15.\n  -7.   0.  -1.   7.   7.  -1.  -9.  -6.  -1.   6.  -2.  -4.   4.   1.\n  -1.  -5.  -3.   7.  -2.  -3.  -6.   0.   8.  -5.  -3.   9.  -5.   2.\n   5.  -1.  -2.  -8.  -3.  -3.  -8.  -2.  -7.  -7.  -5.  14.  -4.   6.\n  -1. -10.  11.   1. -11. -16.  -3.  12.   7.   9.  12.   0.  -8.   3.\n  -5.   4.  11.  13.  -6.  -9.   1.   3.  -9.   0.  -3.  -2.  -5.   2.\n  -5.  -2.   8.  -7. -15.   4.   6.  17.   0.  -4.   5. -14.  -2.  -2.\n  -1. -11.  -7. -10.   3. -15. -10.  10.  -4.   7.   2.   2.   1.   6.\n   3.  -1.   0. -12.   2.   1.   2.   3.   2.  -1.  -2.   9.   1.   3.\n   3.   1.   6.  -8. -18. -21.   6.  -6.  -3.  -8. -17.  -1.   4.  -6.\n  -4.  -5.   9.  -2.   2.   8.   9.  -6.   0.  -7.   9.   0.  -2.  -3.\n   1.   8.   2.  -8.   7.  -2.  -4.   3.   0.  -6.   9.   0.   5.   6.\n   5.  -6.   6.   5.   0.   0.  -1.  -6.   4.  -4.  -9.   0.  11.  14.\n   3.   2.  -7.   3.  -6.   6.  14.   0.   5.   4.  -3.  -6.  -1. -17.\n -13.   0.  -2.  -4.  -3. -10.   0.  -2. -17.  -7.  10.  -5.   4.  -3.\n -13. -10. -13.   4.  -3.  -9.   2.  -3.   3.   8.  11.   5. -14.   4.\n   0.   0.   1.   6.   1.  10.  -5.  -5.  -9. -10.   9.  10.   0. -14.\n   9.   7.   7.  -8.  -7.   1.   3.  -4.  -3.  -8.   2.   0.   2.   0.\n  -2.   2.   8. -11.   7.   5.   1.   9. -12.  -3.   0. -15.   5. -14.\n  -9.  -7.  12.  -7. -14.   6.   9.   7.  -2.  -4.  -4.   0.   0.  -4.\n  -8.  10.   2.  -1.   1.   8.   0.  -7. -18.  -7.   1.  -8.  -4.   1.\n  -2.  -4.   0.   2.   1.  -8.  -7.   7.   1.  -5.  -8.  -1.   1.  13.\n   2.   2.  -5.  -5.   2.   0. -11. -10.  -1.  -1. -12.  -8.   7.  -8.\n  -3.   5.  -1.  -5.   0.  -4. -10. -11.  -3.   2. -14.  -3.  -7.  -2.\n  -5.  -9.  10.   7.   4.  -6.  -9.  -3.  10.   2. -13.  -5.   0.  -8.\n  -6.  -4.   8.   6.   0.  -6.   4.  -2.   8.  -4.  -3.   8. -13.   2.\n   0.   4.   3.   3.   6.  16.  -3.  -6.   3.  -5.   9.   2.  10.   2.\n  -5.   0.  15.   2.   1.  -6.  -8.   0.   2.  10.   2.  -7.  -6.   0.\n   5.   4.  -3.   2.  -3.   3.   0.  -3. -10.   3.  -2.  -6.   3.  -8.\n   4.   1.  18.  -4.   7.   2.  -3.   0.   2.  -5.   2.  13.   6.  11.\n  -9.   6. -13.   1.  11.   1. -15.  -3.  -1. -12.   1.  -6.  -3.   1.\n  12.  -2.]\"\t1802.0(0.0)\t1863.0(0.03)\ng3.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[41. 41. 47. 51. 55. 54. 39. 42. 48. 50. 44. 41. 57. 42. 44. 57. 59. 55.\n 39. 39. 54. 41. 44. 49. 48. 37. 55. 45. 53. 54. 39. 46. 52. 51. 36. 43.\n 47. 53. 47. 64. 45. 62. 57. 51. 48. 48. 41. 51. 41. 54. 66. 49. 53. 45.\n 54. 41. 36. 40. 52. 58. 42. 46. 50. 44. 44. 42. 56. 45. 43. 59. 59. 51.\n 45. 56. 56. 50. 43. 48. 47. 46. 45. 42. 65. 61. 58. 46. 48. 59. 39. 60.\n 41. 46. 47. 36. 46. 45. 33. 55. 50. 40. 49. 43. 30. 59. 36. 45. 41. 48.\n 38. 54. 53. 42. 61. 44. 36. 40. 43. 44. 33. 39. 49. 51. 47. 37. 63. 49.\n 45. 50. 64. 42. 33. 42. 42. 45. 48. 50. 59. 53. 43. 47. 52. 43. 60. 47.\n 41. 42. 56. 50. 46. 51. 60. 54. 45. 36. 54. 41. 49. 31. 44. 50. 38. 61.\n 56. 41. 60. 42. 47. 50. 42. 45. 51. 50. 37. 42. 37. 41. 48. 52. 49. 60.\n 50. 53. 41. 54. 52. 48. 47. 51. 47. 44. 50. 55. 46. 48. 53. 42. 48. 43.\n 47. 48. 43. 59. 54. 35. 41. 49. 39. 53. 49. 46. 49. 47. 64. 57. 56. 51.\n 41. 44. 42. 55. 55. 50. 48. 52. 43. 33. 49. 40. 41. 46. 40. 45. 44. 45.\n 48. 49. 51. 49. 43. 42. 47. 40. 50. 55. 48. 47. 48. 58. 52. 51. 53. 37.\n 52. 45. 49. 43. 45. 45. 43. 39. 58. 56. 47. 69. 65. 46. 43. 45. 54. 48.\n 48. 49. 39. 55. 53. 52. 50. 49. 57. 53. 46. 48. 51. 55. 50. 46. 42. 52.\n 52. 51. 49. 49. 43. 43. 46. 46. 48. 44. 42. 51. 39. 50. 61. 53. 47. 41.\n 55. 48. 39. 42. 57. 55. 46. 59. 36. 47. 57. 46. 48. 58. 37. 45. 50. 51.\n 52. 52. 39. 47. 56. 36. 55. 57. 39. 48. 44. 54. 42. 45. 57. 45. 46. 48.\n 39. 46. 40. 45. 45. 46. 52. 42. 46. 46. 63. 45. 42. 37. 47. 46. 51. 52.\n 50. 41. 46. 46. 51. 43. 52. 42. 48. 47. 57. 59. 48. 56. 52. 43. 56. 57.\n 45. 56. 51. 49. 55. 39. 45. 44. 45. 50. 58. 52. 44. 41. 43. 47. 49. 53.\n 50. 49. 50. 40. 50. 51. 39. 65. 41. 46. 45. 49. 50. 50. 37. 53. 56. 46.\n 45. 53. 63. 36. 58. 54. 61. 44. 47. 41. 61. 46. 39. 34. 41. 57. 48. 42.\n 38. 49. 43. 44. 43. 37. 50. 63. 55. 63. 37. 52. 43. 44. 47. 42. 33. 46.\n 50. 55. 51. 54. 58. 61. 44. 46. 47. 48. 38. 58. 41. 55. 51. 42. 51. 55.\n 46. 52. 46. 45. 60. 48. 47. 42. 63. 47. 44. 46. 52. 43. 56. 49. 44. 47.\n 52. 55. 49. 33. 55. 45. 48. 38. 50. 49. 46. 48. 55. 40. 57. 45. 44. 44.\n 50. 51. 47. 42. 48. 42. 49. 48. 46. 45. 51. 44. 54. 51. 43. 50. 48. 32.\n 51. 42. 42. 43. 54. 54. 49. 60. 31. 44. 57. 46. 50. 39. 38. 50. 47. 56.\n 56. 44. 55. 42. 45. 56. 41. 56. 43. 43. 46. 52. 42. 44. 47. 46. 57. 50.\n 39. 51. 43. 46. 46. 54. 35. 52. 52. 36. 57. 45. 50. 49. 44. 47. 51. 60.\n 49. 40. 35. 45. 48. 53. 51. 40. 47. 53. 42. 62. 50. 52. 47. 56. 43. 48.\n 45. 69. 41. 46. 49. 40. 46. 46. 39. 51. 53. 66. 35. 49. 45. 36. 49. 52.\n 54. 36. 46. 54. 40. 56. 44. 55. 49. 51. 39. 43. 42. 43. 38. 53. 57. 60.\n 51. 51. 48. 43. 44. 52. 51. 45. 46. 50. 54. 54. 46. 46. 50. 52. 40. 49.\n 41. 50. 42. 57. 48. 37. 41. 40. 50. 49. 58. 52. 58. 52. 51. 54. 47. 65.\n 53. 47. 58. 49. 43. 47. 48. 52. 43. 53. 40. 50. 51. 54. 37. 51. 60. 40.\n 51. 52. 51. 41. 45. 59. 46. 46. 54. 43. 43. 46. 58. 61. 51. 42. 39. 39.\n 56. 53. 52. 62. 57. 41. 42. 54. 51. 53. 52. 62. 38. 52. 48. 40. 50. 42.\n 46. 60. 48. 50. 43. 50. 49. 48. 50. 40. 47. 41. 42. 54. 45. 48. 49. 57.\n 45. 42. 42. 46. 39. 48. 47. 54. 53. 38. 50. 42. 46. 50. 48. 47. 36. 44.\n 39. 44. 53. 46. 45. 47. 52. 45. 40. 39. 60. 48. 49. 52. 34. 61. 46. 48.\n 45. 38. 45. 46. 48. 57. 34. 45. 52. 51. 63. 50. 51. 47. 45. 53. 47. 45.\n 53. 52. 43. 42. 47. 39. 48. 50.]\"\t11421.0(0.0)\t11405.0(-0.0)\ng5.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[40. 51. 49. 66. 50. 43. 55. 42. 46. 39. 43. 47. 47. 51. 49. 46. 36. 44.\n 42. 33. 49. 45. 41. 43. 55. 47. 51. 44. 53. 50. 49. 43. 42. 49. 64. 39.\n 41. 35. 52. 49. 52. 58. 57. 42. 53. 49. 61. 45. 44. 35. 51. 50. 45. 46.\n 65. 42. 39. 42. 45. 41. 48. 42. 51. 45. 54. 47. 47. 37. 43. 44. 47. 47.\n 38. 39. 34. 46. 54. 50. 47. 38. 63. 54. 48. 53. 45. 44. 34. 48. 57. 40.\n 48. 51. 55. 48. 51. 44. 70. 53. 56. 57. 46. 55. 40. 39. 45. 47. 47. 44.\n 38. 40. 49. 48. 48. 35. 57. 45. 43. 44. 56. 47. 48. 36. 58. 51. 58. 44.\n 36. 36. 41. 51. 46. 49. 42. 46. 41. 44. 42. 47. 54. 39. 51. 56. 51. 55.\n 63. 62. 52. 47. 48. 47. 54. 40. 52. 47. 46. 40. 50. 55. 45. 42. 51. 44.\n 59. 53. 36. 46. 54. 45. 58. 52. 53. 39. 38. 57. 51. 52. 44. 54. 42. 30.\n 50. 58. 46. 43. 50. 49. 38. 30. 42. 52. 47. 57. 47. 44. 42. 47. 50. 58.\n 53. 50. 51. 52. 61. 57. 52. 48. 43. 51. 44. 45. 57. 43. 52. 43. 52. 51.\n 59. 47. 49. 53. 55. 48. 51. 47. 56. 48. 59. 42. 41. 45. 40. 68. 43. 50.\n 49. 40. 49. 46. 44. 50. 46. 54. 60. 42. 52. 58. 56. 46. 55. 45. 50. 66.\n 51. 57. 48. 59. 51. 45. 51. 51. 38. 50. 65. 56. 51. 36. 48. 47. 45. 43.\n 51. 57. 48. 46. 50. 51. 55. 51. 51. 44. 50. 49. 51. 44. 61. 57. 46. 46.\n 48. 43. 52. 41. 48. 54. 45. 39. 43. 37. 49. 44. 45. 38. 44. 57. 45. 34.\n 44. 51. 57. 56. 47. 46. 38. 53. 52. 62. 37. 39. 46. 49. 51. 45. 49. 44.\n 32. 56. 54. 42. 44. 55. 38. 47. 56. 41. 34. 49. 45. 52. 51. 57. 50. 40.\n 52. 51. 49. 58. 44. 45. 47. 37. 45. 32. 65. 44. 58. 54. 46. 45. 55. 35.\n 40. 42. 56. 53. 45. 50. 40. 44. 49. 51. 43. 53. 42. 51. 55. 57. 49. 44.\n 49. 60. 46. 66. 52. 49. 47. 46. 41. 54. 61. 45. 44. 58. 42. 52. 42. 66.\n 60. 39. 46. 42. 54. 67. 51. 35. 42. 45. 46. 52. 47. 49. 48. 55. 41. 42.\n 49. 40. 41. 59. 48. 43. 65. 54. 54. 43. 46. 56. 55. 49. 42. 40. 53. 46.\n 50. 50. 43. 47. 54. 37. 38. 54. 47. 59. 42. 54. 61. 49. 44. 57. 50. 53.\n 42. 40. 56. 45. 50. 57. 36. 40. 43. 43. 41. 45. 39. 51. 55. 55. 50. 46.\n 57. 50. 46. 49. 38. 47. 57. 45. 53. 45. 45. 52. 41. 45. 50. 50. 56. 53.\n 57. 43. 49. 52. 67. 52. 50. 37. 43. 45. 45. 50. 46. 50. 39. 39. 49. 47.\n 40. 54. 46. 52. 40. 56. 45. 40. 46. 38. 58. 48. 55. 40. 61. 51. 47. 42.\n 53. 39. 56. 45. 31. 45. 46. 42. 58. 47. 51. 43. 46. 55. 42. 50. 57. 43.\n 46. 46. 46. 42. 46. 55. 54. 48. 47. 51. 38. 57. 45. 46. 38. 54. 48. 49.\n 40. 51. 63. 45. 45. 46. 40. 41. 53. 53. 56. 50. 48. 46. 45. 53. 48. 54.\n 58. 38. 48. 47. 49. 39. 48. 54. 51. 60. 52. 48. 51. 36. 43. 52. 36. 47.\n 51. 50. 42. 45. 41. 51. 50. 40. 51. 54. 52. 46. 48. 34. 39. 50. 45. 47.\n 54. 54. 50. 49. 48. 41. 54. 56. 47. 43. 51. 58. 51. 46. 52. 40. 52. 53.\n 41. 48. 51. 41. 50. 45. 50. 41. 41. 46. 35. 42. 51. 57. 40. 53. 59. 47.\n 42. 40. 50. 45. 38. 42. 56. 60. 47. 53. 43. 46. 43. 39. 43. 45. 50. 53.\n 42. 55. 43. 51. 45. 50. 49. 51. 46. 35. 44. 48. 48. 50. 59. 53. 58. 59.\n 56. 50. 63. 58. 47. 54. 45. 46. 42. 44. 54. 49. 47. 45. 45. 56. 47. 54.\n 48. 46. 42. 49. 51. 49. 45. 42. 35. 50. 50. 43. 38. 42. 46. 51. 55. 50.\n 51. 37. 52. 47. 44. 47. 42. 49. 34. 45. 37. 41. 37. 47. 33. 54. 41. 47.\n 42. 50. 41. 52. 49. 47. 35. 56. 39. 58. 46. 37. 47. 49. 49. 43. 48. 40.\n 42. 57. 57. 41. 40. 52. 51. 57. 44. 32. 44. 49. 50. 50. 50. 57. 45. 48.\n 48. 49. 56. 49. 44. 41. 43. 48. 56. 50. 46. 52. 50. 51. 51. 45. 53. 37.\n 51. 52. 48. 57. 50. 50. 60. 57.]\"\t11420.0(0.0)\t11531.0(0.01)\ng1.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[47. 51. 50. 51. 39. 49. 50. 48. 33. 46. 47. 40. 47. 53. 55. 64. 63. 52.\n 42. 49. 53. 53. 59. 39. 50. 53. 45. 49. 53. 45. 37. 54. 53. 51. 47. 49.\n 52. 45. 45. 48. 50. 46. 50. 48. 49. 34. 45. 42. 37. 43. 44. 53. 49. 39.\n 39. 52. 42. 50. 43. 50. 40. 48. 50. 45. 43. 54. 46. 51. 58. 55. 52. 46.\n 57. 46. 39. 46. 48. 54. 34. 45. 49. 51. 45. 43. 52. 53. 49. 55. 58. 48.\n 47. 40. 42. 44. 39. 45. 58. 43. 52. 45. 42. 50. 65. 39. 39. 48. 38. 38.\n 49. 49. 54. 61. 49. 47. 50. 59. 52. 54. 46. 58. 44. 41. 47. 53. 43. 53.\n 47. 49. 41. 42. 46. 57. 48. 43. 47. 42. 47. 45. 42. 49. 46. 55. 46. 62.\n 56. 45. 47. 45. 44. 55. 46. 42. 41. 59. 48. 40. 61. 53. 48. 46. 45. 46.\n 52. 54. 50. 42. 48. 58. 49. 46. 46. 46. 49. 37. 47. 54. 40. 57. 52. 43.\n 41. 45. 40. 52. 47. 51. 44. 59. 45. 50. 48. 50. 53. 45. 35. 59. 44. 65.\n 52. 52. 49. 41. 47. 45. 46. 50. 40. 42. 48. 47. 50. 46. 51. 36. 44. 56.\n 56. 55. 33. 36. 58. 43. 43. 41. 54. 48. 53. 64. 56. 44. 49. 51. 50. 56.\n 59. 45. 33. 40. 44. 48. 46. 39. 39. 59. 44. 62. 39. 46. 62. 45. 60. 53.\n 55. 48. 47. 45. 46. 60. 49. 55. 41. 55. 59. 43. 53. 50. 52. 45. 45. 55.\n 58. 37. 57. 45. 48. 47. 42. 48. 55. 42. 47. 58. 56. 49. 47. 50. 52. 45.\n 48. 46. 55. 42. 44. 55. 41. 41. 51. 40. 44. 45. 42. 62. 52. 50. 50. 44.\n 49. 56. 48. 46. 36. 43. 54. 42. 60. 44. 40. 46. 37. 61. 49. 53. 50. 46.\n 61. 44. 52. 27. 40. 49. 50. 53. 51. 54. 46. 43. 49. 47. 38. 48. 51. 39.\n 37. 53. 47. 49. 54. 53. 55. 44. 44. 50. 49. 45. 51. 42. 57. 47. 41. 42.\n 56. 56. 44. 44. 47. 56. 50. 61. 58. 44. 47. 35. 45. 37. 39. 51. 43. 43.\n 55. 46. 53. 48. 52. 51. 37. 54. 52. 45. 51. 45. 37. 44. 59. 39. 47. 48.\n 58. 42. 47. 47. 51. 48. 47. 40. 37. 49. 48. 49. 43. 41. 45. 39. 47. 57.\n 51. 34. 59. 40. 45. 38. 61. 50. 57. 48. 43. 50. 43. 48. 45. 51. 56. 34.\n 47. 57. 34. 45. 47. 67. 53. 37. 52. 43. 49. 49. 51. 62. 56. 59. 32. 49.\n 50. 43. 52. 54. 54. 48. 55. 53. 54. 46. 43. 54. 51. 42. 45. 46. 43. 52.\n 44. 47. 41. 52. 47. 38. 46. 43. 51. 42. 51. 47. 46. 47. 44. 37. 44. 47.\n 43. 48. 37. 50. 44. 45. 51. 45. 50. 47. 52. 57. 49. 52. 45. 40. 51. 46.\n 55. 37. 51. 41. 47. 50. 60. 49. 47. 44. 45. 52. 55. 53. 45. 40. 43. 39.\n 53. 50. 49. 53. 47. 39. 53. 54. 37. 48. 50. 47. 41. 41. 50. 46. 50. 49.\n 44. 56. 51. 44. 45. 50. 41. 52. 43. 40. 58. 56. 43. 41. 47. 47. 36. 40.\n 41. 62. 42. 46. 43. 40. 45. 47. 41. 48. 52. 55. 55. 45. 53. 49. 56. 51.\n 52. 53. 51. 35. 55. 44. 59. 55. 46. 36. 55. 49. 42. 51. 62. 52. 48. 53.\n 29. 52. 44. 44. 51. 52. 41. 55. 44. 40. 52. 48. 49. 53. 49. 47. 46. 55.\n 56. 46. 50. 51. 59. 49. 58. 44. 43. 45. 52. 52. 48. 52. 42. 50. 48. 44.\n 49. 46. 46. 44. 35. 47. 50. 66. 50. 40. 52. 56. 56. 42. 47. 47. 46. 58.\n 55. 53. 46. 52. 57. 47. 59. 53. 39. 42. 53. 41. 47. 47. 42. 50. 54. 34.\n 47. 48. 38. 51. 49. 47. 39. 37. 55. 58. 44. 49. 50. 42. 45. 49. 43. 58.\n 48. 48. 40. 53. 46. 51. 48. 33. 54. 48. 55. 44. 44. 48. 44. 43. 38. 50.\n 51. 45. 49. 50. 53. 52. 58. 62. 43. 49. 50. 51. 50. 52. 46. 49. 50. 44.\n 61. 52. 38. 46. 46. 39. 44. 40. 59. 58. 45. 49. 51. 50. 38. 43. 44. 49.\n 63. 44. 38. 47. 59. 55. 61. 40. 44. 45. 40. 39. 51. 38. 41. 49. 47. 46.\n 45. 33. 41. 52. 53. 49. 45. 60. 50. 44. 49. 55. 57. 55. 43. 50. 49. 44.\n 45. 50. 46. 51. 55. 46. 57. 50. 43. 45. 55. 40. 44. 54. 46. 47. 42. 48.\n 49. 54. 48. 42. 43. 52. 49. 39.]\"\t11443.0(0.0)\t11394.0(-0.0)\ng6.mc\t47.94\t0.06\t0.00803087192323738\t1.0\t-1.0\t0.9999677520281105\t\"[  3.  -5. -10.  -3.  -7.   9.   4.   4.  -5.   8.  -3.  -4.  15.  -1.\n  -7.  -6.   3.  22.   2.   9.  -5.  -5. -19.  -3.   8.   7.   5.   7.\n  -1.   5.  15.   2.   1. -15.  -3.  -7.  -2.   3.   9.   0.  -8.  -6.\n -10.  -2.   3.   4.  -1.   6.  -5.   5.   2.  -1.  -5.   1.   7.  -2.\n   6.  -4.  -1.  -6.  -6.  -8.   0.   3.   9.  10.  -6.   7.  12.   5.\n  -6.  -4.  11.   2. -17.   2.   4.  -4.  -2.   7.  -7.  -3.   9.   3.\n   8.  -1.  -3.   3.   2.   4.   7.  -6.  -4.   2.   3.   5. -12.   7.\n  10.   3.  -8.   8.  -7.  -5.  -1.  -2.   0.   2.  -7.   5. -10.   1.\n   7.   5.  12.  -7.  -2.   4.   4.   0.   4.  -5.   1.   3.  -1.  -5.\n   7.  11.  -7.   8.   4.  -1.  -2.  -3.   1. -14.   9.  17.   4.   3.\n   4.   3.  14.   2. -16.  -5.   1.   9.  10.  -9.  -2.   2.  -3.  11.\n -10.   0.   3.   9.  -4.  -6.   7.   6.   4.   0. -12.   0.  -6. -14.\n  -3.   8.   0.   0.  -5.   5.  -7.   8.  -4. -15. -12.   3. -15.  11.\n -10.  -8.  -1.   7.   4.  -3.  13.   4.  -4.   0.  -5.   5.   3.  -9.\n   6.  -1.   0.   6.  -5.  -3.   7.   3.   4.  12.   4.   4.  -8. -17.\n  -2.   2.   9.   0.   2.   2.   4.  -3.   3.  -4. -10.   5.  11.   5.\n -12.   0.  29.   4.  -2.   0.   1.  -5.  -4.  24.  -9.   1.  -3.   0.\n  -2.  -6.   0.   5.  -9.  -9.   8.  -2.   1.   4.  10.   7.  -2.  -5.\n   9.  -2.   3.   3.   8. -10.   3.   5.   5. -11.  -1.  -7.  13.  -2.\n  16.  -5.   5.  -7.  16.   3.   5.  -9. -14.  -5.  -6.   2.   9.  -8.\n   9.   6. -10.  -3.  -3.  -8.   0.  -9.  -8.   0.  -1.   0.   8.  -9.\n   3. -17.  -9.   0.   0.   7.   0. -10.  -2.  12.   4.   8.   7.  -2.\n   6.   0.   8.   5.  10.  -6.   4.   4.   8.  -4.  -7.  11. -11.   5.\n  -6.   8.  -3.   6.   6.   7.  -4.  -5.  -4.   7.  -1.  -2.  -2.   3.\n -19. -15.   2. -10.  -7.  -9.  -7.   1.  -5.  -1.   0.   7.  -3.   4.\n   4. -12.   9.   5.   7.   2.   9.  -3.   1.  -6.   6.   4.   0.  -6.\n  11.   2.   2.  11.   8.  -8.  -9.  -5.   9.   3.  -5.   7.  -3.   1.\n  -1.   4.  -1.  16.   8.   1.  -5.  10. -14. -13.   1.   9.   3.  -8.\n  -1.  -3.  -1. -10.   0.   4.  -7.   5. -15.   4.  11.  -4.   3.   9.\n  -6.  -9.  -3.   7.   5. -17.   3.   3.   1.  10.   1.   0.  -3.   0.\n   9.  -6.  -7.   6.   5.   8.  -1.   2.  11.   3.   4.   0.  -5.  -1.\n  -2. -15.   5. -15.   3.  -1.   8.  -1.  -1.  -1.  -3.   0.  14.   3.\n  -8. -17.  14.   3.   2.  22.  -8.  16.   1.  -1.   6.   4.   9.   0.\n   5.   6.  -3.   2.   7.   2.   4.  11.   1.   0.   3.   0. -10.  -5.\n  -7.   0.  -7.  -1.  -2.   5.   2.   5.   0.  11.   7.  16.   3.   4.\n -16.  -1.  -5.  -5.  -8.  -3.  12. -21.   1.   2.   5.  -4.   1.  -8.\n  -5.  -1.  -7.   1.  -7.  12.  10.   5.   7.   0.  -1.   2.  -3. -13.\n   3.  -2.  11.   5.   5.  -2.  -7.  -1.  -3. -11. -15.  -6.  -1.   6.\n   8.   5.   7.  -5. -10.  -6.   8.   1.   6.   8.   1.   6.  -1.  -2.\n  -1.  -2.   5.  -4. -12.  -6.  -3.  -5.   1.   3.  -8.   2.  -1.  12.\n  -4.   0.   5.  -4.   1. -13.  11.   2.  -2.  11.   3.   3.  -7.  -1.\n  -8.  -1.   4.   3.  -5.   3.   5.  -4.  -3.  -1.   2.   2.  -3.   5.\n  10.  -3.   4.  10.   2.  -3.   1.   4. -10.   4.  -7.   0.   3.  -3.\n  10.   6.   8.   2. -19. -11.   9.  -3.   4.   3.   4.   6.  -4.  -1.\n   7.   1.   2.  -4.  -5.  11.  -8.  10.   6.   0. -12.   4.  14.  -6.\n  -1.  16.   2.  -4.   5.   3.  -8.  -2. -10.   0. -12. -12.  14.  -6.\n   1.  -5.   4.  -4.  -3.   7.  -2.   8.   3.   3.  -3.   3.  -3. -16.\n  -9.   7.  -5.  -1.  12.   2. -10. -14. -21.  -6.   2.  -1. -11.   5.\n  -3.   3.  -5.  10.   4.   5.  12.   6.  -3.  -1.  -5.  -6.  -2.   4.\n   4.  11.   4.   3.   6.   5. -10.  -6.   5.   6.   4.   6.  -2.  -7.\n   6.  14.  17.   7.   1.  -8.  -5.   0.  -6.  -4.   1. -11.  -8.  -9.\n  -6.   0.  -8.   5.   2.  -6.  -9.  14.  -4.  12.  10.  -1.  -6.   2.\n  -7.  10.  -5.  -3.  11.  16.   0.   9.  -8.   5.   3.   0.  -4.   1.\n  -3. -11.   7.   6.  -4.  -3.   0.   5.  15.  -4.  -1.  -7.   5.  -2.\n   7.   9.  -9.  14.  -9.   5.  -3.   2.   6.   8.   5.   1.   9.   5.\n  -5.  14.  11.   0. -13.  14. -10.   5. -15.   4.  -5.   2.  -3.  -1.\n -13.   0.  12.  -8.  -2.   7.  14.   0.   5.  -2.   4.  -8.   1.   2.\n  -5.  -3.]\"\t1927.0(0.0)\t1974.0(0.02)\n\nIt is evident that in some scenarios, the new heuristic performs well, although there is still room for improvement. Based on the results, please analyze the following:\n1. In which cases does the new heuristic perform well, and in which cases is it not satisfactory?\n2. What factors contribute to these differences in performance?\n3. Is the updated heuristic most_weight_neighbors_3e17 better than previous heuristic most_weight_neighbors_d31b and is the previous evolution suggestion 1. Adjust the scaling_factor to a higher value, such as 1.0, to give more weight to future impacts, potentially improving decision-making for InsertNodeOperator.\n2. Add logic to consider negative weights more critically, potentially through an additional condition that prioritizes nodes that balance negative and positive impacts.\n3. Introduce an if-check to ensure nodes with negative weighted degrees are directed to the set that minimizes the negative impact.\n4. Incorporate logic from highest_weight_edge_ca02 to evaluate the potential delta for each node and choose based on the highest positive impact to the cut value. works?\n4. How can we further refine the new heuristics?\n\nWe believe that further refinement of the new heuristic is necessary.\n\nPlease analyze the results and provide one executable code adjustment suggestion (in word, not code).\n\nTo make sure the code adjustment suggestion is executable, here are some aspects to consider and you can provide more:\n1. This may involve changing default values or adopting dynamic values tailored to specific data.\n2. Implement checks to retain the original algorithm when the data does not favor heuristic updates.\n3. This includes aspects such as operation type, subject, location, and timing.\n4. Should we continue to deepen origin suggestion or weaken this suggestion: 1. Adjust the scaling_factor to a higher value, such as 1.0, to give more weight to future impacts, potentially improving decision-making for InsertNodeOperator.\n2. Add logic to consider negative weights more critically, potentially through an additional condition that prioritizes nodes that balance negative and positive impacts.\n3. Introduce an if-check to ensure nodes with negative weighted degrees are directed to the set that minimizes the negative impact.\n4. Incorporate logic from highest_weight_edge_ca02 to evaluate the potential delta for each node and choose based on the highest positive impact to the cut value..\n5. Explore insights and strategies from other successful heuristics.\n\nThe response format is very important. For better communication, please respond to me in this format:\n1. If you believe the code still need to be updated, please respond to me in this format:\n***refinement: \nresult analysis: xxx\ncode adjustment suggestion: xxx\n***\n2. If you think current heuristic are good enough, please respond to me in this format:\n***refinement: \nresult analysis: None\ncode adjustment suggestion: None\n***\nEnsure there is no other content inside the ***, and analysis outside *** is welcome.\nIf you have no information to provide, simply respond with ***None***."
            }
        ]
    },
    {
        "role": "assistant",
        "content": [
            {
                "type": "text",
                "text": "***refinement: \nresult analysis: The new heuristic most_weight_neighbors_3e17 performs well in instances where there is a higher average edge weight and less variability in edge weights (e.g., g2.mc, g6.mc). However, it underperforms in cases with more complex weight distributions and higher variability (e.g., g9.mc, g4.mc). The differences in performance are likely due to the scaling_factor's uniform application across varying distributions, which may not adequately address diverse graph characteristics.\ncode adjustment suggestion: Implement a dynamic scaling_factor that adjusts based on the standard deviation of edge weights. This allows for better tuning of future impacts in diverse graph instances, reducing the rigidity of a fixed scaling_factor.\n***"
            }
        ]
    },
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "Based on previous discuss, we get the heuristic most_weight_neighbors:\nNow, based on these suggestions:\n Implement a dynamic scaling_factor that adjusts based on the standard deviation of edge weights. This allows for better tuning of future impacts in diverse graph instances, reducing the rigidity of a fixed scaling_factor.\nUpdate the most_weight_neighbors_3e17.\n\nNow please implement it in python function most_weight_neighbors_2e6e.\n\nTo implement this heuristic function in python, please note:\n1. We can assume that Solution and all Operators are imported.\n2. The operator type must be defined previously, do not create a new one in your code.\n3. Never modify the data in global, state and algorithm data.\n4. All hyper parameters in kwargs should be set a default value.\n5. Any reasonable partial solution may be used as input, such as an empty solution.\n6. Comments in the code are very important. They must clearly explain which hyper-parameters are required by the algorithm (i.e., the parameters in **kwargs), how the algorithm proceeds, and under what circumstances it will not return any operator or will return an empty operator. We hope that people can understand the principles and workflow of the algorithm clearly just by reading the comments, without needing to look at the code.\n7. The name of function must be most_weight_neighbors_2e6e. The TargetOperatorType should be replaced by real returned operator type.\n8. No any omissions or placeholders, I'm just going to use the code.\n9. For the algorithm to update the algorithm_data, do not modify directly \"algorithm_data[\"abc\"] = 123\", we should return operator, {\"abc\": 123}.For algorithm that do not update them, just return operator, {}\n10. For the circumstances that algorithm return empty operator, please return None, {}.\n11. Do not forget to check whether the content is empty when calculating the min/max/average/std/..., such as when using min([distance[node][other] for other in nodes if other != node]).\n12. Make the result must be valid.\n\nFor max_cut problem, take about following:\n1. Make the result must be valid, such as: no re-inserted nodes, never insert a node in set A, which are already in set B, etc.\n2. The graph is undirected, meaning each edge connecting two nodes is treated as a single, bidirectional edge.\n\n\nThe problem will work on following data.\nPlease try to understand the data and generate based on them, especially setting the default value for hyper-parameters:\nAll data is possible\n\nThe response format is very important. For better communication, please respond to me in this format:\n***python_code:\nfrom src.problems.max_cut.components import *\nimport other_library (if necessary)\ndef most_weight_neighbors_2e6e(global_data: dict, state_data: dict, algorithm_data: dict, get_state_data_function: callable, **kwargs) -> tuple[TargetOperatorType, dict]:\n    \"\"\" Description for this heuristic algorithm.\n\n    Args:\n        global_data (dict): The global data dict containing the global data. In this algorithm, the following items are necessary:\n            - necessary_key (value type): description\n            - necessary_key (value type): description\n            ...\n        state_data (dict): The state dictionary containing the current state information. In this algorithm, the following items are necessary:\n            - necessary_key (value type): description\n            - necessary_key (value type): description\n            ...\n        (Optional and can be omitted if no algorithm data) algorithm_data (dict): The algorithm dictionary for current algorithm only. In this algorithm, the following items are necessary:\n            - necessary_key (value type): description\n            - necessary_key (value type): description\n            ...\n        (Optional and can be omitted if no used) get_state_data_function (callable): The function receives the new solution as input and return the state dictionary for new solution, and it will not modify the origin solution.\n        (Optional and can be omitted if no hyper parameters data) introduction for hyper parameters in kwargs if used.\n\n    Returns:\n        Description of this operator.\n        Description of updated algorithm data.\n    \"\"\"\n    xxx\nEnsure there is no other content inside the ***, and analysis outside *** is welcome.\nIf you have no information to provide, simply respond with ***None***.\n"
            }
        ]
    },
    {
        "role": "assistant",
        "content": [
            {
                "type": "text",
                "text": "***python_code:\nfrom src.problems.max_cut.components import Solution, InsertNodeOperator, SwapOperator\nimport numpy as np\n\ndef most_weight_neighbors_2e6e(global_data: dict, state_data: dict, algorithm_data: dict, get_state_data_function: callable, swap_frequency: int = 5, **kwargs) -> tuple[InsertNodeOperator, dict]:\n    \"\"\" Heuristic algorithm to optimize the MaxCut problem by adjusting the scaling factor dynamically based on edge weight variability.\n\n    Args:\n        global_data (dict): The global data dict containing the global data. In this algorithm, the following items are necessary:\n            - \"weight_matrix\" (numpy.ndarray): A 2D array representing the weight between nodes.\n        state_data (dict): The state dictionary containing the current state information. In this algorithm, the following items are necessary:\n            - \"current_solution\" (Solution): The current solution of the MaxCut problem.\n            - \"unselected_nodes\" (set[int]): The set of unselected nodes.\n        algorithm_data (dict): The algorithm dictionary for the current algorithm only. In this algorithm, the following items are necessary:\n            - \"operation_count\" (int): The number of operations performed so far. Default is 0.\n            - \"sorted_nodes\" (list of tuples): A sorted list of (node, future_impact) in descending order. Default is empty.\n        get_state_data_function (callable): The function receives the new solution as input and returns the state dictionary for the new solution. It does not modify the original solution.\n        kwargs: \n            - swap_frequency (int, optional): Frequency (in terms of operations) at which swap operations are considered. Defaults to 5.\n\n    Returns:\n        InsertNodeOperator or SwapOperator: The operator to modify the solution.\n        dict: Updated algorithm data with the sorted list of nodes and operation count.\n    \"\"\"\n    \n    # Extract necessary data\n    weight_matrix = global_data[\"weight_matrix\"]\n    current_solution = state_data[\"current_solution\"]\n    unselected_nodes = state_data[\"unselected_nodes\"]\n    set_a, set_b = current_solution.set_a, current_solution.set_b\n    operation_count = algorithm_data.get(\"operation_count\", 0)\n    sorted_nodes = algorithm_data.get(\"sorted_nodes\", [])\n\n    # Calculate dynamic scaling factor based on standard deviation of edge weights\n    std_dev = np.std(weight_matrix)\n    scaling_factor = 1.0 / (1 + std_dev)  # Dynamic adjustment\n\n    # Step 1: Perform a swap operation periodically\n    if operation_count % swap_frequency == 0 and set_a and set_b:\n        weight_to_a = weight_matrix[:, list(set_a)].sum(axis=1)\n        weight_to_b = weight_matrix[:, list(set_b)].sum(axis=1)\n        best_increase = float(\"-inf\")\n        best_pair = None\n\n        # Evaluate all possible swaps\n        for i in set_a:\n            for j in set_b:\n                delta = weight_to_a[i] - weight_to_a[j] + weight_to_b[j] - weight_to_b[i]\n                if weight_matrix[i, j] != 0:  # Adjust for the edge between i and j if it exists\n                    delta += 2 * weight_matrix[i, j]\n                if delta > best_increase:\n                    best_increase = delta\n                    best_pair = (i, j)\n\n        # If a beneficial swap is found, return the SwapOperator\n        if best_pair and best_increase > 0:\n            return SwapOperator(nodes=list(best_pair)), {\"operation_count\": operation_count + 1}\n\n    # Step 2: Sort unselected nodes based on future impact if not already sorted\n    if not sorted_nodes:\n        # Calculate the future impact for each unselected node\n        sorted_nodes = sorted(\n            [(node, sum(abs(weight_matrix[node][other]) for other in range(len(weight_matrix)))) for node in unselected_nodes],\n            key=lambda x: x[1],\n            reverse=True\n        )\n    else:\n        # Filter out nodes that have been selected since the last run\n        sorted_nodes = [\n            (node, future_impact) for (node, future_impact) in sorted_nodes\n            if node in unselected_nodes\n        ]\n\n    # Step 3: Select the best unselected node based on both immediate and future impacts\n    if not sorted_nodes:\n        return None, {}\n\n    # Extract the best node and its future impact\n    best_node, future_impact = sorted_nodes.pop(0)\n\n    # Calculate the potential increase in cut value for adding the node to each set\n    potential_increase_a = sum(weight_matrix[best_node][other] for other in set_b)\n    potential_increase_b = sum(weight_matrix[best_node][other] for other in set_a)\n\n    # Adjust the potential increases by adding the scaled future impact\n    adjusted_increase_a = potential_increase_a + scaling_factor * future_impact\n    adjusted_increase_b = potential_increase_b + scaling_factor * future_impact\n\n    # Consider negative weights critically and choose the set that maximizes the cut value\n    if sum(weight_matrix[best_node]) < 0:\n        target_set = \"A\" if adjusted_increase_a > adjusted_increase_b else \"B\"\n    else:\n        # Choose the set that gives the maximum adjusted increase in cut value\n        target_set = \"A\" if adjusted_increase_a >= adjusted_increase_b else \"B\"\n\n    # Create the operator to insert the selected node into the chosen set\n    operator = InsertNodeOperator(best_node, target_set)\n\n    # Return the operator and the updated algorithm data\n    return operator, {\"sorted_nodes\": sorted_nodes, \"operation_count\": operation_count + 1}\n***"
            }
        ]
    },
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "To ensure the stable of heuristics, we adjust the code to:\nfrom src.problems.max_cut.components import Solution, InsertNodeOperator, SwapOperator\nimport numpy as np\n\ndef most_weight_neighbors_2e6e(global_data: dict, state_data: dict, algorithm_data: dict, get_state_data_function: callable, swap_frequency: int = 5, **kwargs) -> tuple[InsertNodeOperator, dict]:\n    \"\"\" Heuristic algorithm to optimize the MaxCut problem by adjusting the scaling factor dynamically based on edge weight variability.\n\n    Args:\n        global_data (dict): The global data dict containing the global data. In this algorithm, the following items are necessary:\n            - \"weight_matrix\" (numpy.ndarray): A 2D array representing the weight between nodes.\n        state_data (dict): The state dictionary containing the current state information. In this algorithm, the following items are necessary:\n            - \"current_solution\" (Solution): The current solution of the MaxCut problem.\n            - \"unselected_nodes\" (set[int]): The set of unselected nodes.\n        algorithm_data (dict): The algorithm dictionary for the current algorithm only. In this algorithm, the following items are necessary:\n            - \"operation_count\" (int): The number of operations performed so far. Default is 0.\n            - \"sorted_nodes\" (list of tuples): A sorted list of (node, future_impact) in descending order. Default is empty.\n        get_state_data_function (callable): The function receives the new solution as input and returns the state dictionary for the new solution. It does not modify the original solution.\n        kwargs: \n            - swap_frequency (int, optional): Frequency (in terms of operations) at which swap operations are considered. Defaults to 5.\n\n    Returns:\n        InsertNodeOperator or SwapOperator: The operator to modify the solution.\n        dict: Updated algorithm data with the sorted list of nodes and operation count.\n    \"\"\"\n    \n    # Extract necessary data\n    weight_matrix = global_data[\"weight_matrix\"]\n    current_solution = state_data[\"current_solution\"]\n    unselected_nodes = state_data[\"unselected_nodes\"]\n    set_a, set_b = current_solution.set_a, current_solution.set_b\n    operation_count = algorithm_data.get(\"operation_count\", 0)\n    sorted_nodes = algorithm_data.get(\"sorted_nodes\", [])\n\n    # Calculate dynamic scaling factor based on standard deviation of edge weights\n    std_dev = np.std(weight_matrix)\n    scaling_factor = 1.0 / (1 + std_dev)  # Dynamic adjustment\n\n    # Step 1: Perform a swap operation periodically\n    if operation_count % swap_frequency == 0 and set_a and set_b:\n        weight_to_a = weight_matrix[:, list(set_a)].sum(axis=1)\n        weight_to_b = weight_matrix[:, list(set_b)].sum(axis=1)\n        best_increase = float(\"-inf\")\n        best_pair = None\n\n        # Evaluate all possible swaps\n        for i in set_a:\n            for j in set_b:\n                delta = weight_to_a[i] - weight_to_a[j] + weight_to_b[j] - weight_to_b[i]\n                if weight_matrix[i, j] != 0:  # Adjust for the edge between i and j if it exists\n                    delta += 2 * weight_matrix[i, j]\n                if delta > best_increase:\n                    best_increase = delta\n                    best_pair = (i, j)\n\n        # If a beneficial swap is found, return the SwapOperator\n        if best_pair and best_increase > 0:\n            return SwapOperator(nodes=list(best_pair)), {\"operation_count\": operation_count + 1}\n\n    # Step 2: Sort unselected nodes based on future impact if not already sorted\n    if not sorted_nodes:\n        # Calculate the future impact for each unselected node\n        sorted_nodes = sorted(\n            [(node, sum(abs(weight_matrix[node][other]) for other in range(len(weight_matrix)))) for node in unselected_nodes],\n            key=lambda x: x[1],\n            reverse=True\n        )\n    else:\n        # Filter out nodes that have been selected since the last run\n        sorted_nodes = [\n            (node, future_impact) for (node, future_impact) in sorted_nodes\n            if node in unselected_nodes\n        ]\n\n    # Step 3: Select the best unselected node based on both immediate and future impacts\n    if not sorted_nodes:\n        return None, {}\n\n    # Extract the best node and its future impact\n    best_node, future_impact = sorted_nodes.pop(0)\n\n    # Calculate the potential increase in cut value for adding the node to each set\n    potential_increase_a = sum(weight_matrix[best_node][other] for other in set_b)\n    potential_increase_b = sum(weight_matrix[best_node][other] for other in set_a)\n\n    # Adjust the potential increases by adding the scaled future impact\n    adjusted_increase_a = potential_increase_a + scaling_factor * future_impact\n    adjusted_increase_b = potential_increase_b + scaling_factor * future_impact\n\n    # Consider negative weights critically and choose the set that maximizes the cut value\n    if sum(weight_matrix[best_node]) < 0:\n        target_set = \"A\" if adjusted_increase_a > adjusted_increase_b else \"B\"\n    else:\n        # Choose the set that gives the maximum adjusted increase in cut value\n        target_set = \"A\" if adjusted_increase_a >= adjusted_increase_b else \"B\"\n\n    # Create the operator to insert the selected node into the chosen set\n    operator = InsertNodeOperator(best_node, target_set)\n\n    # Return the operator and the updated algorithm data\n    return operator, {\"sorted_nodes\": sorted_nodes, \"operation_count\": operation_count + 1}"
            }
        ]
    },
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "To assess the performance of the evolved heuristics, I have tested both the original heuristic most_weight_neighbors_d31b and the evolved heuristic most_weight_neighbors_2e6e.\nThe following table presents a comparative analysis of these heuristics. The first 8 columns represent the features of the testing data, while the remaining columns show the results.\nA higher result indicates better performance, and the improvement ratio (in brackets, positive means to improve and negative means decline) reflects the enhancement relative to most_weight_neighbors_d31b.\n\ndata_name\taverage_node_degree\tedge_density\taverage_edge_weight\tmax_edge_weight\tmin_edge_weight\tstandard_deviation_edge_weight\tweighted_degree_distribution\tmost_weight_neighbors_d31b\tmost_weight_neighbors_3e17\tmost_weight_neighbors_2e6e\ng9.mc\t47.94\t0.06\t-0.003337505214851898\t1.0\t-1.0\t0.9999944305139609\t\"[-10.  -3. -13.  -4.  -6.   2.  -9.   3.  -1.  -6.   4.   3.   1.   1.\n   5.  -7. -12.  -4. -16.   1.   8.  12. -11.   0.  -3.   3.  -4.  -2.\n  -2.   7.  -8.  11.  -6.  -4.   0.   5.  -2.  10.  -8.   3.   8.   4.\n   0.   8. -16. -13.  -4.  -5.  -5.  11.   1.   9.  -1.  -3. -11.   6.\n  -2.  10.  -4.  11.  -9.   4.   5.  13.   6. -18.  14.  -6.   7.  -2.\n   9.   1.  -6.   4.   3.  -1.   3.  -6.   2.   3. -12.  -8.  -8. -10.\n  -5.  -2.  13. -11.  -2.   1.   4. -12.   2. -10.   1.   7. -11.   6.\n  -4.   5.   0.  -3.  -7. -13.  -5.   1.   1.  14.   4.   3.   1.  -7.\n  -4.  -5.  -6.  -4.   0.  -6.   2.  -4.  -4.  13.   9.  -6.   3.   3.\n  -3.  -2.   7. -10. -10.   1.  -9.   1.  -1.  10.  -1.  -2.   3.  -6.\n   4.   1.   3.  -8.   4.   0.  -6.   2. -20.  19.   1.  -1.  -5.   6.\n   7.   2.   9.  11.  -3.   7.  -2.  -6.  -6.  -1.   4.   5.   5.   6.\n  -5.  -3.  -2.   0.  -6.   3.   9. -15.  -8.  15.  -5.   6.  -5.   4.\n   0.  -3.   6.  10.   3. -12.  -6.   6.   1.   1.  -9.  -3.  -3.   1.\n   3.  -2.   0.  -3.  -4.   1.  -4.   7. -11.   4. -10.   1.   0.  -6.\n  -1.  17. -11.  12.   5.  11.   1.  -5.   6. -13.   9.  -7.  -8.   5.\n  -2.   3.  -3.   1.   4.  -9.   5.   4.  -2. -13.  -2.  -6.  14.  -5.\n  -8.   1. -21.  -3.  -2.  -2.   2.  -5.   1.  -1.   5.  -9.  -1. -13.\n   1.  -5.   5.  -2.   0.   0.   9.  -9.   8.   8.  -1.   4.  -1. -13.\n  -1.   3.  -9.   0.  -2. -10. -11.  -1.   2.   9.  -1.  -2.   2.  11.\n   4.  -9.  10.  18.  -6.   2.  -3.  -2.  -6.  -4.  10.   1.   3.   0.\n   4.   2.   0.   2.  -9.  -1.   2.  -8.  -3. -12.   7.  -1.   1.   7.\n   5.   2.  -2.  -2.   2.  -4.  -4.   0.  -2.  -1.  -5.   1.  11.  10.\n   3.  -6.  -4.   3.  -3.  -4.   1.  -3.   0.  -5.  -4.  -4.  -6.  -9.\n  -5.  11.  -1.   7.   0.   7.   3.  -1.  -2.  -4.  -3.   1.  -3.   0.\n   5.   1.   5. -12.  -8.  12.   1.   3.   1.   2.  11.  -2.  10.   1.\n  -6.  -1.  16.  10.  -8.  10.  -3.  -4.  -2.  13.  -6.  -4.   1. -10.\n   2.   2.  -2.   3.  -9.   8.   0.   4.  -2.  -1.  -4.   5.   0.   8.\n  -7.   1. -11.   0.   2.   5.   1.   0.  -8.  -7.   1.  -6.   6. -15.\n  -1.  -1. -15.  12.  18.  -5.  -6.   3.  13. -12.   0.  13.   2.   7.\n  -8.  -4. -20.  -4.  -7.  -5.  17.  -8.   0.  -1.   2.  -3.  -5.  13.\n   0.  -5.  -3.  -1.   6.   2.   2.  -6.  -3.   5. -12.   7.   5.   9.\n   6.  -1.   3.   1. -12.  -8.  -8.  -1.  -2.  -6.  -4.  -5.  -3.   6.\n  -1.   7.  14.   2.  -2.   8.  -4.   7.   0.   1.  -6.   7.  -5.   3.\n -15.   7.   2.   2. -12. -11.   0.  -1.   7.   0.  -5.   7.  -3.  -2.\n   8.   3.   1.  -6.   2.   4.  17.   9.  -3.  11.  -6.  -1.  -3.  11.\n  -5.  -9.  -3.  -4.  -2.  -4.   9.   9.   1.  -3.   1.  -4. -10.  -3.\n -12.  -1.  -8.  -3.   6. -11.   3.  -3.  10.  -1.   2.   2.   0.  -9.\n  -6.   6.  -4. -10.  -1.   5.  -6.  -6.  -6.  -4. -10.   8.  -4.   6.\n   1.  -7.   0.  26.   6.  -7.  -3.   1.   0.  -3.   6.   1.  11.   7.\n  -2.   5.  -9.  -6.  13.  -2.   3.   3.   2.   0.  -2.   7.  14.   3.\n  11.  -1.  -1.  -4.  -6.  -1.   4.  -5.  -5.   1.   1.  -1.  10.   4.\n   4.   0.  -4. -10.   3.   2.  -4.  -2.   0.  17.   3. -15.   4.   6.\n   7.  -6.  -2.   2.  -5. -14.  -6.   1.   9.   2. -18.   6. -14.  -7.\n  11.  11.  -1. -13.  -1.  -4.   1.   7.  -4.   1.  -4.  -1.   4.   2.\n -11.  12.  -7.   5.  -3.  -6.  -6.   7.  -4.   1.  -3.  -7.  10.   8.\n   0.   4.  -5.   6.  -2.  10.  -8.  -1.   2.   2.  -2.  -7.  -1.   2.\n   5.   7.   3.  10.   4.  -3.  -7.   0.  16.  -6.  -6.  -4.  -2.  -6.\n   3.   9.  -8.  -8.  -3.  -4.   0.  -3.   0.   2.  -2.   6.  -4.  -3.\n   1.   7.  -2.   4.  -2.   0.  -6.   8.  -1.   2.  -1.  12.  14.  -6.\n  -1.  14.   7.  -5.   4.   3.  -3.   3.  -3.  -9.   4. -11.   5.  -3.\n   3.   0.  -7.  -4.  14.  -7.  -3.   3.  -3.   7.  -4. -11.  -3.   2.\n  -3.  -8.   2.  -1.  -9.   6.  -1.  -4.   3.  -7.  -5.  17.   1.  -2.\n  15.  -1.  -3.  -2. -10.  -4.   1.   9.   3.  -3.   5.   4.  -4.   0.\n  -2.   9.   2.  -1.   9. -13.  -2.   3.  -4.  -4.  -1.  -6.   2.   4.\n  20.  -1. -11.   8.   1.   6. -10.   5.  10.   3.  -1.   5.   1.   8.\n  -4.   1.  13.  -3.   6.  -3.   1.   9.  -4.   7.   0.  -5.  -6.  -3.\n   8.   9.]\"\t1859.0\t1843.0(-0.01)\t1843.0(-0.01)\ng10.mc\t47.94\t0.06\t-0.008343763037129746\t1.0\t-1.0\t0.9999651902033291\t\"[  6.  -3.  -7.  -8.  12.  -3.  -1.   6.   2.   3.   1.  -9.   1.   7.\n -11.   2.   4.   6. -10.   5.  -3.   7.   7.   5.   5.   7.  -9.   4.\n   3.  -2.  -5.  -1.  10.  15.  -6.  -7.  -1.   1.  10.   3.   4. -10.\n  -1.  -2.   3.  -5.   9.   1.   4.  -1.  11.   4.  -3.   4.  -9.  -2.\n  -7.   0.   3.   5.  -6.  -4.   3.   7.  -2.  -7.   1.  -5.   3.   4.\n   9.  -1.  -2.  -1.  -4.  -4. -10.   8.   1.  -6.  -1.   0.   2. -13.\n  11.   8.  -2.   2. -11. -14.   0.   5.  -5.  -8.  -3.  -2.  10.  -1.\n   2.   1.  -4.  -7.  -8.  -5.  -5.   7.   5. -12.  12.   4.   1.  -8.\n   2.  -3.  -3.   5.  13.   2.   0.  13.   4. -14. -10.  -9.   0.   0.\n -12.  -6.  -9.  -9.  10.  -3.  -2. -16.  -5.   2.   8.   9.   4.  -5.\n   5.  16.   1. -11.   7.   0.  10.  -1.  16.   5.   6.   2.  -6.  -7.\n   6.  10.   2.   1. -11.   6.  -3.   6.   3.   5.  -2.   0. -12.  -3.\n -12.  -6.   5.   3.  -6.  -9.  -5.  -8.  -4.   2.   2.   4.  -2.   6.\n -16.  -1.   8.  -5.   2.  -4.   0.  -6.   5.  13. -11.   2.  -4.   3.\n  -2.  14.  -7.  -4.   3.   4.   7.   1.   2.  12.  -9.   7.  -4.  -5.\n  -9.   3. -12.   3.   0.   3.  -3.   1.  -7.  -5.   1.   4.  -5.   3.\n   2.  10.   3.   2.  -7.   7.  10.   2.  -3.   2.  -1.   2.  -5. -10.\n  -2.   4.  10.  18.   0.  -4.   4.   0.   0.   2.  -3.  -5.  -6.   0.\n  -1.   5.  16.  13.  -5. -13.   3.  -7.  -6.  -2.  -1. -10.  -5.  -8.\n  -8.  -3.  -1.  -7.   5.  -1.  -2.   2.   8.   3.   1.   1.  11.   4.\n  -4.   3.  11.  10.  17.   9.   6.   2.  -2.   3.   2.  -5.   4.  -2.\n  -5.  -3.   5.  -5.   3.  -2.  -3.  -2. -12.   5.  -1. -20.  -2.  -7.\n -19.  -6.  -3.  -8.  -8.  -1.   4.  -2. -11. -11. -10.   5.  -3.   3.\n   5.   2.   0.  -6.   4.   6.   4.  -3.  12.  11.  -6.   1.   8.   5.\n   3.   8.  -9.  -9.   2.  -8.   4.  -1.  -7.  -2.   2.  -9.  -7.   3.\n  11.  10.  -7.  -2.  -6.   0.  10.  -3.   1.   3.   4.  16. -12.   5.\n -13.  -6. -10.  -6.  -1.  -5. -11.  -7.   0.  -1. -11.  11.  -1.   6.\n   7. -18. -14.  -4.   6.   3.  -1.  -2.   1. -10.  -5.  -9.  -4.  12.\n   2.   0.   0.  -2.  18.   1.  -8.   2.  16.  -3.  -3.  -5.  10.  -9.\n   2.   0.   1.  -5.   4.  -1.  -9.  -4.  -7.  10.  15.   7.  -2.  -9.\n   5.   2.  -6.  -5. -12.  12.   9.   3.  -4.  -2.   1.   2. -10.   6.\n  -7.   9.   0.  -1.   0.  -4.   5.  -9.   2.  -8.   3.   3.  -8.   7.\n  -4.  -1.  -4.   0.   0.   5.  -4.  11.   4.   2. -15.   1.  -9.  -1.\n  -5.  11.   5.   1.  -6.  18.   1.  -6.  -6.   7.  -8.  -3.   3.  -1.\n  -1. -11.   1.  -4.  -7.  -3.  -6. -10.   6.   5.  11.  -7.  11.   2.\n  -9. -12.   0.  11.   3.   5.  13.   4.   6.   4.  -5.  -7.   1.  -7.\n  10.   0. -16.  -6.  -4.   2.   7.  -4. -10.   0. -10.   4.   5.  -6.\n   5.  -3.   3.   4.  -1.  -5.  10.  -3.  -3.  -5.  16.   4.   0.  -1.\n   7.   5. -10.  -3.  -2.  -4.   7.   3.   4.   0.   2.   0.   2.   9.\n   0.   4.  -3.  -7.  -2.   3.   3.  -4.  -2.  -4.   6.  -1.  -4.  -1.\n   5.  13.  -9.  -2.  16.  -5.  -3.   3.   4. -22.   6. -10.  -9.  -7.\n   4.  -6.   0.   4.  -8.  -7.  -5.   3.   4. -12.  -3.  -8.  -6.   8.\n  -9.   0.   5.   2. -12.   3.  -3.   2.  -4.  -5.  -9.  -1.   2.  16.\n -17.  -2.  -2.  -2.   4.   2.  -7.  -8.   7. -11. -10. -14.   4.   3.\n   2.   7.   0. -10.  -3.  -1.   7.  18. -13.   2.  -2.  -6.  -4.   3.\n  -3.   2.   3.  -1. -10.  -3.  -2. -15.   5.  -8.  -3. -10.   1.  -7.\n   0.   3.   3.   5.  -6.   6.  -2.   1.   2.  -4.   8.   4.  -3.   3.\n  -1.  -8.  -1.  -5.  -1.   7.   6.  11.   2.   3.  -5.  -3.  -1.  -2.\n  -1.  -5.  -4.  -1.   4.  10.  -6.  -2.  -7.  15.   0.  -5.  -6.  -6.\n   9.   0.  -7.   4.   5. -10.  10.   8.   6.   3.  -1.   3.  -1.  12.\n  -5.   4. -10.  12.  -2.  -1.   1.  -3.   5.  -4.  -1.  -2. -18.  -1.\n  -6.  -8.   2.   1.   3.   8.   3.  -7.  -4.  -3.  -6.   7.   2.   3.\n   2. -15.   5.  -1.   1.   5.  11.   2.  -1.   1.   0.   8. -17.   8.\n   7.  -3.  -7.   0.   5.   0.  -6.  -7.  -5. -13.  -5.  11.  -8.   0.\n  -2.  -9.  -7.  -9.   2.   4.  13.   7.   6.  -4.  10. -11.  -2. -12.\n  -4.   1.   9. -18.   0.   9.   4. -11.  -6.  -5.  -9.   2.  -6.  -2.\n   2.  -8.  14.  -1.  -1.   9.  -3.  -5.   3.  -4.   0.   1.  10.   4.\n   4.  -5.]\"\t1714.0\t1808.0(0.05)\t1808.0(0.05)\ng2.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[42. 46. 46. 45. 59. 47. 50. 58. 48. 54. 44. 36. 49. 41. 42. 46. 49. 45.\n 50. 42. 45. 47. 53. 39. 38. 61. 54. 52. 55. 52. 52. 44. 57. 46. 53. 57.\n 39. 52. 48. 54. 45. 57. 38. 45. 36. 46. 41. 46. 58. 50. 43. 56. 47. 41.\n 51. 58. 43. 54. 40. 41. 40. 42. 49. 47. 50. 66. 44. 54. 61. 50. 61. 59.\n 48. 52. 52. 50. 44. 47. 49. 49. 50. 45. 43. 44. 52. 45. 51. 47. 45. 53.\n 42. 44. 50. 47. 50. 47. 57. 40. 53. 50. 64. 66. 49. 57. 46. 45. 47. 43.\n 48. 43. 52. 51. 45. 50. 56. 61. 49. 44. 47. 45. 39. 57. 49. 45. 60. 39.\n 48. 51. 45. 43. 48. 47. 48. 57. 45. 47. 52. 53. 48. 49. 38. 40. 54. 52.\n 45. 47. 48. 52. 42. 48. 49. 50. 49. 52. 52. 48. 49. 48. 53. 41. 48. 50.\n 42. 55. 54. 46. 42. 42. 50. 56. 45. 52. 45. 65. 54. 43. 44. 45. 55. 48.\n 49. 43. 51. 45. 37. 42. 45. 59. 46. 52. 45. 42. 34. 55. 52. 41. 52. 52.\n 40. 51. 53. 55. 51. 52. 52. 40. 43. 51. 42. 43. 49. 47. 45. 63. 50. 65.\n 28. 49. 47. 50. 35. 55. 39. 38. 38. 49. 48. 46. 60. 47. 51. 59. 56. 41.\n 55. 41. 53. 47. 38. 43. 44. 46. 46. 45. 31. 34. 52. 38. 44. 54. 48. 45.\n 45. 43. 51. 44. 51. 58. 41. 41. 50. 54. 41. 47. 40. 48. 61. 54. 40. 36.\n 36. 45. 40. 37. 56. 42. 58. 51. 49. 54. 41. 48. 43. 42. 56. 55. 53. 41.\n 62. 48. 52. 45. 51. 48. 58. 46. 44. 51. 40. 39. 51. 54. 58. 43. 46. 47.\n 48. 49. 53. 50. 52. 55. 58. 62. 47. 53. 51. 51. 45. 54. 38. 54. 43. 55.\n 47. 40. 43. 48. 55. 45. 47. 41. 61. 52. 43. 48. 45. 57. 48. 40. 47. 57.\n 44. 47. 46. 41. 43. 48. 57. 57. 41. 54. 56. 45. 58. 42. 51. 47. 48. 53.\n 63. 52. 41. 35. 44. 48. 56. 44. 65. 54. 46. 43. 50. 52. 36. 39. 41. 38.\n 47. 48. 47. 46. 43. 40. 46. 49. 51. 50. 46. 44. 54. 51. 36. 47. 48. 43.\n 43. 44. 44. 39. 43. 30. 49. 51. 48. 53. 43. 43. 43. 41. 51. 47. 56. 41.\n 57. 52. 54. 65. 56. 49. 50. 47. 41. 37. 43. 48. 42. 56. 36. 48. 46. 60.\n 41. 49. 42. 41. 54. 52. 55. 43. 46. 55. 53. 57. 53. 43. 55. 54. 42. 59.\n 29. 42. 58. 40. 52. 50. 60. 53. 43. 37. 43. 45. 50. 46. 36. 45. 49. 39.\n 46. 48. 44. 53. 36. 49. 53. 45. 45. 55. 50. 40. 36. 55. 36. 58. 48. 45.\n 46. 39. 50. 53. 46. 52. 44. 46. 65. 40. 61. 60. 48. 52. 51. 44. 50. 49.\n 41. 39. 43. 48. 49. 52. 42. 54. 46. 42. 53. 43. 57. 40. 48. 56. 49. 38.\n 59. 50. 54. 44. 47. 53. 42. 55. 55. 51. 39. 52. 54. 47. 52. 39. 57. 49.\n 48. 45. 36. 50. 44. 42. 42. 53. 54. 39. 45. 55. 52. 34. 57. 53. 52. 43.\n 62. 54. 48. 56. 50. 53. 40. 33. 43. 45. 47. 48. 39. 56. 39. 50. 52. 43.\n 52. 61. 54. 46. 45. 47. 49. 41. 53. 58. 39. 47. 40. 54. 45. 53. 51. 54.\n 44. 58. 54. 49. 51. 45. 59. 45. 49. 54. 48. 37. 58. 37. 49. 39. 53. 42.\n 46. 50. 50. 41. 53. 44. 40. 43. 38. 42. 40. 44. 59. 51. 56. 40. 45. 45.\n 58. 43. 48. 56. 49. 55. 44. 46. 50. 41. 47. 41. 46. 47. 47. 58. 49. 42.\n 49. 45. 49. 48. 43. 64. 52. 46. 47. 54. 49. 51. 43. 54. 47. 57. 45. 37.\n 50. 44. 41. 60. 49. 46. 41. 50. 41. 33. 48. 45. 46. 53. 50. 57. 56. 47.\n 59. 40. 48. 49. 42. 47. 36. 42. 46. 45. 42. 54. 57. 49. 44. 54. 54. 52.\n 50. 50. 39. 36. 55. 41. 48. 55. 46. 52. 37. 42. 58. 52. 47. 58. 55. 34.\n 42. 49. 46. 52. 39. 44. 48. 52. 56. 39. 41. 60. 49. 42. 44. 50. 47. 60.\n 45. 54. 34. 53. 40. 40. 52. 49. 54. 44. 47. 43. 45. 51. 33. 55. 47. 56.\n 44. 47. 51. 50. 48. 46. 54. 47. 37. 47. 49. 46. 45. 52. 38. 49. 56. 47.\n 44. 63. 40. 61. 46. 46. 51. 63. 62. 45. 45. 37. 50. 52. 48. 42. 51. 51.\n 51. 49. 45. 53. 35. 44. 33. 48.]\"\t11420.0\t11433.0(0.0)\t11433.0(0.0)\ng4.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[44. 57. 41. 54. 44. 38. 45. 47. 45. 44. 48. 43. 51. 61. 45. 57. 44. 42.\n 56. 41. 60. 58. 49. 44. 37. 47. 48. 36. 56. 49. 50. 61. 46. 56. 44. 53.\n 36. 46. 54. 47. 50. 46. 42. 44. 54. 45. 50. 65. 37. 49. 41. 39. 33. 51.\n 45. 52. 60. 40. 48. 53. 35. 44. 39. 53. 44. 52. 50. 48. 47. 44. 51. 55.\n 40. 44. 53. 51. 43. 46. 46. 61. 48. 48. 54. 46. 49. 50. 61. 51. 40. 45.\n 46. 42. 48. 40. 45. 43. 53. 48. 48. 53. 44. 53. 39. 47. 49. 43. 45. 44.\n 46. 45. 37. 51. 58. 47. 44. 42. 32. 50. 48. 42. 40. 65. 57. 62. 43. 53.\n 45. 52. 47. 56. 46. 47. 51. 51. 55. 46. 49. 48. 35. 50. 50. 61. 39. 54.\n 42. 44. 58. 48. 50. 53. 57. 63. 57. 42. 39. 46. 53. 59. 55. 39. 58. 46.\n 36. 33. 48. 51. 61. 48. 47. 53. 56. 46. 60. 53. 45. 51. 46. 55. 45. 40.\n 41. 60. 50. 31. 44. 56. 49. 46. 50. 58. 57. 49. 41. 39. 47. 51. 55. 48.\n 52. 51. 52. 39. 32. 57. 43. 34. 52. 49. 50. 46. 49. 43. 51. 54. 33. 45.\n 57. 57. 54. 47. 45. 45. 58. 59. 46. 41. 41. 53. 44. 59. 43. 42. 50. 47.\n 50. 42. 46. 59. 54. 57. 45. 45. 42. 44. 52. 57. 57. 59. 37. 57. 47. 45.\n 51. 37. 51. 38. 68. 40. 53. 51. 62. 46. 55. 46. 55. 35. 41. 53. 45. 46.\n 44. 50. 61. 41. 46. 55. 39. 48. 52. 45. 40. 41. 52. 56. 56. 48. 49. 46.\n 50. 50. 56. 45. 49. 38. 46. 50. 52. 42. 39. 45. 42. 58. 47. 54. 41. 37.\n 49. 43. 53. 38. 46. 52. 46. 46. 52. 46. 44. 43. 55. 51. 53. 46. 49. 50.\n 42. 45. 41. 48. 45. 55. 38. 51. 52. 44. 48. 29. 53. 59. 45. 45. 48. 57.\n 37. 51. 36. 40. 55. 37. 39. 54. 33. 57. 49. 48. 48. 54. 53. 43. 49. 34.\n 45. 50. 50. 51. 50. 53. 40. 48. 60. 46. 47. 44. 42. 51. 48. 46. 49. 52.\n 44. 54. 44. 49. 47. 56. 46. 52. 56. 63. 48. 53. 50. 50. 43. 49. 51. 60.\n 44. 49. 51. 50. 52. 51. 37. 48. 48. 57. 41. 59. 51. 50. 56. 45. 40. 43.\n 49. 52. 38. 51. 56. 35. 58. 52. 64. 64. 55. 61. 41. 40. 38. 53. 46. 55.\n 51. 41. 44. 43. 53. 39. 42. 48. 42. 46. 39. 51. 40. 47. 53. 51. 44. 39.\n 51. 47. 48. 48. 40. 55. 52. 46. 52. 55. 55. 46. 53. 59. 48. 38. 42. 54.\n 48. 51. 48. 47. 48. 55. 47. 39. 51. 49. 40. 52. 52. 59. 48. 41. 51. 42.\n 61. 47. 47. 50. 42. 47. 47. 44. 40. 40. 53. 45. 49. 41. 50. 45. 59. 57.\n 45. 41. 47. 54. 34. 62. 35. 51. 43. 53. 41. 48. 52. 51. 42. 45. 42. 55.\n 44. 55. 57. 33. 50. 43. 50. 48. 48. 53. 38. 54. 48. 48. 43. 51. 52. 52.\n 48. 42. 42. 62. 42. 42. 51. 59. 40. 42. 62. 55. 59. 45. 46. 45. 40. 45.\n 55. 49. 38. 57. 39. 54. 39. 60. 47. 53. 50. 56. 46. 41. 52. 37. 47. 45.\n 49. 50. 42. 45. 38. 35. 45. 57. 43. 41. 48. 46. 52. 56. 52. 52. 51. 52.\n 38. 48. 36. 49. 59. 57. 54. 46. 45. 40. 58. 54. 59. 48. 58. 43. 43. 62.\n 56. 44. 40. 49. 37. 49. 57. 55. 47. 54. 37. 47. 44. 49. 48. 53. 38. 46.\n 49. 42. 41. 51. 43. 50. 52. 45. 52. 37. 45. 35. 52. 46. 48. 52. 35. 42.\n 46. 42. 62. 41. 56. 46. 46. 55. 45. 42. 55. 39. 59. 54. 60. 41. 45. 46.\n 50. 44. 48. 58. 52. 48. 51. 57. 42. 44. 51. 48. 40. 49. 52. 46. 42. 46.\n 52. 55. 55. 43. 44. 42. 40. 44. 42. 56. 59. 42. 57. 50. 50. 54. 49. 38.\n 53. 47. 40. 41. 45. 51. 51. 47. 42. 35. 41. 43. 43. 38. 45. 36. 54. 53.\n 39. 39. 59. 51. 48. 47. 49. 44. 47. 56. 38. 47. 61. 48. 49. 44. 59. 43.\n 39. 41. 57. 52. 47. 43. 49. 38. 48. 48. 45. 51. 43. 45. 55. 50. 54. 42.\n 52. 49. 46. 39. 57. 55. 40. 49. 58. 52. 51. 44. 48. 38. 62. 43. 51. 40.\n 47. 40. 46. 51. 52. 41. 37. 39. 43. 40. 46. 35. 53. 51. 46. 45. 49. 39.\n 52. 53. 42. 51. 54. 75. 48. 47.]\"\t11384.0\t11377.0(-0.0)\t11377.0(-0.0)\ng7.mc\t47.94\t0.06\t-0.007822277847309137\t1.0\t-1.0\t0.9999694055166285\t\"[ -2.   6.   0.  -9.  -5.   5. -14.  -4.  -2.  -2.   2.  -6.   7.   3.\n   2.  -2.   7.   3.   0. -10.   5.  -3.  11.   7.  -6.  19.  -2.   0.\n -11.  -2.   4.  -6.  -5.   0.  -5. -13.  -7.   4. -12.   4.   9.  -7.\n  -4.   3.  -4.   2. -11.  -4. -16.  -6.   1.  -8.   5.   3. -15. -18.\n   5.  -2. -18.   5.   6.   4.   1.   1.  10.   4.  10. -12.   1.   2.\n  -3.  -7.  -4.   2. -10. -14.   4.  -9.  -3.   7.   6.   5.   3.   2.\n   2.   3.   3.  -7.   3.  -9.  12.   6.   0.  -1.  -2.   3.  -3.   4.\n   3.  -4.  -8.  -8. -13.  -7.  -4. -11.  -9.  -3. -12.  -7.  -6.   3.\n   7.   0.  12.   1.  -3.  12.   7.  -1.   7.  -1.   9.  -5.   4.  -5.\n   2.  15.   9.  -7.  16.  -1.   2.  -5.   9.  13.   2.   1.   4.  -9.\n  -2.   6. -10.  -6.  -5.  13.  20.   2.   2.   4.   1. -12.  -1.   8.\n  -8.  -4. -11.  -6.   3.   7.   8.   4.  -4.  -1.  -2.  -6.  14.   2.\n  -8.   0.   7.   2.  -7.  11.  -2.   5. -10.  -3. -17. -14.  -3.  -5.\n  -9.   9.  -7.  -8.  -7.  -1.  -4.  -8.  -1.  -2.  -2. -11. -16.   5.\n  -8. -12.   0.  -5.   1.  -3. -13.   0.   2. -14.   5.   3.   0.  -3.\n  -3.   1.  -1.  -9.  16.   3.  -2.  -1.   5.   0.   7.   1.  -1.   6.\n   2.  -9.   6.   0. -10.  -7.  -3.   7.   8.  -7.  11.   1.  -1.  -5.\n   2.  -5.  -2.  -6.   6.   1.  -1.   4.   0. -10.   8.  10.  -2.  -3.\n   9. -11.  -1.  -4.  -5.   4.   1.  -7.   2.  -2.  -5.  -1.  -8.  -6.\n  -1.   4.   8.   2.   0.   1.   2. -11.   6.   6.   6.   3.  -1.  14.\n  -5.   8.   3.  -6.   2.  -3.   9.   5.  -2. -10.   6.   3.   3. -12.\n   2.   0.   6.   7.   8.  19. -11.  -4. -10.  -7.   8.  -7.  -2.  11.\n  -3.  12.   6.   5. -10.   8.  -1.  -1.   7.  -3.   7.  -6.  -4.   2.\n  11.  -1.  -1.  -4.  -9.  -2.  -5.   7.  11.  -1.  -3.  -6.  -1.  -4.\n   3.  -7.  16.  -4. -11.   7.  14.  -9.  12.  -1.   3.  -6.  -5.  -9.\n -15.   0. -18.  13.   4.  10.  -5.  -3.   4.  11.  -3.  -2.   3.   1.\n   4.  12.  -6.   8.   5.  10.  -8.  -1.   6.  -8.   0.  -5.  -3.   2.\n   7.   2.  -1.   2.   7.  -2.   0.  -1. -15.  -6.  -2.   0.  10.  -7.\n   8.  -5.   0.  -1.   3.   4.  -2.   5.  13.   0.   9.   7. -10.   3.\n   1.   5.  -5.  -3.  -3.  -7.  -2.   5.  11.  10.  -4.   9.  -4. -17.\n   2.   3.   1.   1.  -3.   0.   0.  -4.   2.  -4.   0.   4.   1.  -3.\n  -6. -13.  -4.  -2.  -7.   3.   8.  13.   7.  -1. -13.  13.  -9.   2.\n   2. -11.  -1.   4.   2.   4.   0.   4.  -8.  -7.  -5.  -7.   9.   5.\n   6.  -8.   0.  15.  11.  -1.   4.   6.   2.   9.   0.  -3.   5. -11.\n   5.   1. -10.  10.   0.   7.   0.  -4.   2.  -7.  -4.   3.  10.   3.\n   6. -14. -12.   6.  -3.   2.  -1.   6.   4.  -4.   5.  -2.   0.  -7.\n   3.   3.   9.   8. -15.  -6.  -2.  10.   2.   0.  15. -13. -13.  -8.\n   4. -16.   5.   0.   1.  10.  10. -12.   1.   5.   2.   3.  -5.  -3.\n -13.   4.  -6.  -7.   0.   7.  -9.  -7.  -2.   1.   2.   2.   4.   2.\n   2.  -9.  14.  11.  -7.   9.   8.  -2. -11.  -7.  -4.   1.  -4.   0.\n  -4.   2.  -4.  -3.   8.  -3.  -5.   3.   3.  16.  -3.  10.  -1.   6.\n  12.  11.  -6.   5.  -6.  -6.   5. -11.  13.   1.  -9. -14.   3. -15.\n -12.  -8.   1.  -7.   7. -10. -12.  -8.   4.  -7. -15.  -1. -11.  -1.\n  -1.  14.   2.  -1. -14.   3. -11.  -3.  -3.  -6. -10.   4.   4.  -3.\n  -3. -10.  -2.   5.  -4.  -8.  12.   4.   3.  -5.  -2.  -4.  -5.  -9.\n  -6.   9.  -8.  -4.   7.  -7.   6.   6.   6.  -3. -11.   7.  -4.  -9.\n  -3.   8. -11.  14.   3.   5.   5.  -2.  -1.  12. -10. -14.   3.  14.\n   1.   1.   5.   8.  -3.  -9.   7.   7. -18. -14.  15.  -6.   1.  -4.\n  -5.  -2.   7.   9.  10. -11.  -2.   7.  -4.  -7.  -2.   3.  -7.   4.\n   8.  -7.  -6.  -3.   2. -10.  -2.   1.   2.  -4.   3.  -7.  -2. -10.\n  -2.  -4.  -4.  -4.   9.   6.  13.  -1.   4.  -7.   0.  12.   1.   0.\n  -4.   4.  -3.  -2.  -5.  -2.   0.  17.   6. -12.  -1.   0.  -6.   4.\n   0. -13.  -3.   6.  -3.   6.   6.  -6.   9. -12.  -9.   2.  -2.  -1.\n -12.   4.  -6.   3.   4.   0.  -7.  -1.  -1.   7.   5.  -7.  -7.   8.\n  14. -11.   3.  -2.  -6.  -4.  -4.   7.   7.   1.   5.   4. -11.  -8.\n   4.   7. -12.  -5.   4.   5. -10.  17.  -6.  -2.   1.  -9.  -2. -11.\n  -3.  -3.   4.  -2.  -2.   8.   5.  13.  -1.  -7.   5.   9.  -5.   2.\n   3.   0.]\"\t1806.0\t1818.0(0.01)\t1818.0(0.01)\ng8.mc\t47.94\t0.06\t-0.008865248226950355\t1.0\t-1.0\t0.9999607029148068\t\"[ -5.   7.  -3.   1.  -9.   4.   9.  -6.  -2.   0.   0.  11.   3.   2.\n  -2.  -5.  13.  -1.  -9.   3.   4.   1.   2.   3.   2.  -1.  -1.   9.\n   3.  -8.  -1.   8.  -4.   7.  -2.   5.  -1.   5.  -1.   0.  -1. -10.\n  -5. -11.  -4.  -4.  13.  15.   3.   4. -10.   1. -17.  -7.   4. -11.\n   6.   0.  12.  -4.  -4. -28.  -2.  -8.  -4.  12.  -2. -15.  -9.   1.\n  -7.   9.  -7.  -6.   6. -18.   5.  -2.  -3.   6.   3.   2.  -1.   3.\n   8.  12.  -2.  -5.  15.   4.  -7.  16.   1.   6.  -2.  -5. -11.  -5.\n -12.  -4.  -5.  -3.   0.   1.   6.  -1.   9.   6.  -8.   0.  -9.   6.\n   3.   4. -12.  -6.  -1.   8.  -1.   1.  -3.   3.   5.   3.   5.   3.\n  -5.   6.   2.   2.  17.  -4.  -6.   3.  -6.   4. -11.  -9.  -5.  -1.\n   2.  -1.  -8.   1.   7.   4.   2.   2.  -4.  -3.  10.  20.  -3.  -4.\n  -4.   1.   7.  11.   2. -12.  -2.   1.   6.  -1.   0.  -8.   9.  -2.\n   2.  -3.  13.  -8.   3.   4.   1.   7.   0.  -8.  -1.   8.   4.   1.\n  -1.   4.  -6.  10.   1.   9.   3.   0. -12.  -7.   0.   6.  -5.   0.\n   6.   5. -11.  10.  11. -13.   0.  -1.   9.   9.   1.   7.  -9.  10.\n   3.  -7.  -6.  -1.   2.  -1.  -5.   2.  -2.   3.   5.   2.   2.   2.\n -15.   1. -21.  -4.  -3.   4.   2.  -5.  -6.   1.  -6.   5.  -9.  -1.\n  -1.  -8.   3.  -2.  -2. -11.  -4.  -9. -12. -12.  -6.  -3.  -1. -13.\n  -4.   5.  -3.   1.   5.  -1.  -7.  -5.  -8.   8.  11.  13.  -9.   8.\n  -3.  -5.   4.  -2.   6.   3.   3.   1.  -9.  -8. -12.  -7.   7. -13.\n   0.   2.  11.  11.   2.  -2.  10.  -4.  -6.  15.   1.  21.  -7.   3.\n  -6.   0.  -6.   6.   8.  -5.  -5. -14.  -3. -11.  11.   9.   5.   4.\n   3.   8.  11.   1.   0.  -3.  -4. -13.   1.   2.  14.   4.  -5.   5.\n   0.   5.  -4.  -8.   9.  -5.   0.   2.  -3.   7.  15.   4.   4.   8.\n   4.  -3.   5. -11.   0.   8.   1.   4.   8.   3.  -5. -14.   0. -12.\n   4.   2.  -3.  -1.   2.  -1.  -3.  -2.  -5.   4.  -6.  11.   0.   4.\n   9.   1.   0.  -2.  -6.   3.  -3.  -3.   8.   8.  12.  -9.  -4. -15.\n  -7.   0.  -1.   7.   7.  -1.  -9.  -6.  -1.   6.  -2.  -4.   4.   1.\n  -1.  -5.  -3.   7.  -2.  -3.  -6.   0.   8.  -5.  -3.   9.  -5.   2.\n   5.  -1.  -2.  -8.  -3.  -3.  -8.  -2.  -7.  -7.  -5.  14.  -4.   6.\n  -1. -10.  11.   1. -11. -16.  -3.  12.   7.   9.  12.   0.  -8.   3.\n  -5.   4.  11.  13.  -6.  -9.   1.   3.  -9.   0.  -3.  -2.  -5.   2.\n  -5.  -2.   8.  -7. -15.   4.   6.  17.   0.  -4.   5. -14.  -2.  -2.\n  -1. -11.  -7. -10.   3. -15. -10.  10.  -4.   7.   2.   2.   1.   6.\n   3.  -1.   0. -12.   2.   1.   2.   3.   2.  -1.  -2.   9.   1.   3.\n   3.   1.   6.  -8. -18. -21.   6.  -6.  -3.  -8. -17.  -1.   4.  -6.\n  -4.  -5.   9.  -2.   2.   8.   9.  -6.   0.  -7.   9.   0.  -2.  -3.\n   1.   8.   2.  -8.   7.  -2.  -4.   3.   0.  -6.   9.   0.   5.   6.\n   5.  -6.   6.   5.   0.   0.  -1.  -6.   4.  -4.  -9.   0.  11.  14.\n   3.   2.  -7.   3.  -6.   6.  14.   0.   5.   4.  -3.  -6.  -1. -17.\n -13.   0.  -2.  -4.  -3. -10.   0.  -2. -17.  -7.  10.  -5.   4.  -3.\n -13. -10. -13.   4.  -3.  -9.   2.  -3.   3.   8.  11.   5. -14.   4.\n   0.   0.   1.   6.   1.  10.  -5.  -5.  -9. -10.   9.  10.   0. -14.\n   9.   7.   7.  -8.  -7.   1.   3.  -4.  -3.  -8.   2.   0.   2.   0.\n  -2.   2.   8. -11.   7.   5.   1.   9. -12.  -3.   0. -15.   5. -14.\n  -9.  -7.  12.  -7. -14.   6.   9.   7.  -2.  -4.  -4.   0.   0.  -4.\n  -8.  10.   2.  -1.   1.   8.   0.  -7. -18.  -7.   1.  -8.  -4.   1.\n  -2.  -4.   0.   2.   1.  -8.  -7.   7.   1.  -5.  -8.  -1.   1.  13.\n   2.   2.  -5.  -5.   2.   0. -11. -10.  -1.  -1. -12.  -8.   7.  -8.\n  -3.   5.  -1.  -5.   0.  -4. -10. -11.  -3.   2. -14.  -3.  -7.  -2.\n  -5.  -9.  10.   7.   4.  -6.  -9.  -3.  10.   2. -13.  -5.   0.  -8.\n  -6.  -4.   8.   6.   0.  -6.   4.  -2.   8.  -4.  -3.   8. -13.   2.\n   0.   4.   3.   3.   6.  16.  -3.  -6.   3.  -5.   9.   2.  10.   2.\n  -5.   0.  15.   2.   1.  -6.  -8.   0.   2.  10.   2.  -7.  -6.   0.\n   5.   4.  -3.   2.  -3.   3.   0.  -3. -10.   3.  -2.  -6.   3.  -8.\n   4.   1.  18.  -4.   7.   2.  -3.   0.   2.  -5.   2.  13.   6.  11.\n  -9.   6. -13.   1.  11.   1. -15.  -3.  -1. -12.   1.  -6.  -3.   1.\n  12.  -2.]\"\t1802.0\t1863.0(0.03)\t1863.0(0.03)\ng3.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[41. 41. 47. 51. 55. 54. 39. 42. 48. 50. 44. 41. 57. 42. 44. 57. 59. 55.\n 39. 39. 54. 41. 44. 49. 48. 37. 55. 45. 53. 54. 39. 46. 52. 51. 36. 43.\n 47. 53. 47. 64. 45. 62. 57. 51. 48. 48. 41. 51. 41. 54. 66. 49. 53. 45.\n 54. 41. 36. 40. 52. 58. 42. 46. 50. 44. 44. 42. 56. 45. 43. 59. 59. 51.\n 45. 56. 56. 50. 43. 48. 47. 46. 45. 42. 65. 61. 58. 46. 48. 59. 39. 60.\n 41. 46. 47. 36. 46. 45. 33. 55. 50. 40. 49. 43. 30. 59. 36. 45. 41. 48.\n 38. 54. 53. 42. 61. 44. 36. 40. 43. 44. 33. 39. 49. 51. 47. 37. 63. 49.\n 45. 50. 64. 42. 33. 42. 42. 45. 48. 50. 59. 53. 43. 47. 52. 43. 60. 47.\n 41. 42. 56. 50. 46. 51. 60. 54. 45. 36. 54. 41. 49. 31. 44. 50. 38. 61.\n 56. 41. 60. 42. 47. 50. 42. 45. 51. 50. 37. 42. 37. 41. 48. 52. 49. 60.\n 50. 53. 41. 54. 52. 48. 47. 51. 47. 44. 50. 55. 46. 48. 53. 42. 48. 43.\n 47. 48. 43. 59. 54. 35. 41. 49. 39. 53. 49. 46. 49. 47. 64. 57. 56. 51.\n 41. 44. 42. 55. 55. 50. 48. 52. 43. 33. 49. 40. 41. 46. 40. 45. 44. 45.\n 48. 49. 51. 49. 43. 42. 47. 40. 50. 55. 48. 47. 48. 58. 52. 51. 53. 37.\n 52. 45. 49. 43. 45. 45. 43. 39. 58. 56. 47. 69. 65. 46. 43. 45. 54. 48.\n 48. 49. 39. 55. 53. 52. 50. 49. 57. 53. 46. 48. 51. 55. 50. 46. 42. 52.\n 52. 51. 49. 49. 43. 43. 46. 46. 48. 44. 42. 51. 39. 50. 61. 53. 47. 41.\n 55. 48. 39. 42. 57. 55. 46. 59. 36. 47. 57. 46. 48. 58. 37. 45. 50. 51.\n 52. 52. 39. 47. 56. 36. 55. 57. 39. 48. 44. 54. 42. 45. 57. 45. 46. 48.\n 39. 46. 40. 45. 45. 46. 52. 42. 46. 46. 63. 45. 42. 37. 47. 46. 51. 52.\n 50. 41. 46. 46. 51. 43. 52. 42. 48. 47. 57. 59. 48. 56. 52. 43. 56. 57.\n 45. 56. 51. 49. 55. 39. 45. 44. 45. 50. 58. 52. 44. 41. 43. 47. 49. 53.\n 50. 49. 50. 40. 50. 51. 39. 65. 41. 46. 45. 49. 50. 50. 37. 53. 56. 46.\n 45. 53. 63. 36. 58. 54. 61. 44. 47. 41. 61. 46. 39. 34. 41. 57. 48. 42.\n 38. 49. 43. 44. 43. 37. 50. 63. 55. 63. 37. 52. 43. 44. 47. 42. 33. 46.\n 50. 55. 51. 54. 58. 61. 44. 46. 47. 48. 38. 58. 41. 55. 51. 42. 51. 55.\n 46. 52. 46. 45. 60. 48. 47. 42. 63. 47. 44. 46. 52. 43. 56. 49. 44. 47.\n 52. 55. 49. 33. 55. 45. 48. 38. 50. 49. 46. 48. 55. 40. 57. 45. 44. 44.\n 50. 51. 47. 42. 48. 42. 49. 48. 46. 45. 51. 44. 54. 51. 43. 50. 48. 32.\n 51. 42. 42. 43. 54. 54. 49. 60. 31. 44. 57. 46. 50. 39. 38. 50. 47. 56.\n 56. 44. 55. 42. 45. 56. 41. 56. 43. 43. 46. 52. 42. 44. 47. 46. 57. 50.\n 39. 51. 43. 46. 46. 54. 35. 52. 52. 36. 57. 45. 50. 49. 44. 47. 51. 60.\n 49. 40. 35. 45. 48. 53. 51. 40. 47. 53. 42. 62. 50. 52. 47. 56. 43. 48.\n 45. 69. 41. 46. 49. 40. 46. 46. 39. 51. 53. 66. 35. 49. 45. 36. 49. 52.\n 54. 36. 46. 54. 40. 56. 44. 55. 49. 51. 39. 43. 42. 43. 38. 53. 57. 60.\n 51. 51. 48. 43. 44. 52. 51. 45. 46. 50. 54. 54. 46. 46. 50. 52. 40. 49.\n 41. 50. 42. 57. 48. 37. 41. 40. 50. 49. 58. 52. 58. 52. 51. 54. 47. 65.\n 53. 47. 58. 49. 43. 47. 48. 52. 43. 53. 40. 50. 51. 54. 37. 51. 60. 40.\n 51. 52. 51. 41. 45. 59. 46. 46. 54. 43. 43. 46. 58. 61. 51. 42. 39. 39.\n 56. 53. 52. 62. 57. 41. 42. 54. 51. 53. 52. 62. 38. 52. 48. 40. 50. 42.\n 46. 60. 48. 50. 43. 50. 49. 48. 50. 40. 47. 41. 42. 54. 45. 48. 49. 57.\n 45. 42. 42. 46. 39. 48. 47. 54. 53. 38. 50. 42. 46. 50. 48. 47. 36. 44.\n 39. 44. 53. 46. 45. 47. 52. 45. 40. 39. 60. 48. 49. 52. 34. 61. 46. 48.\n 45. 38. 45. 46. 48. 57. 34. 45. 52. 51. 63. 50. 51. 47. 45. 53. 47. 45.\n 53. 52. 43. 42. 47. 39. 48. 50.]\"\t11421.0\t11405.0(-0.0)\t11405.0(-0.0)\ng5.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[40. 51. 49. 66. 50. 43. 55. 42. 46. 39. 43. 47. 47. 51. 49. 46. 36. 44.\n 42. 33. 49. 45. 41. 43. 55. 47. 51. 44. 53. 50. 49. 43. 42. 49. 64. 39.\n 41. 35. 52. 49. 52. 58. 57. 42. 53. 49. 61. 45. 44. 35. 51. 50. 45. 46.\n 65. 42. 39. 42. 45. 41. 48. 42. 51. 45. 54. 47. 47. 37. 43. 44. 47. 47.\n 38. 39. 34. 46. 54. 50. 47. 38. 63. 54. 48. 53. 45. 44. 34. 48. 57. 40.\n 48. 51. 55. 48. 51. 44. 70. 53. 56. 57. 46. 55. 40. 39. 45. 47. 47. 44.\n 38. 40. 49. 48. 48. 35. 57. 45. 43. 44. 56. 47. 48. 36. 58. 51. 58. 44.\n 36. 36. 41. 51. 46. 49. 42. 46. 41. 44. 42. 47. 54. 39. 51. 56. 51. 55.\n 63. 62. 52. 47. 48. 47. 54. 40. 52. 47. 46. 40. 50. 55. 45. 42. 51. 44.\n 59. 53. 36. 46. 54. 45. 58. 52. 53. 39. 38. 57. 51. 52. 44. 54. 42. 30.\n 50. 58. 46. 43. 50. 49. 38. 30. 42. 52. 47. 57. 47. 44. 42. 47. 50. 58.\n 53. 50. 51. 52. 61. 57. 52. 48. 43. 51. 44. 45. 57. 43. 52. 43. 52. 51.\n 59. 47. 49. 53. 55. 48. 51. 47. 56. 48. 59. 42. 41. 45. 40. 68. 43. 50.\n 49. 40. 49. 46. 44. 50. 46. 54. 60. 42. 52. 58. 56. 46. 55. 45. 50. 66.\n 51. 57. 48. 59. 51. 45. 51. 51. 38. 50. 65. 56. 51. 36. 48. 47. 45. 43.\n 51. 57. 48. 46. 50. 51. 55. 51. 51. 44. 50. 49. 51. 44. 61. 57. 46. 46.\n 48. 43. 52. 41. 48. 54. 45. 39. 43. 37. 49. 44. 45. 38. 44. 57. 45. 34.\n 44. 51. 57. 56. 47. 46. 38. 53. 52. 62. 37. 39. 46. 49. 51. 45. 49. 44.\n 32. 56. 54. 42. 44. 55. 38. 47. 56. 41. 34. 49. 45. 52. 51. 57. 50. 40.\n 52. 51. 49. 58. 44. 45. 47. 37. 45. 32. 65. 44. 58. 54. 46. 45. 55. 35.\n 40. 42. 56. 53. 45. 50. 40. 44. 49. 51. 43. 53. 42. 51. 55. 57. 49. 44.\n 49. 60. 46. 66. 52. 49. 47. 46. 41. 54. 61. 45. 44. 58. 42. 52. 42. 66.\n 60. 39. 46. 42. 54. 67. 51. 35. 42. 45. 46. 52. 47. 49. 48. 55. 41. 42.\n 49. 40. 41. 59. 48. 43. 65. 54. 54. 43. 46. 56. 55. 49. 42. 40. 53. 46.\n 50. 50. 43. 47. 54. 37. 38. 54. 47. 59. 42. 54. 61. 49. 44. 57. 50. 53.\n 42. 40. 56. 45. 50. 57. 36. 40. 43. 43. 41. 45. 39. 51. 55. 55. 50. 46.\n 57. 50. 46. 49. 38. 47. 57. 45. 53. 45. 45. 52. 41. 45. 50. 50. 56. 53.\n 57. 43. 49. 52. 67. 52. 50. 37. 43. 45. 45. 50. 46. 50. 39. 39. 49. 47.\n 40. 54. 46. 52. 40. 56. 45. 40. 46. 38. 58. 48. 55. 40. 61. 51. 47. 42.\n 53. 39. 56. 45. 31. 45. 46. 42. 58. 47. 51. 43. 46. 55. 42. 50. 57. 43.\n 46. 46. 46. 42. 46. 55. 54. 48. 47. 51. 38. 57. 45. 46. 38. 54. 48. 49.\n 40. 51. 63. 45. 45. 46. 40. 41. 53. 53. 56. 50. 48. 46. 45. 53. 48. 54.\n 58. 38. 48. 47. 49. 39. 48. 54. 51. 60. 52. 48. 51. 36. 43. 52. 36. 47.\n 51. 50. 42. 45. 41. 51. 50. 40. 51. 54. 52. 46. 48. 34. 39. 50. 45. 47.\n 54. 54. 50. 49. 48. 41. 54. 56. 47. 43. 51. 58. 51. 46. 52. 40. 52. 53.\n 41. 48. 51. 41. 50. 45. 50. 41. 41. 46. 35. 42. 51. 57. 40. 53. 59. 47.\n 42. 40. 50. 45. 38. 42. 56. 60. 47. 53. 43. 46. 43. 39. 43. 45. 50. 53.\n 42. 55. 43. 51. 45. 50. 49. 51. 46. 35. 44. 48. 48. 50. 59. 53. 58. 59.\n 56. 50. 63. 58. 47. 54. 45. 46. 42. 44. 54. 49. 47. 45. 45. 56. 47. 54.\n 48. 46. 42. 49. 51. 49. 45. 42. 35. 50. 50. 43. 38. 42. 46. 51. 55. 50.\n 51. 37. 52. 47. 44. 47. 42. 49. 34. 45. 37. 41. 37. 47. 33. 54. 41. 47.\n 42. 50. 41. 52. 49. 47. 35. 56. 39. 58. 46. 37. 47. 49. 49. 43. 48. 40.\n 42. 57. 57. 41. 40. 52. 51. 57. 44. 32. 44. 49. 50. 50. 50. 57. 45. 48.\n 48. 49. 56. 49. 44. 41. 43. 48. 56. 50. 46. 52. 50. 51. 51. 45. 53. 37.\n 51. 52. 48. 57. 50. 50. 60. 57.]\"\t11420.0\t11531.0(0.01)\t11531.0(0.01)\ng1.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[47. 51. 50. 51. 39. 49. 50. 48. 33. 46. 47. 40. 47. 53. 55. 64. 63. 52.\n 42. 49. 53. 53. 59. 39. 50. 53. 45. 49. 53. 45. 37. 54. 53. 51. 47. 49.\n 52. 45. 45. 48. 50. 46. 50. 48. 49. 34. 45. 42. 37. 43. 44. 53. 49. 39.\n 39. 52. 42. 50. 43. 50. 40. 48. 50. 45. 43. 54. 46. 51. 58. 55. 52. 46.\n 57. 46. 39. 46. 48. 54. 34. 45. 49. 51. 45. 43. 52. 53. 49. 55. 58. 48.\n 47. 40. 42. 44. 39. 45. 58. 43. 52. 45. 42. 50. 65. 39. 39. 48. 38. 38.\n 49. 49. 54. 61. 49. 47. 50. 59. 52. 54. 46. 58. 44. 41. 47. 53. 43. 53.\n 47. 49. 41. 42. 46. 57. 48. 43. 47. 42. 47. 45. 42. 49. 46. 55. 46. 62.\n 56. 45. 47. 45. 44. 55. 46. 42. 41. 59. 48. 40. 61. 53. 48. 46. 45. 46.\n 52. 54. 50. 42. 48. 58. 49. 46. 46. 46. 49. 37. 47. 54. 40. 57. 52. 43.\n 41. 45. 40. 52. 47. 51. 44. 59. 45. 50. 48. 50. 53. 45. 35. 59. 44. 65.\n 52. 52. 49. 41. 47. 45. 46. 50. 40. 42. 48. 47. 50. 46. 51. 36. 44. 56.\n 56. 55. 33. 36. 58. 43. 43. 41. 54. 48. 53. 64. 56. 44. 49. 51. 50. 56.\n 59. 45. 33. 40. 44. 48. 46. 39. 39. 59. 44. 62. 39. 46. 62. 45. 60. 53.\n 55. 48. 47. 45. 46. 60. 49. 55. 41. 55. 59. 43. 53. 50. 52. 45. 45. 55.\n 58. 37. 57. 45. 48. 47. 42. 48. 55. 42. 47. 58. 56. 49. 47. 50. 52. 45.\n 48. 46. 55. 42. 44. 55. 41. 41. 51. 40. 44. 45. 42. 62. 52. 50. 50. 44.\n 49. 56. 48. 46. 36. 43. 54. 42. 60. 44. 40. 46. 37. 61. 49. 53. 50. 46.\n 61. 44. 52. 27. 40. 49. 50. 53. 51. 54. 46. 43. 49. 47. 38. 48. 51. 39.\n 37. 53. 47. 49. 54. 53. 55. 44. 44. 50. 49. 45. 51. 42. 57. 47. 41. 42.\n 56. 56. 44. 44. 47. 56. 50. 61. 58. 44. 47. 35. 45. 37. 39. 51. 43. 43.\n 55. 46. 53. 48. 52. 51. 37. 54. 52. 45. 51. 45. 37. 44. 59. 39. 47. 48.\n 58. 42. 47. 47. 51. 48. 47. 40. 37. 49. 48. 49. 43. 41. 45. 39. 47. 57.\n 51. 34. 59. 40. 45. 38. 61. 50. 57. 48. 43. 50. 43. 48. 45. 51. 56. 34.\n 47. 57. 34. 45. 47. 67. 53. 37. 52. 43. 49. 49. 51. 62. 56. 59. 32. 49.\n 50. 43. 52. 54. 54. 48. 55. 53. 54. 46. 43. 54. 51. 42. 45. 46. 43. 52.\n 44. 47. 41. 52. 47. 38. 46. 43. 51. 42. 51. 47. 46. 47. 44. 37. 44. 47.\n 43. 48. 37. 50. 44. 45. 51. 45. 50. 47. 52. 57. 49. 52. 45. 40. 51. 46.\n 55. 37. 51. 41. 47. 50. 60. 49. 47. 44. 45. 52. 55. 53. 45. 40. 43. 39.\n 53. 50. 49. 53. 47. 39. 53. 54. 37. 48. 50. 47. 41. 41. 50. 46. 50. 49.\n 44. 56. 51. 44. 45. 50. 41. 52. 43. 40. 58. 56. 43. 41. 47. 47. 36. 40.\n 41. 62. 42. 46. 43. 40. 45. 47. 41. 48. 52. 55. 55. 45. 53. 49. 56. 51.\n 52. 53. 51. 35. 55. 44. 59. 55. 46. 36. 55. 49. 42. 51. 62. 52. 48. 53.\n 29. 52. 44. 44. 51. 52. 41. 55. 44. 40. 52. 48. 49. 53. 49. 47. 46. 55.\n 56. 46. 50. 51. 59. 49. 58. 44. 43. 45. 52. 52. 48. 52. 42. 50. 48. 44.\n 49. 46. 46. 44. 35. 47. 50. 66. 50. 40. 52. 56. 56. 42. 47. 47. 46. 58.\n 55. 53. 46. 52. 57. 47. 59. 53. 39. 42. 53. 41. 47. 47. 42. 50. 54. 34.\n 47. 48. 38. 51. 49. 47. 39. 37. 55. 58. 44. 49. 50. 42. 45. 49. 43. 58.\n 48. 48. 40. 53. 46. 51. 48. 33. 54. 48. 55. 44. 44. 48. 44. 43. 38. 50.\n 51. 45. 49. 50. 53. 52. 58. 62. 43. 49. 50. 51. 50. 52. 46. 49. 50. 44.\n 61. 52. 38. 46. 46. 39. 44. 40. 59. 58. 45. 49. 51. 50. 38. 43. 44. 49.\n 63. 44. 38. 47. 59. 55. 61. 40. 44. 45. 40. 39. 51. 38. 41. 49. 47. 46.\n 45. 33. 41. 52. 53. 49. 45. 60. 50. 44. 49. 55. 57. 55. 43. 50. 49. 44.\n 45. 50. 46. 51. 55. 46. 57. 50. 43. 45. 55. 40. 44. 54. 46. 47. 42. 48.\n 49. 54. 48. 42. 43. 52. 49. 39.]\"\t11443.0\t11394.0(-0.0)\t11394.0(-0.0)\ng6.mc\t47.94\t0.06\t0.00803087192323738\t1.0\t-1.0\t0.9999677520281105\t\"[  3.  -5. -10.  -3.  -7.   9.   4.   4.  -5.   8.  -3.  -4.  15.  -1.\n  -7.  -6.   3.  22.   2.   9.  -5.  -5. -19.  -3.   8.   7.   5.   7.\n  -1.   5.  15.   2.   1. -15.  -3.  -7.  -2.   3.   9.   0.  -8.  -6.\n -10.  -2.   3.   4.  -1.   6.  -5.   5.   2.  -1.  -5.   1.   7.  -2.\n   6.  -4.  -1.  -6.  -6.  -8.   0.   3.   9.  10.  -6.   7.  12.   5.\n  -6.  -4.  11.   2. -17.   2.   4.  -4.  -2.   7.  -7.  -3.   9.   3.\n   8.  -1.  -3.   3.   2.   4.   7.  -6.  -4.   2.   3.   5. -12.   7.\n  10.   3.  -8.   8.  -7.  -5.  -1.  -2.   0.   2.  -7.   5. -10.   1.\n   7.   5.  12.  -7.  -2.   4.   4.   0.   4.  -5.   1.   3.  -1.  -5.\n   7.  11.  -7.   8.   4.  -1.  -2.  -3.   1. -14.   9.  17.   4.   3.\n   4.   3.  14.   2. -16.  -5.   1.   9.  10.  -9.  -2.   2.  -3.  11.\n -10.   0.   3.   9.  -4.  -6.   7.   6.   4.   0. -12.   0.  -6. -14.\n  -3.   8.   0.   0.  -5.   5.  -7.   8.  -4. -15. -12.   3. -15.  11.\n -10.  -8.  -1.   7.   4.  -3.  13.   4.  -4.   0.  -5.   5.   3.  -9.\n   6.  -1.   0.   6.  -5.  -3.   7.   3.   4.  12.   4.   4.  -8. -17.\n  -2.   2.   9.   0.   2.   2.   4.  -3.   3.  -4. -10.   5.  11.   5.\n -12.   0.  29.   4.  -2.   0.   1.  -5.  -4.  24.  -9.   1.  -3.   0.\n  -2.  -6.   0.   5.  -9.  -9.   8.  -2.   1.   4.  10.   7.  -2.  -5.\n   9.  -2.   3.   3.   8. -10.   3.   5.   5. -11.  -1.  -7.  13.  -2.\n  16.  -5.   5.  -7.  16.   3.   5.  -9. -14.  -5.  -6.   2.   9.  -8.\n   9.   6. -10.  -3.  -3.  -8.   0.  -9.  -8.   0.  -1.   0.   8.  -9.\n   3. -17.  -9.   0.   0.   7.   0. -10.  -2.  12.   4.   8.   7.  -2.\n   6.   0.   8.   5.  10.  -6.   4.   4.   8.  -4.  -7.  11. -11.   5.\n  -6.   8.  -3.   6.   6.   7.  -4.  -5.  -4.   7.  -1.  -2.  -2.   3.\n -19. -15.   2. -10.  -7.  -9.  -7.   1.  -5.  -1.   0.   7.  -3.   4.\n   4. -12.   9.   5.   7.   2.   9.  -3.   1.  -6.   6.   4.   0.  -6.\n  11.   2.   2.  11.   8.  -8.  -9.  -5.   9.   3.  -5.   7.  -3.   1.\n  -1.   4.  -1.  16.   8.   1.  -5.  10. -14. -13.   1.   9.   3.  -8.\n  -1.  -3.  -1. -10.   0.   4.  -7.   5. -15.   4.  11.  -4.   3.   9.\n  -6.  -9.  -3.   7.   5. -17.   3.   3.   1.  10.   1.   0.  -3.   0.\n   9.  -6.  -7.   6.   5.   8.  -1.   2.  11.   3.   4.   0.  -5.  -1.\n  -2. -15.   5. -15.   3.  -1.   8.  -1.  -1.  -1.  -3.   0.  14.   3.\n  -8. -17.  14.   3.   2.  22.  -8.  16.   1.  -1.   6.   4.   9.   0.\n   5.   6.  -3.   2.   7.   2.   4.  11.   1.   0.   3.   0. -10.  -5.\n  -7.   0.  -7.  -1.  -2.   5.   2.   5.   0.  11.   7.  16.   3.   4.\n -16.  -1.  -5.  -5.  -8.  -3.  12. -21.   1.   2.   5.  -4.   1.  -8.\n  -5.  -1.  -7.   1.  -7.  12.  10.   5.   7.   0.  -1.   2.  -3. -13.\n   3.  -2.  11.   5.   5.  -2.  -7.  -1.  -3. -11. -15.  -6.  -1.   6.\n   8.   5.   7.  -5. -10.  -6.   8.   1.   6.   8.   1.   6.  -1.  -2.\n  -1.  -2.   5.  -4. -12.  -6.  -3.  -5.   1.   3.  -8.   2.  -1.  12.\n  -4.   0.   5.  -4.   1. -13.  11.   2.  -2.  11.   3.   3.  -7.  -1.\n  -8.  -1.   4.   3.  -5.   3.   5.  -4.  -3.  -1.   2.   2.  -3.   5.\n  10.  -3.   4.  10.   2.  -3.   1.   4. -10.   4.  -7.   0.   3.  -3.\n  10.   6.   8.   2. -19. -11.   9.  -3.   4.   3.   4.   6.  -4.  -1.\n   7.   1.   2.  -4.  -5.  11.  -8.  10.   6.   0. -12.   4.  14.  -6.\n  -1.  16.   2.  -4.   5.   3.  -8.  -2. -10.   0. -12. -12.  14.  -6.\n   1.  -5.   4.  -4.  -3.   7.  -2.   8.   3.   3.  -3.   3.  -3. -16.\n  -9.   7.  -5.  -1.  12.   2. -10. -14. -21.  -6.   2.  -1. -11.   5.\n  -3.   3.  -5.  10.   4.   5.  12.   6.  -3.  -1.  -5.  -6.  -2.   4.\n   4.  11.   4.   3.   6.   5. -10.  -6.   5.   6.   4.   6.  -2.  -7.\n   6.  14.  17.   7.   1.  -8.  -5.   0.  -6.  -4.   1. -11.  -8.  -9.\n  -6.   0.  -8.   5.   2.  -6.  -9.  14.  -4.  12.  10.  -1.  -6.   2.\n  -7.  10.  -5.  -3.  11.  16.   0.   9.  -8.   5.   3.   0.  -4.   1.\n  -3. -11.   7.   6.  -4.  -3.   0.   5.  15.  -4.  -1.  -7.   5.  -2.\n   7.   9.  -9.  14.  -9.   5.  -3.   2.   6.   8.   5.   1.   9.   5.\n  -5.  14.  11.   0. -13.  14. -10.   5. -15.   4.  -5.   2.  -3.  -1.\n -13.   0.  12.  -8.  -2.   7.  14.   0.   5.  -2.   4.  -8.   1.   2.\n  -5.  -3.]\"\t1927.0\t1974.0(0.02)\t1974.0(0.02)\n\nIt is evident that in some scenarios, the new heuristic performs well, although there is still room for improvement. Based on the results, please analyze the following:\n1. In which cases does the new heuristic perform well, and in which cases is it not satisfactory?\n2. What factors contribute to these differences in performance?\n3. Is the updated heuristic most_weight_neighbors_2e6e better than previous heuristic most_weight_neighbors_3e17 and is the previous evolution suggestion  Implement a dynamic scaling_factor that adjusts based on the standard deviation of edge weights. This allows for better tuning of future impacts in diverse graph instances, reducing the rigidity of a fixed scaling_factor. works?\n4. How can we further refine the new heuristics?\n\nWe believe that further refinement of the new heuristic is necessary.\n\nPlease analyze the results and provide one executable code adjustment suggestion (in word, not code).\n\nTo make sure the code adjustment suggestion is executable, here are some aspects to consider and you can provide more:\n1. This may involve changing default values or adopting dynamic values tailored to specific data.\n2. Implement checks to retain the original algorithm when the data does not favor heuristic updates.\n3. This includes aspects such as operation type, subject, location, and timing.\n4. Should we continue to deepen origin suggestion or weaken this suggestion:  Implement a dynamic scaling_factor that adjusts based on the standard deviation of edge weights. This allows for better tuning of future impacts in diverse graph instances, reducing the rigidity of a fixed scaling_factor..\n5. Explore insights and strategies from other successful heuristics.\n\nThe response format is very important. For better communication, please respond to me in this format:\n1. If you believe the code still need to be updated, please respond to me in this format:\n***refinement: \nresult analysis: xxx\ncode adjustment suggestion: xxx\n***\n2. If you think current heuristic are good enough, please respond to me in this format:\n***refinement: \nresult analysis: None\ncode adjustment suggestion: None\n***\nEnsure there is no other content inside the ***, and analysis outside *** is welcome.\nIf you have no information to provide, simply respond with ***None***."
            }
        ]
    },
    {
        "role": "assistant",
        "content": [
            {
                "type": "text",
                "text": "***refinement: \nresult analysis: The new heuristic most_weight_neighbors_2e6e performs well in instances with high variability in edge weights (e.g., g6.mc, g8.mc), where dynamic adjustment helps in capturing the diverse impacts of edges. However, it is less satisfactory in scenarios with low variability or uniform edge weights (e.g., g9.mc, g4.mc), where the dynamic scaling factor may not significantly alter the decision-making process. The performance differences are largely influenced by the standard deviation of edge weights, which affects the dynamic scaling factor's adjustment and the balance of immediate versus future impacts.\ncode adjustment suggestion: Introduce a threshold-based approach for the dynamic scaling factor. When the standard deviation of edge weights is below a certain threshold, revert to a fixed scaling factor to prevent unnecessary adjustments in uniform cases. This can enhance stability and performance in less variable instances.\n***"
            }
        ]
    },
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "Based on previous discuss, we get the heuristic most_weight_neighbors:\nNow, based on these suggestions:\n Introduce a threshold-based approach for the dynamic scaling factor. When the standard deviation of edge weights is below a certain threshold, revert to a fixed scaling factor to prevent unnecessary adjustments in uniform cases. This can enhance stability and performance in less variable instances.\nUpdate the most_weight_neighbors_2e6e.\n\nNow please implement it in python function most_weight_neighbors_641f.\n\nTo implement this heuristic function in python, please note:\n1. We can assume that Solution and all Operators are imported.\n2. The operator type must be defined previously, do not create a new one in your code.\n3. Never modify the data in global, state and algorithm data.\n4. All hyper parameters in kwargs should be set a default value.\n5. Any reasonable partial solution may be used as input, such as an empty solution.\n6. Comments in the code are very important. They must clearly explain which hyper-parameters are required by the algorithm (i.e., the parameters in **kwargs), how the algorithm proceeds, and under what circumstances it will not return any operator or will return an empty operator. We hope that people can understand the principles and workflow of the algorithm clearly just by reading the comments, without needing to look at the code.\n7. The name of function must be most_weight_neighbors_641f. The TargetOperatorType should be replaced by real returned operator type.\n8. No any omissions or placeholders, I'm just going to use the code.\n9. For the algorithm to update the algorithm_data, do not modify directly \"algorithm_data[\"abc\"] = 123\", we should return operator, {\"abc\": 123}.For algorithm that do not update them, just return operator, {}\n10. For the circumstances that algorithm return empty operator, please return None, {}.\n11. Do not forget to check whether the content is empty when calculating the min/max/average/std/..., such as when using min([distance[node][other] for other in nodes if other != node]).\n12. Make the result must be valid.\n\nFor max_cut problem, take about following:\n1. Make the result must be valid, such as: no re-inserted nodes, never insert a node in set A, which are already in set B, etc.\n2. The graph is undirected, meaning each edge connecting two nodes is treated as a single, bidirectional edge.\n\n\nThe problem will work on following data.\nPlease try to understand the data and generate based on them, especially setting the default value for hyper-parameters:\nAll data is possible\n\nThe response format is very important. For better communication, please respond to me in this format:\n***python_code:\nfrom src.problems.max_cut.components import *\nimport other_library (if necessary)\ndef most_weight_neighbors_641f(global_data: dict, state_data: dict, algorithm_data: dict, get_state_data_function: callable, **kwargs) -> tuple[TargetOperatorType, dict]:\n    \"\"\" Description for this heuristic algorithm.\n\n    Args:\n        global_data (dict): The global data dict containing the global data. In this algorithm, the following items are necessary:\n            - necessary_key (value type): description\n            - necessary_key (value type): description\n            ...\n        state_data (dict): The state dictionary containing the current state information. In this algorithm, the following items are necessary:\n            - necessary_key (value type): description\n            - necessary_key (value type): description\n            ...\n        (Optional and can be omitted if no algorithm data) algorithm_data (dict): The algorithm dictionary for current algorithm only. In this algorithm, the following items are necessary:\n            - necessary_key (value type): description\n            - necessary_key (value type): description\n            ...\n        (Optional and can be omitted if no used) get_state_data_function (callable): The function receives the new solution as input and return the state dictionary for new solution, and it will not modify the origin solution.\n        (Optional and can be omitted if no hyper parameters data) introduction for hyper parameters in kwargs if used.\n\n    Returns:\n        Description of this operator.\n        Description of updated algorithm data.\n    \"\"\"\n    xxx\nEnsure there is no other content inside the ***, and analysis outside *** is welcome.\nIf you have no information to provide, simply respond with ***None***.\n"
            }
        ]
    },
    {
        "role": "assistant",
        "content": [
            {
                "type": "text",
                "text": "***python_code:\nfrom src.problems.max_cut.components import Solution, InsertNodeOperator, SwapOperator\nimport numpy as np\n\ndef most_weight_neighbors_641f(global_data: dict, state_data: dict, algorithm_data: dict, get_state_data_function: callable, swap_frequency: int = 5, threshold: float = 0.1, fixed_scaling_factor: float = 0.5, **kwargs) -> tuple[InsertNodeOperator, dict]:\n    \"\"\" Heuristic algorithm to optimize the MaxCut problem by adjusting the scaling factor dynamically based on edge weight variability and using a threshold to determine when to use a fixed scaling factor.\n\n    Args:\n        global_data (dict): The global data dict containing the global data. In this algorithm, the following items are necessary:\n            - \"weight_matrix\" (numpy.ndarray): A 2D array representing the weight between nodes.\n        state_data (dict): The state dictionary containing the current state information. In this algorithm, the following items are necessary:\n            - \"current_solution\" (Solution): The current solution of the MaxCut problem.\n            - \"unselected_nodes\" (set[int]): The set of unselected nodes.\n        algorithm_data (dict): The algorithm dictionary for the current algorithm only. In this algorithm, the following items are necessary:\n            - \"operation_count\" (int): The number of operations performed so far. Default is 0.\n            - \"sorted_nodes\" (list of tuples): A sorted list of (node, future_impact) in descending order. Default is empty.\n        get_state_data_function (callable): The function receives the new solution as input and returns the state dictionary for the new solution. It does not modify the original solution.\n        kwargs: \n            - swap_frequency (int, optional): Frequency (in terms of operations) at which swap operations are considered. Defaults to 5.\n            - threshold (float, optional): The standard deviation threshold to decide whether to use a dynamic or fixed scaling factor. Defaults to 0.1.\n            - fixed_scaling_factor (float, optional): The fixed scaling factor used when the standard deviation is below the threshold. Defaults to 0.5.\n\n    Returns:\n        InsertNodeOperator or SwapOperator: The operator to modify the solution.\n        dict: Updated algorithm data with the sorted list of nodes and operation count.\n    \"\"\"\n    \n    # Extract necessary data\n    weight_matrix = global_data[\"weight_matrix\"]\n    current_solution = state_data[\"current_solution\"]\n    unselected_nodes = state_data[\"unselected_nodes\"]\n    set_a, set_b = current_solution.set_a, current_solution.set_b\n    operation_count = algorithm_data.get(\"operation_count\", 0)\n    sorted_nodes = algorithm_data.get(\"sorted_nodes\", [])\n\n    # Calculate standard deviation of edge weights\n    std_dev = np.std(weight_matrix)\n\n    # Determine scaling factor based on standard deviation threshold\n    if std_dev < threshold:\n        scaling_factor = fixed_scaling_factor\n    else:\n        scaling_factor = 1.0 / (1 + std_dev)  # Dynamic adjustment\n\n    # Step 1: Perform a swap operation periodically\n    if operation_count % swap_frequency == 0 and set_a and set_b:\n        weight_to_a = weight_matrix[:, list(set_a)].sum(axis=1)\n        weight_to_b = weight_matrix[:, list(set_b)].sum(axis=1)\n        best_increase = float(\"-inf\")\n        best_pair = None\n\n        # Evaluate all possible swaps\n        for i in set_a:\n            for j in set_b:\n                delta = weight_to_a[i] - weight_to_a[j] + weight_to_b[j] - weight_to_b[i]\n                if weight_matrix[i, j] != 0:  # Adjust for the edge between i and j if it exists\n                    delta += 2 * weight_matrix[i, j]\n                if delta > best_increase:\n                    best_increase = delta\n                    best_pair = (i, j)\n\n        # If a beneficial swap is found, return the SwapOperator\n        if best_pair and best_increase > 0:\n            return SwapOperator(nodes=list(best_pair)), {\"operation_count\": operation_count + 1}\n\n    # Step 2: Sort unselected nodes based on future impact if not already sorted\n    if not sorted_nodes:\n        # Calculate the future impact for each unselected node\n        sorted_nodes = sorted(\n            [(node, sum(abs(weight_matrix[node][other]) for other in range(len(weight_matrix)))) for node in unselected_nodes],\n            key=lambda x: x[1],\n            reverse=True\n        )\n    else:\n        # Filter out nodes that have been selected since the last run\n        sorted_nodes = [\n            (node, future_impact) for (node, future_impact) in sorted_nodes\n            if node in unselected_nodes\n        ]\n\n    # Step 3: Select the best unselected node based on both immediate and future impacts\n    if not sorted_nodes:\n        return None, {}\n\n    # Extract the best node and its future impact\n    best_node, future_impact = sorted_nodes.pop(0)\n\n    # Calculate the potential increase in cut value for adding the node to each set\n    potential_increase_a = sum(weight_matrix[best_node][other] for other in set_b)\n    potential_increase_b = sum(weight_matrix[best_node][other] for other in set_a)\n\n    # Adjust the potential increases by adding the scaled future impact\n    adjusted_increase_a = potential_increase_a + scaling_factor * future_impact\n    adjusted_increase_b = potential_increase_b + scaling_factor * future_impact\n\n    # Consider negative weights critically and choose the set that maximizes the cut value\n    if sum(weight_matrix[best_node]) < 0:\n        target_set = \"A\" if adjusted_increase_a > adjusted_increase_b else \"B\"\n    else:\n        # Choose the set that gives the maximum adjusted increase in cut value\n        target_set = \"A\" if adjusted_increase_a >= adjusted_increase_b else \"B\"\n\n    # Create the operator to insert the selected node into the chosen set\n    operator = InsertNodeOperator(best_node, target_set)\n\n    # Return the operator and the updated algorithm data\n    return operator, {\"sorted_nodes\": sorted_nodes, \"operation_count\": operation_count + 1}\n***"
            }
        ]
    },
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "To ensure the stable of heuristics, we adjust the code to:\nfrom src.problems.max_cut.components import Solution, InsertNodeOperator, SwapOperator\nimport numpy as np\n\ndef most_weight_neighbors_641f(global_data: dict, state_data: dict, algorithm_data: dict, get_state_data_function: callable, swap_frequency: int = 5, threshold: float = 0.1, fixed_scaling_factor: float = 0.5, **kwargs) -> tuple[InsertNodeOperator, dict]:\n    \"\"\" Heuristic algorithm to optimize the MaxCut problem by adjusting the scaling factor dynamically based on edge weight variability and using a threshold to determine when to use a fixed scaling factor.\n\n    Args:\n        global_data (dict): The global data dict containing the global data. In this algorithm, the following items are necessary:\n            - \"weight_matrix\" (numpy.ndarray): A 2D array representing the weight between nodes.\n        state_data (dict): The state dictionary containing the current state information. In this algorithm, the following items are necessary:\n            - \"current_solution\" (Solution): The current solution of the MaxCut problem.\n            - \"unselected_nodes\" (set[int]): The set of unselected nodes.\n        algorithm_data (dict): The algorithm dictionary for the current algorithm only. In this algorithm, the following items are necessary:\n            - \"operation_count\" (int): The number of operations performed so far. Default is 0.\n            - \"sorted_nodes\" (list of tuples): A sorted list of (node, future_impact) in descending order. Default is empty.\n        get_state_data_function (callable): The function receives the new solution as input and returns the state dictionary for the new solution. It does not modify the original solution.\n        kwargs: \n            - swap_frequency (int, optional): Frequency (in terms of operations) at which swap operations are considered. Defaults to 5.\n            - threshold (float, optional): The standard deviation threshold to decide whether to use a dynamic or fixed scaling factor. Defaults to 0.1.\n            - fixed_scaling_factor (float, optional): The fixed scaling factor used when the standard deviation is below the threshold. Defaults to 0.5.\n\n    Returns:\n        InsertNodeOperator or SwapOperator: The operator to modify the solution.\n        dict: Updated algorithm data with the sorted list of nodes and operation count.\n    \"\"\"\n    \n    # Extract necessary data\n    weight_matrix = global_data[\"weight_matrix\"]\n    current_solution = state_data[\"current_solution\"]\n    unselected_nodes = state_data[\"unselected_nodes\"]\n    set_a, set_b = current_solution.set_a, current_solution.set_b\n    operation_count = algorithm_data.get(\"operation_count\", 0)\n    sorted_nodes = algorithm_data.get(\"sorted_nodes\", [])\n\n    # Calculate standard deviation of edge weights\n    std_dev = np.std(weight_matrix)\n\n    # Determine scaling factor based on standard deviation threshold\n    if std_dev < threshold:\n        scaling_factor = fixed_scaling_factor\n    else:\n        scaling_factor = 1.0 / (1 + std_dev)  # Dynamic adjustment\n\n    # Step 1: Perform a swap operation periodically\n    if operation_count % swap_frequency == 0 and set_a and set_b:\n        weight_to_a = weight_matrix[:, list(set_a)].sum(axis=1)\n        weight_to_b = weight_matrix[:, list(set_b)].sum(axis=1)\n        best_increase = float(\"-inf\")\n        best_pair = None\n\n        # Evaluate all possible swaps\n        for i in set_a:\n            for j in set_b:\n                delta = weight_to_a[i] - weight_to_a[j] + weight_to_b[j] - weight_to_b[i]\n                if weight_matrix[i, j] != 0:  # Adjust for the edge between i and j if it exists\n                    delta += 2 * weight_matrix[i, j]\n                if delta > best_increase:\n                    best_increase = delta\n                    best_pair = (i, j)\n\n        # If a beneficial swap is found, return the SwapOperator\n        if best_pair and best_increase > 0:\n            return SwapOperator(nodes=list(best_pair)), {\"operation_count\": operation_count + 1}\n\n    # Step 2: Sort unselected nodes based on future impact if not already sorted\n    if not sorted_nodes:\n        # Calculate the future impact for each unselected node\n        sorted_nodes = sorted(\n            [(node, sum(abs(weight_matrix[node][other]) for other in range(len(weight_matrix)))) for node in unselected_nodes],\n            key=lambda x: x[1],\n            reverse=True\n        )\n    else:\n        # Filter out nodes that have been selected since the last run\n        sorted_nodes = [\n            (node, future_impact) for (node, future_impact) in sorted_nodes\n            if node in unselected_nodes\n        ]\n\n    # Step 3: Select the best unselected node based on both immediate and future impacts\n    if not sorted_nodes:\n        return None, {}\n\n    # Extract the best node and its future impact\n    best_node, future_impact = sorted_nodes.pop(0)\n\n    # Calculate the potential increase in cut value for adding the node to each set\n    potential_increase_a = sum(weight_matrix[best_node][other] for other in set_b)\n    potential_increase_b = sum(weight_matrix[best_node][other] for other in set_a)\n\n    # Adjust the potential increases by adding the scaled future impact\n    adjusted_increase_a = potential_increase_a + scaling_factor * future_impact\n    adjusted_increase_b = potential_increase_b + scaling_factor * future_impact\n\n    # Consider negative weights critically and choose the set that maximizes the cut value\n    if sum(weight_matrix[best_node]) < 0:\n        target_set = \"A\" if adjusted_increase_a > adjusted_increase_b else \"B\"\n    else:\n        # Choose the set that gives the maximum adjusted increase in cut value\n        target_set = \"A\" if adjusted_increase_a >= adjusted_increase_b else \"B\"\n\n    # Create the operator to insert the selected node into the chosen set\n    operator = InsertNodeOperator(best_node, target_set)\n\n    # Return the operator and the updated algorithm data\n    return operator, {\"sorted_nodes\": sorted_nodes, \"operation_count\": operation_count + 1}"
            }
        ]
    },
    {
        "role": "user",
        "content": [
            {
                "type": "text",
                "text": "To assess the performance of the evolved heuristics, I have tested both the original heuristic most_weight_neighbors_d31b and the evolved heuristic most_weight_neighbors_641f.\nThe following table presents a comparative analysis of these heuristics. The first 8 columns represent the features of the testing data, while the remaining columns show the results.\nA higher result indicates better performance, and the improvement ratio (in brackets, positive means to improve and negative means decline) reflects the enhancement relative to most_weight_neighbors_d31b.\n\ndata_name\taverage_node_degree\tedge_density\taverage_edge_weight\tmax_edge_weight\tmin_edge_weight\tstandard_deviation_edge_weight\tweighted_degree_distribution\tmost_weight_neighbors_d31b\tmost_weight_neighbors_2e6e\tmost_weight_neighbors_641f\ng9.mc\t47.94\t0.06\t-0.003337505214851898\t1.0\t-1.0\t0.9999944305139609\t\"[-10.  -3. -13.  -4.  -6.   2.  -9.   3.  -1.  -6.   4.   3.   1.   1.\n   5.  -7. -12.  -4. -16.   1.   8.  12. -11.   0.  -3.   3.  -4.  -2.\n  -2.   7.  -8.  11.  -6.  -4.   0.   5.  -2.  10.  -8.   3.   8.   4.\n   0.   8. -16. -13.  -4.  -5.  -5.  11.   1.   9.  -1.  -3. -11.   6.\n  -2.  10.  -4.  11.  -9.   4.   5.  13.   6. -18.  14.  -6.   7.  -2.\n   9.   1.  -6.   4.   3.  -1.   3.  -6.   2.   3. -12.  -8.  -8. -10.\n  -5.  -2.  13. -11.  -2.   1.   4. -12.   2. -10.   1.   7. -11.   6.\n  -4.   5.   0.  -3.  -7. -13.  -5.   1.   1.  14.   4.   3.   1.  -7.\n  -4.  -5.  -6.  -4.   0.  -6.   2.  -4.  -4.  13.   9.  -6.   3.   3.\n  -3.  -2.   7. -10. -10.   1.  -9.   1.  -1.  10.  -1.  -2.   3.  -6.\n   4.   1.   3.  -8.   4.   0.  -6.   2. -20.  19.   1.  -1.  -5.   6.\n   7.   2.   9.  11.  -3.   7.  -2.  -6.  -6.  -1.   4.   5.   5.   6.\n  -5.  -3.  -2.   0.  -6.   3.   9. -15.  -8.  15.  -5.   6.  -5.   4.\n   0.  -3.   6.  10.   3. -12.  -6.   6.   1.   1.  -9.  -3.  -3.   1.\n   3.  -2.   0.  -3.  -4.   1.  -4.   7. -11.   4. -10.   1.   0.  -6.\n  -1.  17. -11.  12.   5.  11.   1.  -5.   6. -13.   9.  -7.  -8.   5.\n  -2.   3.  -3.   1.   4.  -9.   5.   4.  -2. -13.  -2.  -6.  14.  -5.\n  -8.   1. -21.  -3.  -2.  -2.   2.  -5.   1.  -1.   5.  -9.  -1. -13.\n   1.  -5.   5.  -2.   0.   0.   9.  -9.   8.   8.  -1.   4.  -1. -13.\n  -1.   3.  -9.   0.  -2. -10. -11.  -1.   2.   9.  -1.  -2.   2.  11.\n   4.  -9.  10.  18.  -6.   2.  -3.  -2.  -6.  -4.  10.   1.   3.   0.\n   4.   2.   0.   2.  -9.  -1.   2.  -8.  -3. -12.   7.  -1.   1.   7.\n   5.   2.  -2.  -2.   2.  -4.  -4.   0.  -2.  -1.  -5.   1.  11.  10.\n   3.  -6.  -4.   3.  -3.  -4.   1.  -3.   0.  -5.  -4.  -4.  -6.  -9.\n  -5.  11.  -1.   7.   0.   7.   3.  -1.  -2.  -4.  -3.   1.  -3.   0.\n   5.   1.   5. -12.  -8.  12.   1.   3.   1.   2.  11.  -2.  10.   1.\n  -6.  -1.  16.  10.  -8.  10.  -3.  -4.  -2.  13.  -6.  -4.   1. -10.\n   2.   2.  -2.   3.  -9.   8.   0.   4.  -2.  -1.  -4.   5.   0.   8.\n  -7.   1. -11.   0.   2.   5.   1.   0.  -8.  -7.   1.  -6.   6. -15.\n  -1.  -1. -15.  12.  18.  -5.  -6.   3.  13. -12.   0.  13.   2.   7.\n  -8.  -4. -20.  -4.  -7.  -5.  17.  -8.   0.  -1.   2.  -3.  -5.  13.\n   0.  -5.  -3.  -1.   6.   2.   2.  -6.  -3.   5. -12.   7.   5.   9.\n   6.  -1.   3.   1. -12.  -8.  -8.  -1.  -2.  -6.  -4.  -5.  -3.   6.\n  -1.   7.  14.   2.  -2.   8.  -4.   7.   0.   1.  -6.   7.  -5.   3.\n -15.   7.   2.   2. -12. -11.   0.  -1.   7.   0.  -5.   7.  -3.  -2.\n   8.   3.   1.  -6.   2.   4.  17.   9.  -3.  11.  -6.  -1.  -3.  11.\n  -5.  -9.  -3.  -4.  -2.  -4.   9.   9.   1.  -3.   1.  -4. -10.  -3.\n -12.  -1.  -8.  -3.   6. -11.   3.  -3.  10.  -1.   2.   2.   0.  -9.\n  -6.   6.  -4. -10.  -1.   5.  -6.  -6.  -6.  -4. -10.   8.  -4.   6.\n   1.  -7.   0.  26.   6.  -7.  -3.   1.   0.  -3.   6.   1.  11.   7.\n  -2.   5.  -9.  -6.  13.  -2.   3.   3.   2.   0.  -2.   7.  14.   3.\n  11.  -1.  -1.  -4.  -6.  -1.   4.  -5.  -5.   1.   1.  -1.  10.   4.\n   4.   0.  -4. -10.   3.   2.  -4.  -2.   0.  17.   3. -15.   4.   6.\n   7.  -6.  -2.   2.  -5. -14.  -6.   1.   9.   2. -18.   6. -14.  -7.\n  11.  11.  -1. -13.  -1.  -4.   1.   7.  -4.   1.  -4.  -1.   4.   2.\n -11.  12.  -7.   5.  -3.  -6.  -6.   7.  -4.   1.  -3.  -7.  10.   8.\n   0.   4.  -5.   6.  -2.  10.  -8.  -1.   2.   2.  -2.  -7.  -1.   2.\n   5.   7.   3.  10.   4.  -3.  -7.   0.  16.  -6.  -6.  -4.  -2.  -6.\n   3.   9.  -8.  -8.  -3.  -4.   0.  -3.   0.   2.  -2.   6.  -4.  -3.\n   1.   7.  -2.   4.  -2.   0.  -6.   8.  -1.   2.  -1.  12.  14.  -6.\n  -1.  14.   7.  -5.   4.   3.  -3.   3.  -3.  -9.   4. -11.   5.  -3.\n   3.   0.  -7.  -4.  14.  -7.  -3.   3.  -3.   7.  -4. -11.  -3.   2.\n  -3.  -8.   2.  -1.  -9.   6.  -1.  -4.   3.  -7.  -5.  17.   1.  -2.\n  15.  -1.  -3.  -2. -10.  -4.   1.   9.   3.  -3.   5.   4.  -4.   0.\n  -2.   9.   2.  -1.   9. -13.  -2.   3.  -4.  -4.  -1.  -6.   2.   4.\n  20.  -1. -11.   8.   1.   6. -10.   5.  10.   3.  -1.   5.   1.   8.\n  -4.   1.  13.  -3.   6.  -3.   1.   9.  -4.   7.   0.  -5.  -6.  -3.\n   8.   9.]\"\t1859.0\t1843.0(-0.01)\t1843.0(-0.01)\ng10.mc\t47.94\t0.06\t-0.008343763037129746\t1.0\t-1.0\t0.9999651902033291\t\"[  6.  -3.  -7.  -8.  12.  -3.  -1.   6.   2.   3.   1.  -9.   1.   7.\n -11.   2.   4.   6. -10.   5.  -3.   7.   7.   5.   5.   7.  -9.   4.\n   3.  -2.  -5.  -1.  10.  15.  -6.  -7.  -1.   1.  10.   3.   4. -10.\n  -1.  -2.   3.  -5.   9.   1.   4.  -1.  11.   4.  -3.   4.  -9.  -2.\n  -7.   0.   3.   5.  -6.  -4.   3.   7.  -2.  -7.   1.  -5.   3.   4.\n   9.  -1.  -2.  -1.  -4.  -4. -10.   8.   1.  -6.  -1.   0.   2. -13.\n  11.   8.  -2.   2. -11. -14.   0.   5.  -5.  -8.  -3.  -2.  10.  -1.\n   2.   1.  -4.  -7.  -8.  -5.  -5.   7.   5. -12.  12.   4.   1.  -8.\n   2.  -3.  -3.   5.  13.   2.   0.  13.   4. -14. -10.  -9.   0.   0.\n -12.  -6.  -9.  -9.  10.  -3.  -2. -16.  -5.   2.   8.   9.   4.  -5.\n   5.  16.   1. -11.   7.   0.  10.  -1.  16.   5.   6.   2.  -6.  -7.\n   6.  10.   2.   1. -11.   6.  -3.   6.   3.   5.  -2.   0. -12.  -3.\n -12.  -6.   5.   3.  -6.  -9.  -5.  -8.  -4.   2.   2.   4.  -2.   6.\n -16.  -1.   8.  -5.   2.  -4.   0.  -6.   5.  13. -11.   2.  -4.   3.\n  -2.  14.  -7.  -4.   3.   4.   7.   1.   2.  12.  -9.   7.  -4.  -5.\n  -9.   3. -12.   3.   0.   3.  -3.   1.  -7.  -5.   1.   4.  -5.   3.\n   2.  10.   3.   2.  -7.   7.  10.   2.  -3.   2.  -1.   2.  -5. -10.\n  -2.   4.  10.  18.   0.  -4.   4.   0.   0.   2.  -3.  -5.  -6.   0.\n  -1.   5.  16.  13.  -5. -13.   3.  -7.  -6.  -2.  -1. -10.  -5.  -8.\n  -8.  -3.  -1.  -7.   5.  -1.  -2.   2.   8.   3.   1.   1.  11.   4.\n  -4.   3.  11.  10.  17.   9.   6.   2.  -2.   3.   2.  -5.   4.  -2.\n  -5.  -3.   5.  -5.   3.  -2.  -3.  -2. -12.   5.  -1. -20.  -2.  -7.\n -19.  -6.  -3.  -8.  -8.  -1.   4.  -2. -11. -11. -10.   5.  -3.   3.\n   5.   2.   0.  -6.   4.   6.   4.  -3.  12.  11.  -6.   1.   8.   5.\n   3.   8.  -9.  -9.   2.  -8.   4.  -1.  -7.  -2.   2.  -9.  -7.   3.\n  11.  10.  -7.  -2.  -6.   0.  10.  -3.   1.   3.   4.  16. -12.   5.\n -13.  -6. -10.  -6.  -1.  -5. -11.  -7.   0.  -1. -11.  11.  -1.   6.\n   7. -18. -14.  -4.   6.   3.  -1.  -2.   1. -10.  -5.  -9.  -4.  12.\n   2.   0.   0.  -2.  18.   1.  -8.   2.  16.  -3.  -3.  -5.  10.  -9.\n   2.   0.   1.  -5.   4.  -1.  -9.  -4.  -7.  10.  15.   7.  -2.  -9.\n   5.   2.  -6.  -5. -12.  12.   9.   3.  -4.  -2.   1.   2. -10.   6.\n  -7.   9.   0.  -1.   0.  -4.   5.  -9.   2.  -8.   3.   3.  -8.   7.\n  -4.  -1.  -4.   0.   0.   5.  -4.  11.   4.   2. -15.   1.  -9.  -1.\n  -5.  11.   5.   1.  -6.  18.   1.  -6.  -6.   7.  -8.  -3.   3.  -1.\n  -1. -11.   1.  -4.  -7.  -3.  -6. -10.   6.   5.  11.  -7.  11.   2.\n  -9. -12.   0.  11.   3.   5.  13.   4.   6.   4.  -5.  -7.   1.  -7.\n  10.   0. -16.  -6.  -4.   2.   7.  -4. -10.   0. -10.   4.   5.  -6.\n   5.  -3.   3.   4.  -1.  -5.  10.  -3.  -3.  -5.  16.   4.   0.  -1.\n   7.   5. -10.  -3.  -2.  -4.   7.   3.   4.   0.   2.   0.   2.   9.\n   0.   4.  -3.  -7.  -2.   3.   3.  -4.  -2.  -4.   6.  -1.  -4.  -1.\n   5.  13.  -9.  -2.  16.  -5.  -3.   3.   4. -22.   6. -10.  -9.  -7.\n   4.  -6.   0.   4.  -8.  -7.  -5.   3.   4. -12.  -3.  -8.  -6.   8.\n  -9.   0.   5.   2. -12.   3.  -3.   2.  -4.  -5.  -9.  -1.   2.  16.\n -17.  -2.  -2.  -2.   4.   2.  -7.  -8.   7. -11. -10. -14.   4.   3.\n   2.   7.   0. -10.  -3.  -1.   7.  18. -13.   2.  -2.  -6.  -4.   3.\n  -3.   2.   3.  -1. -10.  -3.  -2. -15.   5.  -8.  -3. -10.   1.  -7.\n   0.   3.   3.   5.  -6.   6.  -2.   1.   2.  -4.   8.   4.  -3.   3.\n  -1.  -8.  -1.  -5.  -1.   7.   6.  11.   2.   3.  -5.  -3.  -1.  -2.\n  -1.  -5.  -4.  -1.   4.  10.  -6.  -2.  -7.  15.   0.  -5.  -6.  -6.\n   9.   0.  -7.   4.   5. -10.  10.   8.   6.   3.  -1.   3.  -1.  12.\n  -5.   4. -10.  12.  -2.  -1.   1.  -3.   5.  -4.  -1.  -2. -18.  -1.\n  -6.  -8.   2.   1.   3.   8.   3.  -7.  -4.  -3.  -6.   7.   2.   3.\n   2. -15.   5.  -1.   1.   5.  11.   2.  -1.   1.   0.   8. -17.   8.\n   7.  -3.  -7.   0.   5.   0.  -6.  -7.  -5. -13.  -5.  11.  -8.   0.\n  -2.  -9.  -7.  -9.   2.   4.  13.   7.   6.  -4.  10. -11.  -2. -12.\n  -4.   1.   9. -18.   0.   9.   4. -11.  -6.  -5.  -9.   2.  -6.  -2.\n   2.  -8.  14.  -1.  -1.   9.  -3.  -5.   3.  -4.   0.   1.  10.   4.\n   4.  -5.]\"\t1714.0\t1808.0(0.05)\t1808.0(0.05)\ng2.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[42. 46. 46. 45. 59. 47. 50. 58. 48. 54. 44. 36. 49. 41. 42. 46. 49. 45.\n 50. 42. 45. 47. 53. 39. 38. 61. 54. 52. 55. 52. 52. 44. 57. 46. 53. 57.\n 39. 52. 48. 54. 45. 57. 38. 45. 36. 46. 41. 46. 58. 50. 43. 56. 47. 41.\n 51. 58. 43. 54. 40. 41. 40. 42. 49. 47. 50. 66. 44. 54. 61. 50. 61. 59.\n 48. 52. 52. 50. 44. 47. 49. 49. 50. 45. 43. 44. 52. 45. 51. 47. 45. 53.\n 42. 44. 50. 47. 50. 47. 57. 40. 53. 50. 64. 66. 49. 57. 46. 45. 47. 43.\n 48. 43. 52. 51. 45. 50. 56. 61. 49. 44. 47. 45. 39. 57. 49. 45. 60. 39.\n 48. 51. 45. 43. 48. 47. 48. 57. 45. 47. 52. 53. 48. 49. 38. 40. 54. 52.\n 45. 47. 48. 52. 42. 48. 49. 50. 49. 52. 52. 48. 49. 48. 53. 41. 48. 50.\n 42. 55. 54. 46. 42. 42. 50. 56. 45. 52. 45. 65. 54. 43. 44. 45. 55. 48.\n 49. 43. 51. 45. 37. 42. 45. 59. 46. 52. 45. 42. 34. 55. 52. 41. 52. 52.\n 40. 51. 53. 55. 51. 52. 52. 40. 43. 51. 42. 43. 49. 47. 45. 63. 50. 65.\n 28. 49. 47. 50. 35. 55. 39. 38. 38. 49. 48. 46. 60. 47. 51. 59. 56. 41.\n 55. 41. 53. 47. 38. 43. 44. 46. 46. 45. 31. 34. 52. 38. 44. 54. 48. 45.\n 45. 43. 51. 44. 51. 58. 41. 41. 50. 54. 41. 47. 40. 48. 61. 54. 40. 36.\n 36. 45. 40. 37. 56. 42. 58. 51. 49. 54. 41. 48. 43. 42. 56. 55. 53. 41.\n 62. 48. 52. 45. 51. 48. 58. 46. 44. 51. 40. 39. 51. 54. 58. 43. 46. 47.\n 48. 49. 53. 50. 52. 55. 58. 62. 47. 53. 51. 51. 45. 54. 38. 54. 43. 55.\n 47. 40. 43. 48. 55. 45. 47. 41. 61. 52. 43. 48. 45. 57. 48. 40. 47. 57.\n 44. 47. 46. 41. 43. 48. 57. 57. 41. 54. 56. 45. 58. 42. 51. 47. 48. 53.\n 63. 52. 41. 35. 44. 48. 56. 44. 65. 54. 46. 43. 50. 52. 36. 39. 41. 38.\n 47. 48. 47. 46. 43. 40. 46. 49. 51. 50. 46. 44. 54. 51. 36. 47. 48. 43.\n 43. 44. 44. 39. 43. 30. 49. 51. 48. 53. 43. 43. 43. 41. 51. 47. 56. 41.\n 57. 52. 54. 65. 56. 49. 50. 47. 41. 37. 43. 48. 42. 56. 36. 48. 46. 60.\n 41. 49. 42. 41. 54. 52. 55. 43. 46. 55. 53. 57. 53. 43. 55. 54. 42. 59.\n 29. 42. 58. 40. 52. 50. 60. 53. 43. 37. 43. 45. 50. 46. 36. 45. 49. 39.\n 46. 48. 44. 53. 36. 49. 53. 45. 45. 55. 50. 40. 36. 55. 36. 58. 48. 45.\n 46. 39. 50. 53. 46. 52. 44. 46. 65. 40. 61. 60. 48. 52. 51. 44. 50. 49.\n 41. 39. 43. 48. 49. 52. 42. 54. 46. 42. 53. 43. 57. 40. 48. 56. 49. 38.\n 59. 50. 54. 44. 47. 53. 42. 55. 55. 51. 39. 52. 54. 47. 52. 39. 57. 49.\n 48. 45. 36. 50. 44. 42. 42. 53. 54. 39. 45. 55. 52. 34. 57. 53. 52. 43.\n 62. 54. 48. 56. 50. 53. 40. 33. 43. 45. 47. 48. 39. 56. 39. 50. 52. 43.\n 52. 61. 54. 46. 45. 47. 49. 41. 53. 58. 39. 47. 40. 54. 45. 53. 51. 54.\n 44. 58. 54. 49. 51. 45. 59. 45. 49. 54. 48. 37. 58. 37. 49. 39. 53. 42.\n 46. 50. 50. 41. 53. 44. 40. 43. 38. 42. 40. 44. 59. 51. 56. 40. 45. 45.\n 58. 43. 48. 56. 49. 55. 44. 46. 50. 41. 47. 41. 46. 47. 47. 58. 49. 42.\n 49. 45. 49. 48. 43. 64. 52. 46. 47. 54. 49. 51. 43. 54. 47. 57. 45. 37.\n 50. 44. 41. 60. 49. 46. 41. 50. 41. 33. 48. 45. 46. 53. 50. 57. 56. 47.\n 59. 40. 48. 49. 42. 47. 36. 42. 46. 45. 42. 54. 57. 49. 44. 54. 54. 52.\n 50. 50. 39. 36. 55. 41. 48. 55. 46. 52. 37. 42. 58. 52. 47. 58. 55. 34.\n 42. 49. 46. 52. 39. 44. 48. 52. 56. 39. 41. 60. 49. 42. 44. 50. 47. 60.\n 45. 54. 34. 53. 40. 40. 52. 49. 54. 44. 47. 43. 45. 51. 33. 55. 47. 56.\n 44. 47. 51. 50. 48. 46. 54. 47. 37. 47. 49. 46. 45. 52. 38. 49. 56. 47.\n 44. 63. 40. 61. 46. 46. 51. 63. 62. 45. 45. 37. 50. 52. 48. 42. 51. 51.\n 51. 49. 45. 53. 35. 44. 33. 48.]\"\t11420.0\t11433.0(0.0)\t11433.0(0.0)\ng4.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[44. 57. 41. 54. 44. 38. 45. 47. 45. 44. 48. 43. 51. 61. 45. 57. 44. 42.\n 56. 41. 60. 58. 49. 44. 37. 47. 48. 36. 56. 49. 50. 61. 46. 56. 44. 53.\n 36. 46. 54. 47. 50. 46. 42. 44. 54. 45. 50. 65. 37. 49. 41. 39. 33. 51.\n 45. 52. 60. 40. 48. 53. 35. 44. 39. 53. 44. 52. 50. 48. 47. 44. 51. 55.\n 40. 44. 53. 51. 43. 46. 46. 61. 48. 48. 54. 46. 49. 50. 61. 51. 40. 45.\n 46. 42. 48. 40. 45. 43. 53. 48. 48. 53. 44. 53. 39. 47. 49. 43. 45. 44.\n 46. 45. 37. 51. 58. 47. 44. 42. 32. 50. 48. 42. 40. 65. 57. 62. 43. 53.\n 45. 52. 47. 56. 46. 47. 51. 51. 55. 46. 49. 48. 35. 50. 50. 61. 39. 54.\n 42. 44. 58. 48. 50. 53. 57. 63. 57. 42. 39. 46. 53. 59. 55. 39. 58. 46.\n 36. 33. 48. 51. 61. 48. 47. 53. 56. 46. 60. 53. 45. 51. 46. 55. 45. 40.\n 41. 60. 50. 31. 44. 56. 49. 46. 50. 58. 57. 49. 41. 39. 47. 51. 55. 48.\n 52. 51. 52. 39. 32. 57. 43. 34. 52. 49. 50. 46. 49. 43. 51. 54. 33. 45.\n 57. 57. 54. 47. 45. 45. 58. 59. 46. 41. 41. 53. 44. 59. 43. 42. 50. 47.\n 50. 42. 46. 59. 54. 57. 45. 45. 42. 44. 52. 57. 57. 59. 37. 57. 47. 45.\n 51. 37. 51. 38. 68. 40. 53. 51. 62. 46. 55. 46. 55. 35. 41. 53. 45. 46.\n 44. 50. 61. 41. 46. 55. 39. 48. 52. 45. 40. 41. 52. 56. 56. 48. 49. 46.\n 50. 50. 56. 45. 49. 38. 46. 50. 52. 42. 39. 45. 42. 58. 47. 54. 41. 37.\n 49. 43. 53. 38. 46. 52. 46. 46. 52. 46. 44. 43. 55. 51. 53. 46. 49. 50.\n 42. 45. 41. 48. 45. 55. 38. 51. 52. 44. 48. 29. 53. 59. 45. 45. 48. 57.\n 37. 51. 36. 40. 55. 37. 39. 54. 33. 57. 49. 48. 48. 54. 53. 43. 49. 34.\n 45. 50. 50. 51. 50. 53. 40. 48. 60. 46. 47. 44. 42. 51. 48. 46. 49. 52.\n 44. 54. 44. 49. 47. 56. 46. 52. 56. 63. 48. 53. 50. 50. 43. 49. 51. 60.\n 44. 49. 51. 50. 52. 51. 37. 48. 48. 57. 41. 59. 51. 50. 56. 45. 40. 43.\n 49. 52. 38. 51. 56. 35. 58. 52. 64. 64. 55. 61. 41. 40. 38. 53. 46. 55.\n 51. 41. 44. 43. 53. 39. 42. 48. 42. 46. 39. 51. 40. 47. 53. 51. 44. 39.\n 51. 47. 48. 48. 40. 55. 52. 46. 52. 55. 55. 46. 53. 59. 48. 38. 42. 54.\n 48. 51. 48. 47. 48. 55. 47. 39. 51. 49. 40. 52. 52. 59. 48. 41. 51. 42.\n 61. 47. 47. 50. 42. 47. 47. 44. 40. 40. 53. 45. 49. 41. 50. 45. 59. 57.\n 45. 41. 47. 54. 34. 62. 35. 51. 43. 53. 41. 48. 52. 51. 42. 45. 42. 55.\n 44. 55. 57. 33. 50. 43. 50. 48. 48. 53. 38. 54. 48. 48. 43. 51. 52. 52.\n 48. 42. 42. 62. 42. 42. 51. 59. 40. 42. 62. 55. 59. 45. 46. 45. 40. 45.\n 55. 49. 38. 57. 39. 54. 39. 60. 47. 53. 50. 56. 46. 41. 52. 37. 47. 45.\n 49. 50. 42. 45. 38. 35. 45. 57. 43. 41. 48. 46. 52. 56. 52. 52. 51. 52.\n 38. 48. 36. 49. 59. 57. 54. 46. 45. 40. 58. 54. 59. 48. 58. 43. 43. 62.\n 56. 44. 40. 49. 37. 49. 57. 55. 47. 54. 37. 47. 44. 49. 48. 53. 38. 46.\n 49. 42. 41. 51. 43. 50. 52. 45. 52. 37. 45. 35. 52. 46. 48. 52. 35. 42.\n 46. 42. 62. 41. 56. 46. 46. 55. 45. 42. 55. 39. 59. 54. 60. 41. 45. 46.\n 50. 44. 48. 58. 52. 48. 51. 57. 42. 44. 51. 48. 40. 49. 52. 46. 42. 46.\n 52. 55. 55. 43. 44. 42. 40. 44. 42. 56. 59. 42. 57. 50. 50. 54. 49. 38.\n 53. 47. 40. 41. 45. 51. 51. 47. 42. 35. 41. 43. 43. 38. 45. 36. 54. 53.\n 39. 39. 59. 51. 48. 47. 49. 44. 47. 56. 38. 47. 61. 48. 49. 44. 59. 43.\n 39. 41. 57. 52. 47. 43. 49. 38. 48. 48. 45. 51. 43. 45. 55. 50. 54. 42.\n 52. 49. 46. 39. 57. 55. 40. 49. 58. 52. 51. 44. 48. 38. 62. 43. 51. 40.\n 47. 40. 46. 51. 52. 41. 37. 39. 43. 40. 46. 35. 53. 51. 46. 45. 49. 39.\n 52. 53. 42. 51. 54. 75. 48. 47.]\"\t11384.0\t11377.0(-0.0)\t11377.0(-0.0)\ng7.mc\t47.94\t0.06\t-0.007822277847309137\t1.0\t-1.0\t0.9999694055166285\t\"[ -2.   6.   0.  -9.  -5.   5. -14.  -4.  -2.  -2.   2.  -6.   7.   3.\n   2.  -2.   7.   3.   0. -10.   5.  -3.  11.   7.  -6.  19.  -2.   0.\n -11.  -2.   4.  -6.  -5.   0.  -5. -13.  -7.   4. -12.   4.   9.  -7.\n  -4.   3.  -4.   2. -11.  -4. -16.  -6.   1.  -8.   5.   3. -15. -18.\n   5.  -2. -18.   5.   6.   4.   1.   1.  10.   4.  10. -12.   1.   2.\n  -3.  -7.  -4.   2. -10. -14.   4.  -9.  -3.   7.   6.   5.   3.   2.\n   2.   3.   3.  -7.   3.  -9.  12.   6.   0.  -1.  -2.   3.  -3.   4.\n   3.  -4.  -8.  -8. -13.  -7.  -4. -11.  -9.  -3. -12.  -7.  -6.   3.\n   7.   0.  12.   1.  -3.  12.   7.  -1.   7.  -1.   9.  -5.   4.  -5.\n   2.  15.   9.  -7.  16.  -1.   2.  -5.   9.  13.   2.   1.   4.  -9.\n  -2.   6. -10.  -6.  -5.  13.  20.   2.   2.   4.   1. -12.  -1.   8.\n  -8.  -4. -11.  -6.   3.   7.   8.   4.  -4.  -1.  -2.  -6.  14.   2.\n  -8.   0.   7.   2.  -7.  11.  -2.   5. -10.  -3. -17. -14.  -3.  -5.\n  -9.   9.  -7.  -8.  -7.  -1.  -4.  -8.  -1.  -2.  -2. -11. -16.   5.\n  -8. -12.   0.  -5.   1.  -3. -13.   0.   2. -14.   5.   3.   0.  -3.\n  -3.   1.  -1.  -9.  16.   3.  -2.  -1.   5.   0.   7.   1.  -1.   6.\n   2.  -9.   6.   0. -10.  -7.  -3.   7.   8.  -7.  11.   1.  -1.  -5.\n   2.  -5.  -2.  -6.   6.   1.  -1.   4.   0. -10.   8.  10.  -2.  -3.\n   9. -11.  -1.  -4.  -5.   4.   1.  -7.   2.  -2.  -5.  -1.  -8.  -6.\n  -1.   4.   8.   2.   0.   1.   2. -11.   6.   6.   6.   3.  -1.  14.\n  -5.   8.   3.  -6.   2.  -3.   9.   5.  -2. -10.   6.   3.   3. -12.\n   2.   0.   6.   7.   8.  19. -11.  -4. -10.  -7.   8.  -7.  -2.  11.\n  -3.  12.   6.   5. -10.   8.  -1.  -1.   7.  -3.   7.  -6.  -4.   2.\n  11.  -1.  -1.  -4.  -9.  -2.  -5.   7.  11.  -1.  -3.  -6.  -1.  -4.\n   3.  -7.  16.  -4. -11.   7.  14.  -9.  12.  -1.   3.  -6.  -5.  -9.\n -15.   0. -18.  13.   4.  10.  -5.  -3.   4.  11.  -3.  -2.   3.   1.\n   4.  12.  -6.   8.   5.  10.  -8.  -1.   6.  -8.   0.  -5.  -3.   2.\n   7.   2.  -1.   2.   7.  -2.   0.  -1. -15.  -6.  -2.   0.  10.  -7.\n   8.  -5.   0.  -1.   3.   4.  -2.   5.  13.   0.   9.   7. -10.   3.\n   1.   5.  -5.  -3.  -3.  -7.  -2.   5.  11.  10.  -4.   9.  -4. -17.\n   2.   3.   1.   1.  -3.   0.   0.  -4.   2.  -4.   0.   4.   1.  -3.\n  -6. -13.  -4.  -2.  -7.   3.   8.  13.   7.  -1. -13.  13.  -9.   2.\n   2. -11.  -1.   4.   2.   4.   0.   4.  -8.  -7.  -5.  -7.   9.   5.\n   6.  -8.   0.  15.  11.  -1.   4.   6.   2.   9.   0.  -3.   5. -11.\n   5.   1. -10.  10.   0.   7.   0.  -4.   2.  -7.  -4.   3.  10.   3.\n   6. -14. -12.   6.  -3.   2.  -1.   6.   4.  -4.   5.  -2.   0.  -7.\n   3.   3.   9.   8. -15.  -6.  -2.  10.   2.   0.  15. -13. -13.  -8.\n   4. -16.   5.   0.   1.  10.  10. -12.   1.   5.   2.   3.  -5.  -3.\n -13.   4.  -6.  -7.   0.   7.  -9.  -7.  -2.   1.   2.   2.   4.   2.\n   2.  -9.  14.  11.  -7.   9.   8.  -2. -11.  -7.  -4.   1.  -4.   0.\n  -4.   2.  -4.  -3.   8.  -3.  -5.   3.   3.  16.  -3.  10.  -1.   6.\n  12.  11.  -6.   5.  -6.  -6.   5. -11.  13.   1.  -9. -14.   3. -15.\n -12.  -8.   1.  -7.   7. -10. -12.  -8.   4.  -7. -15.  -1. -11.  -1.\n  -1.  14.   2.  -1. -14.   3. -11.  -3.  -3.  -6. -10.   4.   4.  -3.\n  -3. -10.  -2.   5.  -4.  -8.  12.   4.   3.  -5.  -2.  -4.  -5.  -9.\n  -6.   9.  -8.  -4.   7.  -7.   6.   6.   6.  -3. -11.   7.  -4.  -9.\n  -3.   8. -11.  14.   3.   5.   5.  -2.  -1.  12. -10. -14.   3.  14.\n   1.   1.   5.   8.  -3.  -9.   7.   7. -18. -14.  15.  -6.   1.  -4.\n  -5.  -2.   7.   9.  10. -11.  -2.   7.  -4.  -7.  -2.   3.  -7.   4.\n   8.  -7.  -6.  -3.   2. -10.  -2.   1.   2.  -4.   3.  -7.  -2. -10.\n  -2.  -4.  -4.  -4.   9.   6.  13.  -1.   4.  -7.   0.  12.   1.   0.\n  -4.   4.  -3.  -2.  -5.  -2.   0.  17.   6. -12.  -1.   0.  -6.   4.\n   0. -13.  -3.   6.  -3.   6.   6.  -6.   9. -12.  -9.   2.  -2.  -1.\n -12.   4.  -6.   3.   4.   0.  -7.  -1.  -1.   7.   5.  -7.  -7.   8.\n  14. -11.   3.  -2.  -6.  -4.  -4.   7.   7.   1.   5.   4. -11.  -8.\n   4.   7. -12.  -5.   4.   5. -10.  17.  -6.  -2.   1.  -9.  -2. -11.\n  -3.  -3.   4.  -2.  -2.   8.   5.  13.  -1.  -7.   5.   9.  -5.   2.\n   3.   0.]\"\t1806.0\t1818.0(0.01)\t1818.0(0.01)\ng8.mc\t47.94\t0.06\t-0.008865248226950355\t1.0\t-1.0\t0.9999607029148068\t\"[ -5.   7.  -3.   1.  -9.   4.   9.  -6.  -2.   0.   0.  11.   3.   2.\n  -2.  -5.  13.  -1.  -9.   3.   4.   1.   2.   3.   2.  -1.  -1.   9.\n   3.  -8.  -1.   8.  -4.   7.  -2.   5.  -1.   5.  -1.   0.  -1. -10.\n  -5. -11.  -4.  -4.  13.  15.   3.   4. -10.   1. -17.  -7.   4. -11.\n   6.   0.  12.  -4.  -4. -28.  -2.  -8.  -4.  12.  -2. -15.  -9.   1.\n  -7.   9.  -7.  -6.   6. -18.   5.  -2.  -3.   6.   3.   2.  -1.   3.\n   8.  12.  -2.  -5.  15.   4.  -7.  16.   1.   6.  -2.  -5. -11.  -5.\n -12.  -4.  -5.  -3.   0.   1.   6.  -1.   9.   6.  -8.   0.  -9.   6.\n   3.   4. -12.  -6.  -1.   8.  -1.   1.  -3.   3.   5.   3.   5.   3.\n  -5.   6.   2.   2.  17.  -4.  -6.   3.  -6.   4. -11.  -9.  -5.  -1.\n   2.  -1.  -8.   1.   7.   4.   2.   2.  -4.  -3.  10.  20.  -3.  -4.\n  -4.   1.   7.  11.   2. -12.  -2.   1.   6.  -1.   0.  -8.   9.  -2.\n   2.  -3.  13.  -8.   3.   4.   1.   7.   0.  -8.  -1.   8.   4.   1.\n  -1.   4.  -6.  10.   1.   9.   3.   0. -12.  -7.   0.   6.  -5.   0.\n   6.   5. -11.  10.  11. -13.   0.  -1.   9.   9.   1.   7.  -9.  10.\n   3.  -7.  -6.  -1.   2.  -1.  -5.   2.  -2.   3.   5.   2.   2.   2.\n -15.   1. -21.  -4.  -3.   4.   2.  -5.  -6.   1.  -6.   5.  -9.  -1.\n  -1.  -8.   3.  -2.  -2. -11.  -4.  -9. -12. -12.  -6.  -3.  -1. -13.\n  -4.   5.  -3.   1.   5.  -1.  -7.  -5.  -8.   8.  11.  13.  -9.   8.\n  -3.  -5.   4.  -2.   6.   3.   3.   1.  -9.  -8. -12.  -7.   7. -13.\n   0.   2.  11.  11.   2.  -2.  10.  -4.  -6.  15.   1.  21.  -7.   3.\n  -6.   0.  -6.   6.   8.  -5.  -5. -14.  -3. -11.  11.   9.   5.   4.\n   3.   8.  11.   1.   0.  -3.  -4. -13.   1.   2.  14.   4.  -5.   5.\n   0.   5.  -4.  -8.   9.  -5.   0.   2.  -3.   7.  15.   4.   4.   8.\n   4.  -3.   5. -11.   0.   8.   1.   4.   8.   3.  -5. -14.   0. -12.\n   4.   2.  -3.  -1.   2.  -1.  -3.  -2.  -5.   4.  -6.  11.   0.   4.\n   9.   1.   0.  -2.  -6.   3.  -3.  -3.   8.   8.  12.  -9.  -4. -15.\n  -7.   0.  -1.   7.   7.  -1.  -9.  -6.  -1.   6.  -2.  -4.   4.   1.\n  -1.  -5.  -3.   7.  -2.  -3.  -6.   0.   8.  -5.  -3.   9.  -5.   2.\n   5.  -1.  -2.  -8.  -3.  -3.  -8.  -2.  -7.  -7.  -5.  14.  -4.   6.\n  -1. -10.  11.   1. -11. -16.  -3.  12.   7.   9.  12.   0.  -8.   3.\n  -5.   4.  11.  13.  -6.  -9.   1.   3.  -9.   0.  -3.  -2.  -5.   2.\n  -5.  -2.   8.  -7. -15.   4.   6.  17.   0.  -4.   5. -14.  -2.  -2.\n  -1. -11.  -7. -10.   3. -15. -10.  10.  -4.   7.   2.   2.   1.   6.\n   3.  -1.   0. -12.   2.   1.   2.   3.   2.  -1.  -2.   9.   1.   3.\n   3.   1.   6.  -8. -18. -21.   6.  -6.  -3.  -8. -17.  -1.   4.  -6.\n  -4.  -5.   9.  -2.   2.   8.   9.  -6.   0.  -7.   9.   0.  -2.  -3.\n   1.   8.   2.  -8.   7.  -2.  -4.   3.   0.  -6.   9.   0.   5.   6.\n   5.  -6.   6.   5.   0.   0.  -1.  -6.   4.  -4.  -9.   0.  11.  14.\n   3.   2.  -7.   3.  -6.   6.  14.   0.   5.   4.  -3.  -6.  -1. -17.\n -13.   0.  -2.  -4.  -3. -10.   0.  -2. -17.  -7.  10.  -5.   4.  -3.\n -13. -10. -13.   4.  -3.  -9.   2.  -3.   3.   8.  11.   5. -14.   4.\n   0.   0.   1.   6.   1.  10.  -5.  -5.  -9. -10.   9.  10.   0. -14.\n   9.   7.   7.  -8.  -7.   1.   3.  -4.  -3.  -8.   2.   0.   2.   0.\n  -2.   2.   8. -11.   7.   5.   1.   9. -12.  -3.   0. -15.   5. -14.\n  -9.  -7.  12.  -7. -14.   6.   9.   7.  -2.  -4.  -4.   0.   0.  -4.\n  -8.  10.   2.  -1.   1.   8.   0.  -7. -18.  -7.   1.  -8.  -4.   1.\n  -2.  -4.   0.   2.   1.  -8.  -7.   7.   1.  -5.  -8.  -1.   1.  13.\n   2.   2.  -5.  -5.   2.   0. -11. -10.  -1.  -1. -12.  -8.   7.  -8.\n  -3.   5.  -1.  -5.   0.  -4. -10. -11.  -3.   2. -14.  -3.  -7.  -2.\n  -5.  -9.  10.   7.   4.  -6.  -9.  -3.  10.   2. -13.  -5.   0.  -8.\n  -6.  -4.   8.   6.   0.  -6.   4.  -2.   8.  -4.  -3.   8. -13.   2.\n   0.   4.   3.   3.   6.  16.  -3.  -6.   3.  -5.   9.   2.  10.   2.\n  -5.   0.  15.   2.   1.  -6.  -8.   0.   2.  10.   2.  -7.  -6.   0.\n   5.   4.  -3.   2.  -3.   3.   0.  -3. -10.   3.  -2.  -6.   3.  -8.\n   4.   1.  18.  -4.   7.   2.  -3.   0.   2.  -5.   2.  13.   6.  11.\n  -9.   6. -13.   1.  11.   1. -15.  -3.  -1. -12.   1.  -6.  -3.   1.\n  12.  -2.]\"\t1802.0\t1863.0(0.03)\t1863.0(0.03)\ng3.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[41. 41. 47. 51. 55. 54. 39. 42. 48. 50. 44. 41. 57. 42. 44. 57. 59. 55.\n 39. 39. 54. 41. 44. 49. 48. 37. 55. 45. 53. 54. 39. 46. 52. 51. 36. 43.\n 47. 53. 47. 64. 45. 62. 57. 51. 48. 48. 41. 51. 41. 54. 66. 49. 53. 45.\n 54. 41. 36. 40. 52. 58. 42. 46. 50. 44. 44. 42. 56. 45. 43. 59. 59. 51.\n 45. 56. 56. 50. 43. 48. 47. 46. 45. 42. 65. 61. 58. 46. 48. 59. 39. 60.\n 41. 46. 47. 36. 46. 45. 33. 55. 50. 40. 49. 43. 30. 59. 36. 45. 41. 48.\n 38. 54. 53. 42. 61. 44. 36. 40. 43. 44. 33. 39. 49. 51. 47. 37. 63. 49.\n 45. 50. 64. 42. 33. 42. 42. 45. 48. 50. 59. 53. 43. 47. 52. 43. 60. 47.\n 41. 42. 56. 50. 46. 51. 60. 54. 45. 36. 54. 41. 49. 31. 44. 50. 38. 61.\n 56. 41. 60. 42. 47. 50. 42. 45. 51. 50. 37. 42. 37. 41. 48. 52. 49. 60.\n 50. 53. 41. 54. 52. 48. 47. 51. 47. 44. 50. 55. 46. 48. 53. 42. 48. 43.\n 47. 48. 43. 59. 54. 35. 41. 49. 39. 53. 49. 46. 49. 47. 64. 57. 56. 51.\n 41. 44. 42. 55. 55. 50. 48. 52. 43. 33. 49. 40. 41. 46. 40. 45. 44. 45.\n 48. 49. 51. 49. 43. 42. 47. 40. 50. 55. 48. 47. 48. 58. 52. 51. 53. 37.\n 52. 45. 49. 43. 45. 45. 43. 39. 58. 56. 47. 69. 65. 46. 43. 45. 54. 48.\n 48. 49. 39. 55. 53. 52. 50. 49. 57. 53. 46. 48. 51. 55. 50. 46. 42. 52.\n 52. 51. 49. 49. 43. 43. 46. 46. 48. 44. 42. 51. 39. 50. 61. 53. 47. 41.\n 55. 48. 39. 42. 57. 55. 46. 59. 36. 47. 57. 46. 48. 58. 37. 45. 50. 51.\n 52. 52. 39. 47. 56. 36. 55. 57. 39. 48. 44. 54. 42. 45. 57. 45. 46. 48.\n 39. 46. 40. 45. 45. 46. 52. 42. 46. 46. 63. 45. 42. 37. 47. 46. 51. 52.\n 50. 41. 46. 46. 51. 43. 52. 42. 48. 47. 57. 59. 48. 56. 52. 43. 56. 57.\n 45. 56. 51. 49. 55. 39. 45. 44. 45. 50. 58. 52. 44. 41. 43. 47. 49. 53.\n 50. 49. 50. 40. 50. 51. 39. 65. 41. 46. 45. 49. 50. 50. 37. 53. 56. 46.\n 45. 53. 63. 36. 58. 54. 61. 44. 47. 41. 61. 46. 39. 34. 41. 57. 48. 42.\n 38. 49. 43. 44. 43. 37. 50. 63. 55. 63. 37. 52. 43. 44. 47. 42. 33. 46.\n 50. 55. 51. 54. 58. 61. 44. 46. 47. 48. 38. 58. 41. 55. 51. 42. 51. 55.\n 46. 52. 46. 45. 60. 48. 47. 42. 63. 47. 44. 46. 52. 43. 56. 49. 44. 47.\n 52. 55. 49. 33. 55. 45. 48. 38. 50. 49. 46. 48. 55. 40. 57. 45. 44. 44.\n 50. 51. 47. 42. 48. 42. 49. 48. 46. 45. 51. 44. 54. 51. 43. 50. 48. 32.\n 51. 42. 42. 43. 54. 54. 49. 60. 31. 44. 57. 46. 50. 39. 38. 50. 47. 56.\n 56. 44. 55. 42. 45. 56. 41. 56. 43. 43. 46. 52. 42. 44. 47. 46. 57. 50.\n 39. 51. 43. 46. 46. 54. 35. 52. 52. 36. 57. 45. 50. 49. 44. 47. 51. 60.\n 49. 40. 35. 45. 48. 53. 51. 40. 47. 53. 42. 62. 50. 52. 47. 56. 43. 48.\n 45. 69. 41. 46. 49. 40. 46. 46. 39. 51. 53. 66. 35. 49. 45. 36. 49. 52.\n 54. 36. 46. 54. 40. 56. 44. 55. 49. 51. 39. 43. 42. 43. 38. 53. 57. 60.\n 51. 51. 48. 43. 44. 52. 51. 45. 46. 50. 54. 54. 46. 46. 50. 52. 40. 49.\n 41. 50. 42. 57. 48. 37. 41. 40. 50. 49. 58. 52. 58. 52. 51. 54. 47. 65.\n 53. 47. 58. 49. 43. 47. 48. 52. 43. 53. 40. 50. 51. 54. 37. 51. 60. 40.\n 51. 52. 51. 41. 45. 59. 46. 46. 54. 43. 43. 46. 58. 61. 51. 42. 39. 39.\n 56. 53. 52. 62. 57. 41. 42. 54. 51. 53. 52. 62. 38. 52. 48. 40. 50. 42.\n 46. 60. 48. 50. 43. 50. 49. 48. 50. 40. 47. 41. 42. 54. 45. 48. 49. 57.\n 45. 42. 42. 46. 39. 48. 47. 54. 53. 38. 50. 42. 46. 50. 48. 47. 36. 44.\n 39. 44. 53. 46. 45. 47. 52. 45. 40. 39. 60. 48. 49. 52. 34. 61. 46. 48.\n 45. 38. 45. 46. 48. 57. 34. 45. 52. 51. 63. 50. 51. 47. 45. 53. 47. 45.\n 53. 52. 43. 42. 47. 39. 48. 50.]\"\t11421.0\t11405.0(-0.0)\t11405.0(-0.0)\ng5.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[40. 51. 49. 66. 50. 43. 55. 42. 46. 39. 43. 47. 47. 51. 49. 46. 36. 44.\n 42. 33. 49. 45. 41. 43. 55. 47. 51. 44. 53. 50. 49. 43. 42. 49. 64. 39.\n 41. 35. 52. 49. 52. 58. 57. 42. 53. 49. 61. 45. 44. 35. 51. 50. 45. 46.\n 65. 42. 39. 42. 45. 41. 48. 42. 51. 45. 54. 47. 47. 37. 43. 44. 47. 47.\n 38. 39. 34. 46. 54. 50. 47. 38. 63. 54. 48. 53. 45. 44. 34. 48. 57. 40.\n 48. 51. 55. 48. 51. 44. 70. 53. 56. 57. 46. 55. 40. 39. 45. 47. 47. 44.\n 38. 40. 49. 48. 48. 35. 57. 45. 43. 44. 56. 47. 48. 36. 58. 51. 58. 44.\n 36. 36. 41. 51. 46. 49. 42. 46. 41. 44. 42. 47. 54. 39. 51. 56. 51. 55.\n 63. 62. 52. 47. 48. 47. 54. 40. 52. 47. 46. 40. 50. 55. 45. 42. 51. 44.\n 59. 53. 36. 46. 54. 45. 58. 52. 53. 39. 38. 57. 51. 52. 44. 54. 42. 30.\n 50. 58. 46. 43. 50. 49. 38. 30. 42. 52. 47. 57. 47. 44. 42. 47. 50. 58.\n 53. 50. 51. 52. 61. 57. 52. 48. 43. 51. 44. 45. 57. 43. 52. 43. 52. 51.\n 59. 47. 49. 53. 55. 48. 51. 47. 56. 48. 59. 42. 41. 45. 40. 68. 43. 50.\n 49. 40. 49. 46. 44. 50. 46. 54. 60. 42. 52. 58. 56. 46. 55. 45. 50. 66.\n 51. 57. 48. 59. 51. 45. 51. 51. 38. 50. 65. 56. 51. 36. 48. 47. 45. 43.\n 51. 57. 48. 46. 50. 51. 55. 51. 51. 44. 50. 49. 51. 44. 61. 57. 46. 46.\n 48. 43. 52. 41. 48. 54. 45. 39. 43. 37. 49. 44. 45. 38. 44. 57. 45. 34.\n 44. 51. 57. 56. 47. 46. 38. 53. 52. 62. 37. 39. 46. 49. 51. 45. 49. 44.\n 32. 56. 54. 42. 44. 55. 38. 47. 56. 41. 34. 49. 45. 52. 51. 57. 50. 40.\n 52. 51. 49. 58. 44. 45. 47. 37. 45. 32. 65. 44. 58. 54. 46. 45. 55. 35.\n 40. 42. 56. 53. 45. 50. 40. 44. 49. 51. 43. 53. 42. 51. 55. 57. 49. 44.\n 49. 60. 46. 66. 52. 49. 47. 46. 41. 54. 61. 45. 44. 58. 42. 52. 42. 66.\n 60. 39. 46. 42. 54. 67. 51. 35. 42. 45. 46. 52. 47. 49. 48. 55. 41. 42.\n 49. 40. 41. 59. 48. 43. 65. 54. 54. 43. 46. 56. 55. 49. 42. 40. 53. 46.\n 50. 50. 43. 47. 54. 37. 38. 54. 47. 59. 42. 54. 61. 49. 44. 57. 50. 53.\n 42. 40. 56. 45. 50. 57. 36. 40. 43. 43. 41. 45. 39. 51. 55. 55. 50. 46.\n 57. 50. 46. 49. 38. 47. 57. 45. 53. 45. 45. 52. 41. 45. 50. 50. 56. 53.\n 57. 43. 49. 52. 67. 52. 50. 37. 43. 45. 45. 50. 46. 50. 39. 39. 49. 47.\n 40. 54. 46. 52. 40. 56. 45. 40. 46. 38. 58. 48. 55. 40. 61. 51. 47. 42.\n 53. 39. 56. 45. 31. 45. 46. 42. 58. 47. 51. 43. 46. 55. 42. 50. 57. 43.\n 46. 46. 46. 42. 46. 55. 54. 48. 47. 51. 38. 57. 45. 46. 38. 54. 48. 49.\n 40. 51. 63. 45. 45. 46. 40. 41. 53. 53. 56. 50. 48. 46. 45. 53. 48. 54.\n 58. 38. 48. 47. 49. 39. 48. 54. 51. 60. 52. 48. 51. 36. 43. 52. 36. 47.\n 51. 50. 42. 45. 41. 51. 50. 40. 51. 54. 52. 46. 48. 34. 39. 50. 45. 47.\n 54. 54. 50. 49. 48. 41. 54. 56. 47. 43. 51. 58. 51. 46. 52. 40. 52. 53.\n 41. 48. 51. 41. 50. 45. 50. 41. 41. 46. 35. 42. 51. 57. 40. 53. 59. 47.\n 42. 40. 50. 45. 38. 42. 56. 60. 47. 53. 43. 46. 43. 39. 43. 45. 50. 53.\n 42. 55. 43. 51. 45. 50. 49. 51. 46. 35. 44. 48. 48. 50. 59. 53. 58. 59.\n 56. 50. 63. 58. 47. 54. 45. 46. 42. 44. 54. 49. 47. 45. 45. 56. 47. 54.\n 48. 46. 42. 49. 51. 49. 45. 42. 35. 50. 50. 43. 38. 42. 46. 51. 55. 50.\n 51. 37. 52. 47. 44. 47. 42. 49. 34. 45. 37. 41. 37. 47. 33. 54. 41. 47.\n 42. 50. 41. 52. 49. 47. 35. 56. 39. 58. 46. 37. 47. 49. 49. 43. 48. 40.\n 42. 57. 57. 41. 40. 52. 51. 57. 44. 32. 44. 49. 50. 50. 50. 57. 45. 48.\n 48. 49. 56. 49. 44. 41. 43. 48. 56. 50. 46. 52. 50. 51. 51. 45. 53. 37.\n 51. 52. 48. 57. 50. 50. 60. 57.]\"\t11420.0\t11531.0(0.01)\t11531.0(0.01)\ng1.mc\t47.94\t0.06\t1.0\t1.0\t1.0\t0.0\t\"[47. 51. 50. 51. 39. 49. 50. 48. 33. 46. 47. 40. 47. 53. 55. 64. 63. 52.\n 42. 49. 53. 53. 59. 39. 50. 53. 45. 49. 53. 45. 37. 54. 53. 51. 47. 49.\n 52. 45. 45. 48. 50. 46. 50. 48. 49. 34. 45. 42. 37. 43. 44. 53. 49. 39.\n 39. 52. 42. 50. 43. 50. 40. 48. 50. 45. 43. 54. 46. 51. 58. 55. 52. 46.\n 57. 46. 39. 46. 48. 54. 34. 45. 49. 51. 45. 43. 52. 53. 49. 55. 58. 48.\n 47. 40. 42. 44. 39. 45. 58. 43. 52. 45. 42. 50. 65. 39. 39. 48. 38. 38.\n 49. 49. 54. 61. 49. 47. 50. 59. 52. 54. 46. 58. 44. 41. 47. 53. 43. 53.\n 47. 49. 41. 42. 46. 57. 48. 43. 47. 42. 47. 45. 42. 49. 46. 55. 46. 62.\n 56. 45. 47. 45. 44. 55. 46. 42. 41. 59. 48. 40. 61. 53. 48. 46. 45. 46.\n 52. 54. 50. 42. 48. 58. 49. 46. 46. 46. 49. 37. 47. 54. 40. 57. 52. 43.\n 41. 45. 40. 52. 47. 51. 44. 59. 45. 50. 48. 50. 53. 45. 35. 59. 44. 65.\n 52. 52. 49. 41. 47. 45. 46. 50. 40. 42. 48. 47. 50. 46. 51. 36. 44. 56.\n 56. 55. 33. 36. 58. 43. 43. 41. 54. 48. 53. 64. 56. 44. 49. 51. 50. 56.\n 59. 45. 33. 40. 44. 48. 46. 39. 39. 59. 44. 62. 39. 46. 62. 45. 60. 53.\n 55. 48. 47. 45. 46. 60. 49. 55. 41. 55. 59. 43. 53. 50. 52. 45. 45. 55.\n 58. 37. 57. 45. 48. 47. 42. 48. 55. 42. 47. 58. 56. 49. 47. 50. 52. 45.\n 48. 46. 55. 42. 44. 55. 41. 41. 51. 40. 44. 45. 42. 62. 52. 50. 50. 44.\n 49. 56. 48. 46. 36. 43. 54. 42. 60. 44. 40. 46. 37. 61. 49. 53. 50. 46.\n 61. 44. 52. 27. 40. 49. 50. 53. 51. 54. 46. 43. 49. 47. 38. 48. 51. 39.\n 37. 53. 47. 49. 54. 53. 55. 44. 44. 50. 49. 45. 51. 42. 57. 47. 41. 42.\n 56. 56. 44. 44. 47. 56. 50. 61. 58. 44. 47. 35. 45. 37. 39. 51. 43. 43.\n 55. 46. 53. 48. 52. 51. 37. 54. 52. 45. 51. 45. 37. 44. 59. 39. 47. 48.\n 58. 42. 47. 47. 51. 48. 47. 40. 37. 49. 48. 49. 43. 41. 45. 39. 47. 57.\n 51. 34. 59. 40. 45. 38. 61. 50. 57. 48. 43. 50. 43. 48. 45. 51. 56. 34.\n 47. 57. 34. 45. 47. 67. 53. 37. 52. 43. 49. 49. 51. 62. 56. 59. 32. 49.\n 50. 43. 52. 54. 54. 48. 55. 53. 54. 46. 43. 54. 51. 42. 45. 46. 43. 52.\n 44. 47. 41. 52. 47. 38. 46. 43. 51. 42. 51. 47. 46. 47. 44. 37. 44. 47.\n 43. 48. 37. 50. 44. 45. 51. 45. 50. 47. 52. 57. 49. 52. 45. 40. 51. 46.\n 55. 37. 51. 41. 47. 50. 60. 49. 47. 44. 45. 52. 55. 53. 45. 40. 43. 39.\n 53. 50. 49. 53. 47. 39. 53. 54. 37. 48. 50. 47. 41. 41. 50. 46. 50. 49.\n 44. 56. 51. 44. 45. 50. 41. 52. 43. 40. 58. 56. 43. 41. 47. 47. 36. 40.\n 41. 62. 42. 46. 43. 40. 45. 47. 41. 48. 52. 55. 55. 45. 53. 49. 56. 51.\n 52. 53. 51. 35. 55. 44. 59. 55. 46. 36. 55. 49. 42. 51. 62. 52. 48. 53.\n 29. 52. 44. 44. 51. 52. 41. 55. 44. 40. 52. 48. 49. 53. 49. 47. 46. 55.\n 56. 46. 50. 51. 59. 49. 58. 44. 43. 45. 52. 52. 48. 52. 42. 50. 48. 44.\n 49. 46. 46. 44. 35. 47. 50. 66. 50. 40. 52. 56. 56. 42. 47. 47. 46. 58.\n 55. 53. 46. 52. 57. 47. 59. 53. 39. 42. 53. 41. 47. 47. 42. 50. 54. 34.\n 47. 48. 38. 51. 49. 47. 39. 37. 55. 58. 44. 49. 50. 42. 45. 49. 43. 58.\n 48. 48. 40. 53. 46. 51. 48. 33. 54. 48. 55. 44. 44. 48. 44. 43. 38. 50.\n 51. 45. 49. 50. 53. 52. 58. 62. 43. 49. 50. 51. 50. 52. 46. 49. 50. 44.\n 61. 52. 38. 46. 46. 39. 44. 40. 59. 58. 45. 49. 51. 50. 38. 43. 44. 49.\n 63. 44. 38. 47. 59. 55. 61. 40. 44. 45. 40. 39. 51. 38. 41. 49. 47. 46.\n 45. 33. 41. 52. 53. 49. 45. 60. 50. 44. 49. 55. 57. 55. 43. 50. 49. 44.\n 45. 50. 46. 51. 55. 46. 57. 50. 43. 45. 55. 40. 44. 54. 46. 47. 42. 48.\n 49. 54. 48. 42. 43. 52. 49. 39.]\"\t11443.0\t11394.0(-0.0)\t11394.0(-0.0)\ng6.mc\t47.94\t0.06\t0.00803087192323738\t1.0\t-1.0\t0.9999677520281105\t\"[  3.  -5. -10.  -3.  -7.   9.   4.   4.  -5.   8.  -3.  -4.  15.  -1.\n  -7.  -6.   3.  22.   2.   9.  -5.  -5. -19.  -3.   8.   7.   5.   7.\n  -1.   5.  15.   2.   1. -15.  -3.  -7.  -2.   3.   9.   0.  -8.  -6.\n -10.  -2.   3.   4.  -1.   6.  -5.   5.   2.  -1.  -5.   1.   7.  -2.\n   6.  -4.  -1.  -6.  -6.  -8.   0.   3.   9.  10.  -6.   7.  12.   5.\n  -6.  -4.  11.   2. -17.   2.   4.  -4.  -2.   7.  -7.  -3.   9.   3.\n   8.  -1.  -3.   3.   2.   4.   7.  -6.  -4.   2.   3.   5. -12.   7.\n  10.   3.  -8.   8.  -7.  -5.  -1.  -2.   0.   2.  -7.   5. -10.   1.\n   7.   5.  12.  -7.  -2.   4.   4.   0.   4.  -5.   1.   3.  -1.  -5.\n   7.  11.  -7.   8.   4.  -1.  -2.  -3.   1. -14.   9.  17.   4.   3.\n   4.   3.  14.   2. -16.  -5.   1.   9.  10.  -9.  -2.   2.  -3.  11.\n -10.   0.   3.   9.  -4.  -6.   7.   6.   4.   0. -12.   0.  -6. -14.\n  -3.   8.   0.   0.  -5.   5.  -7.   8.  -4. -15. -12.   3. -15.  11.\n -10.  -8.  -1.   7.   4.  -3.  13.   4.  -4.   0.  -5.   5.   3.  -9.\n   6.  -1.   0.   6.  -5.  -3.   7.   3.   4.  12.   4.   4.  -8. -17.\n  -2.   2.   9.   0.   2.   2.   4.  -3.   3.  -4. -10.   5.  11.   5.\n -12.   0.  29.   4.  -2.   0.   1.  -5.  -4.  24.  -9.   1.  -3.   0.\n  -2.  -6.   0.   5.  -9.  -9.   8.  -2.   1.   4.  10.   7.  -2.  -5.\n   9.  -2.   3.   3.   8. -10.   3.   5.   5. -11.  -1.  -7.  13.  -2.\n  16.  -5.   5.  -7.  16.   3.   5.  -9. -14.  -5.  -6.   2.   9.  -8.\n   9.   6. -10.  -3.  -3.  -8.   0.  -9.  -8.   0.  -1.   0.   8.  -9.\n   3. -17.  -9.   0.   0.   7.   0. -10.  -2.  12.   4.   8.   7.  -2.\n   6.   0.   8.   5.  10.  -6.   4.   4.   8.  -4.  -7.  11. -11.   5.\n  -6.   8.  -3.   6.   6.   7.  -4.  -5.  -4.   7.  -1.  -2.  -2.   3.\n -19. -15.   2. -10.  -7.  -9.  -7.   1.  -5.  -1.   0.   7.  -3.   4.\n   4. -12.   9.   5.   7.   2.   9.  -3.   1.  -6.   6.   4.   0.  -6.\n  11.   2.   2.  11.   8.  -8.  -9.  -5.   9.   3.  -5.   7.  -3.   1.\n  -1.   4.  -1.  16.   8.   1.  -5.  10. -14. -13.   1.   9.   3.  -8.\n  -1.  -3.  -1. -10.   0.   4.  -7.   5. -15.   4.  11.  -4.   3.   9.\n  -6.  -9.  -3.   7.   5. -17.   3.   3.   1.  10.   1.   0.  -3.   0.\n   9.  -6.  -7.   6.   5.   8.  -1.   2.  11.   3.   4.   0.  -5.  -1.\n  -2. -15.   5. -15.   3.  -1.   8.  -1.  -1.  -1.  -3.   0.  14.   3.\n  -8. -17.  14.   3.   2.  22.  -8.  16.   1.  -1.   6.   4.   9.   0.\n   5.   6.  -3.   2.   7.   2.   4.  11.   1.   0.   3.   0. -10.  -5.\n  -7.   0.  -7.  -1.  -2.   5.   2.   5.   0.  11.   7.  16.   3.   4.\n -16.  -1.  -5.  -5.  -8.  -3.  12. -21.   1.   2.   5.  -4.   1.  -8.\n  -5.  -1.  -7.   1.  -7.  12.  10.   5.   7.   0.  -1.   2.  -3. -13.\n   3.  -2.  11.   5.   5.  -2.  -7.  -1.  -3. -11. -15.  -6.  -1.   6.\n   8.   5.   7.  -5. -10.  -6.   8.   1.   6.   8.   1.   6.  -1.  -2.\n  -1.  -2.   5.  -4. -12.  -6.  -3.  -5.   1.   3.  -8.   2.  -1.  12.\n  -4.   0.   5.  -4.   1. -13.  11.   2.  -2.  11.   3.   3.  -7.  -1.\n  -8.  -1.   4.   3.  -5.   3.   5.  -4.  -3.  -1.   2.   2.  -3.   5.\n  10.  -3.   4.  10.   2.  -3.   1.   4. -10.   4.  -7.   0.   3.  -3.\n  10.   6.   8.   2. -19. -11.   9.  -3.   4.   3.   4.   6.  -4.  -1.\n   7.   1.   2.  -4.  -5.  11.  -8.  10.   6.   0. -12.   4.  14.  -6.\n  -1.  16.   2.  -4.   5.   3.  -8.  -2. -10.   0. -12. -12.  14.  -6.\n   1.  -5.   4.  -4.  -3.   7.  -2.   8.   3.   3.  -3.   3.  -3. -16.\n  -9.   7.  -5.  -1.  12.   2. -10. -14. -21.  -6.   2.  -1. -11.   5.\n  -3.   3.  -5.  10.   4.   5.  12.   6.  -3.  -1.  -5.  -6.  -2.   4.\n   4.  11.   4.   3.   6.   5. -10.  -6.   5.   6.   4.   6.  -2.  -7.\n   6.  14.  17.   7.   1.  -8.  -5.   0.  -6.  -4.   1. -11.  -8.  -9.\n  -6.   0.  -8.   5.   2.  -6.  -9.  14.  -4.  12.  10.  -1.  -6.   2.\n  -7.  10.  -5.  -3.  11.  16.   0.   9.  -8.   5.   3.   0.  -4.   1.\n  -3. -11.   7.   6.  -4.  -3.   0.   5.  15.  -4.  -1.  -7.   5.  -2.\n   7.   9.  -9.  14.  -9.   5.  -3.   2.   6.   8.   5.   1.   9.   5.\n  -5.  14.  11.   0. -13.  14. -10.   5. -15.   4.  -5.   2.  -3.  -1.\n -13.   0.  12.  -8.  -2.   7.  14.   0.   5.  -2.   4.  -8.   1.   2.\n  -5.  -3.]\"\t1927.0\t1974.0(0.02)\t1974.0(0.02)\n\nIt is evident that in some scenarios, the new heuristic performs well, although there is still room for improvement. Based on the results, please analyze the following:\n1. In which cases does the new heuristic perform well, and in which cases is it not satisfactory?\n2. What factors contribute to these differences in performance?\n3. Is the updated heuristic most_weight_neighbors_641f better than previous heuristic most_weight_neighbors_2e6e and is the previous evolution suggestion  Introduce a threshold-based approach for the dynamic scaling factor. When the standard deviation of edge weights is below a certain threshold, revert to a fixed scaling factor to prevent unnecessary adjustments in uniform cases. This can enhance stability and performance in less variable instances. works?\n4. How can we further refine the new heuristics?\n\nWe believe that further refinement of the new heuristic is necessary.\n\nPlease analyze the results and provide one executable code adjustment suggestion (in word, not code).\n\nTo make sure the code adjustment suggestion is executable, here are some aspects to consider and you can provide more:\n1. This may involve changing default values or adopting dynamic values tailored to specific data.\n2. Implement checks to retain the original algorithm when the data does not favor heuristic updates.\n3. This includes aspects such as operation type, subject, location, and timing.\n4. Should we continue to deepen origin suggestion or weaken this suggestion:  Introduce a threshold-based approach for the dynamic scaling factor. When the standard deviation of edge weights is below a certain threshold, revert to a fixed scaling factor to prevent unnecessary adjustments in uniform cases. This can enhance stability and performance in less variable instances..\n5. Explore insights and strategies from other successful heuristics.\n\nThe response format is very important. For better communication, please respond to me in this format:\n1. If you believe the code still need to be updated, please respond to me in this format:\n***refinement: \nresult analysis: xxx\ncode adjustment suggestion: xxx\n***\n2. If you think current heuristic are good enough, please respond to me in this format:\n***refinement: \nresult analysis: None\ncode adjustment suggestion: None\n***\nEnsure there is no other content inside the ***, and analysis outside *** is welcome.\nIf you have no information to provide, simply respond with ***None***."
            }
        ]
    },
    {
        "role": "assistant",
        "content": [
            {
                "type": "text",
                "text": "***refinement: \nresult analysis: The new heuristic most_weight_neighbors_641f performs well in instances with high edge weight variability (e.g., g6.mc, g8.mc), where dynamic adjustments are beneficial. However, it is less satisfactory in more uniform cases (e.g., g9.mc) where the threshold-based approach does not significantly alter results compared to the fixed scaling factor. The performance differences are influenced by the effectiveness of the scaling factor in capturing and leveraging variability in edge weights.\ncode adjustment suggestion: Further refine the dynamic scaling factor by introducing an adaptive threshold that varies based on the average edge weight and density. This approach can better capture the unique characteristics of each graph instance, ensuring more precise scaling factor adjustments.\n***"
            }
        ]
    }
]