{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_cases/all_days_negativePrices_highInitSoC_1for5/config1_5EVs_1chargers.json\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MultiAgentRoadCharging' object has no attribute 'obs_dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 277\u001b[0m\n\u001b[1;32m    275\u001b[0m env \u001b[38;5;241m=\u001b[39m MultiAgentRoadCharging(data_file)\n\u001b[1;32m    276\u001b[0m n_agents \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mn\n\u001b[0;32m--> 277\u001b[0m trained_agents, rewards_history \u001b[38;5;241m=\u001b[39m \u001b[43mmaddpg_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_agents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 199\u001b[0m, in \u001b[0;36mmaddpg_train\u001b[0;34m(env, n_agents, n_episodes, max_steps, m, gamma, tau, actor_lr, critic_lr, batch_size, buffer_capacity, print_interval, device)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmaddpg_train\u001b[39m(env, \n\u001b[1;32m    188\u001b[0m                  n_agents, \n\u001b[1;32m    189\u001b[0m                  n_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m                  print_interval\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    197\u001b[0m                  device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 199\u001b[0m     obs_dim \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobs_dim\u001b[49m\n\u001b[1;32m    200\u001b[0m     state_dim \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstate_dim\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;66;03m# 初始化agents\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MultiAgentRoadCharging' object has no attribute 'obs_dim'"
     ]
    }
   ],
   "source": [
    "# =========================================\n",
    "# file: maddpg_with_arbitration.py\n",
    "# =========================================\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import copy\n",
    "from env_marl import MultiAgentRoadCharging\n",
    "import os\n",
    "\n",
    "# ============ 1) Replay Buffer =============\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, actions, actions_arbitrated, rewards, next_state, done):\n",
    "        self.buffer.append((state, actions, actions_arbitrated, rewards, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, actions, actions_arbitrated, rewards, next_state, done = zip(*batch)\n",
    "        return (np.array(state), \n",
    "                np.array(actions), \n",
    "                np.array(actions_arbitrated),\n",
    "                np.array(rewards), \n",
    "                np.array(next_state), \n",
    "                np.array(done))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "# ============ 2) Networks (Actor/Critic) =============\n",
    "class ActorNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_dim=64):\n",
    "        super(ActorNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.sigmoid(self.fc3(x))  # 输出[0,1]\n",
    "        return x\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, n_agents, hidden_dim=64):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.input_dim = state_dim + n_agents  # 拼接全局状态 + n_agents动作\n",
    "        self.fc1 = nn.Linear(self.input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "    def forward(self, state, actions):\n",
    "        x = torch.cat([state, actions], dim=-1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# ============ 3) MADDPG Agent =============\n",
    "class MADDPGAgent:\n",
    "    def __init__(self, \n",
    "                 actor_lr, critic_lr, \n",
    "                 obs_dim, state_dim, \n",
    "                 n_agents, agent_index,\n",
    "                 gamma=0.95, tau=0.01, hidden_dim=64,\n",
    "                 device='cpu'):\n",
    "        self.agent_index = agent_index\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.n_agents = n_agents\n",
    "        self.device = device\n",
    "        \n",
    "        # Actor\n",
    "        self.actor = ActorNetwork(obs_dim, hidden_dim).to(device)\n",
    "        self.target_actor = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        \n",
    "        # Critic\n",
    "        self.critic = CriticNetwork(state_dim, n_agents, hidden_dim).to(device)\n",
    "        self.target_critic = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "    \n",
    "    def select_action(self, obs, exploration=False):\n",
    "        obs_t = torch.FloatTensor(obs).unsqueeze(0).to(self.device)\n",
    "        self.actor.eval()\n",
    "        with torch.no_grad():\n",
    "            p = self.actor(obs_t).item()\n",
    "        self.actor.train()\n",
    "        \n",
    "        if exploration:\n",
    "            # 简易随机：eps\n",
    "            eps = 0.05\n",
    "            if np.random.rand() < eps:\n",
    "                return np.random.randint(0,2)\n",
    "        # 伯努力采样\n",
    "        action = 1 if np.random.rand() < p else 0\n",
    "        return action\n",
    "    \n",
    "    def soft_update(self, net, target_net):\n",
    "        for param, target_param in zip(net.parameters(), target_net.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)\n",
    "    \n",
    "    def update_targets(self):\n",
    "        self.soft_update(self.actor, self.target_actor)\n",
    "        self.soft_update(self.critic, self.target_critic)\n",
    "\n",
    "# ============ 4) 仲裁函数 =============\n",
    "def arbitrate_actions(actions, m, mode='random'):\n",
    "    a_tilde = actions[:]\n",
    "    total_requests = sum(a_tilde)\n",
    "    if total_requests <= m:\n",
    "        return a_tilde\n",
    "    \n",
    "    idx_ones = [i for i, val in enumerate(a_tilde) if val == 1]\n",
    "    if mode == 'random':\n",
    "        chosen = np.random.choice(idx_ones, m, replace=False)\n",
    "    else:\n",
    "        # 可做优先级排序\n",
    "        chosen = idx_ones[:m]  # 演示\n",
    "    \n",
    "    for i in idx_ones:\n",
    "        a_tilde[i] = 0\n",
    "    for c in chosen:\n",
    "        a_tilde[c] = 1\n",
    "    return a_tilde\n",
    "\n",
    "# ============ 5) 训练每个agent的函数 ============\n",
    "def train_maddpg_agent(agent_i, agents, replay_buffer, batch_size):\n",
    "    agent = agents[agent_i]\n",
    "    device = agent.device\n",
    "    \n",
    "    # 采样\n",
    "    state_b, actions_b, actions_arbi_b, rewards_b, next_state_b, done_b = replay_buffer.sample(batch_size)\n",
    "    # 转tensor\n",
    "    state_t = torch.FloatTensor(state_b).to(device)\n",
    "    actions_arbi_t = torch.FloatTensor(actions_arbi_b).to(device)\n",
    "    rewards_t = torch.FloatTensor(rewards_b[:, agent_i]).unsqueeze(-1).to(device)\n",
    "    next_state_t = torch.FloatTensor(next_state_b).to(device)\n",
    "    done_t = torch.FloatTensor(done_b).unsqueeze(-1).to(device)\n",
    "    \n",
    "    # Critic Update\n",
    "    with torch.no_grad():\n",
    "        # 下个时刻: 让所有agent用target_actor 输出动作(连续p?),拼合 => Critic\n",
    "        next_actions_list = []\n",
    "        for idx, ag in enumerate(agents):\n",
    "            # 这里假设 local obs == next_state，若不同需拆分\n",
    "            p_next = ag.target_actor(next_state_t).squeeze(-1)\n",
    "            next_actions_list.append(p_next)\n",
    "        next_actions_t = torch.stack(next_actions_list, dim=1)\n",
    "        \n",
    "        # 计算 target Q\n",
    "        Q_next = agent.target_critic(next_state_t, next_actions_t)\n",
    "        y = rewards_t + agent.gamma * (1 - done_t) * Q_next\n",
    "    \n",
    "    Q_now = agent.critic(state_t, actions_arbi_t)\n",
    "    critic_loss = F.mse_loss(Q_now, y)\n",
    "    \n",
    "    agent.critic_optimizer.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    agent.critic_optimizer.step()\n",
    "    \n",
    "    # Actor Update\n",
    "    # 让agent_i 的actor输出新的动作(continuous p), 其他agent也用当前actor(或不更新)\n",
    "    cur_actions_list = []\n",
    "    for idx, ag in enumerate(agents):\n",
    "        if idx == agent_i:\n",
    "            p_cur = ag.actor(state_t).squeeze(-1)  # requires grad\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                p_cur = ag.actor(state_t).squeeze(-1)\n",
    "        cur_actions_list.append(p_cur)\n",
    "    cur_actions_t = torch.stack(cur_actions_list, dim=1)\n",
    "    \n",
    "    actor_loss = -agent.critic(state_t, cur_actions_t).mean()\n",
    "    agent.actor_optimizer.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    agent.actor_optimizer.step()\n",
    "\n",
    "# ============ 6) 主训练循环 ============\n",
    "def maddpg_train(env, \n",
    "                 n_agents, \n",
    "                 n_episodes=500,\n",
    "                 max_steps=1000,\n",
    "                 m=2,\n",
    "                 gamma=0.95, tau=0.01,\n",
    "                 actor_lr=1e-3, critic_lr=1e-3,\n",
    "                 batch_size=64,\n",
    "                 buffer_capacity=100000,\n",
    "                 print_interval=10,\n",
    "                 device='cpu'):\n",
    "    \n",
    "    obs_dim = env.obs_dim\n",
    "    state_dim = env.state_dim\n",
    "    # 初始化agents\n",
    "    agents = []\n",
    "    for i in range(n_agents):\n",
    "        agent_i = MADDPGAgent(actor_lr, critic_lr, \n",
    "                              obs_dim, state_dim, \n",
    "                              n_agents, i, \n",
    "                              gamma=gamma, tau=tau, \n",
    "                              device=device)\n",
    "        agents.append(agent_i)\n",
    "    \n",
    "    replay_buffer = ReplayBuffer(buffer_capacity)\n",
    "    \n",
    "    all_rewards = []\n",
    "    for ep in range(n_episodes):\n",
    "        state, obs_list = env.reset()  # 需要你自己封装\n",
    "        ep_reward = np.zeros(n_agents)\n",
    "        \n",
    "        for t in range(max_steps):\n",
    "            # 每个agent选择动作\n",
    "            actions = []\n",
    "            for i in range(n_agents):\n",
    "                a_i = agents[i].select_action(obs_list[i], exploration=True)\n",
    "                actions.append(a_i)\n",
    "            # 仲裁\n",
    "            actions_arbi = arbitrate_actions(actions, m, 'random')\n",
    "            \n",
    "            # 交互\n",
    "            next_state, next_obs_list, reward_list, done, info = env.step(actions_arbi)\n",
    "            \n",
    "            # 存储\n",
    "            replay_buffer.push(state, actions, actions_arbi, reward_list, next_state, done)\n",
    "            ep_reward += reward_list\n",
    "            \n",
    "            # 更新\n",
    "            if len(replay_buffer) > batch_size:\n",
    "                for i in range(n_agents):\n",
    "                    train_maddpg_agent(i, agents, replay_buffer, batch_size)\n",
    "            \n",
    "            state = next_state\n",
    "            obs_list = next_obs_list\n",
    "            \n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        all_rewards.append(ep_reward)\n",
    "        \n",
    "        # soft update\n",
    "        for ag in agents:\n",
    "            ag.update_targets()\n",
    "        \n",
    "        if (ep+1) % print_interval == 0:\n",
    "            avg_r = np.mean(all_rewards[-print_interval:], axis=0)\n",
    "            print(f\"Episode {ep+1}/{n_episodes}, avg reward per agent = {avg_r}\")\n",
    "    \n",
    "    return agents, all_rewards\n",
    "\n",
    "# ============ 用法示例 ============\n",
    "if __name__ == \"__main__\":\n",
    "    n_EVs = 5\n",
    "    n_chargers = 1\n",
    "    avg_return = 0\n",
    "    SoC_data_type = \"high\"\n",
    "    data_folder = \"test_cases\"\n",
    "    results_folder = \"results\"\n",
    "    policy_name = \"base_policy\"\n",
    "    instance_count = 20\n",
    "    instance_num = 1\n",
    "    \n",
    "    \n",
    "    test_case = f\"all_days_negativePrices_{SoC_data_type}InitSoC_{n_chargers}for{n_EVs}\"\n",
    "    test_cases_dir = os.path.join(data_folder, test_case)  \n",
    "    data_file = os.path.join(test_cases_dir, f\"config{instance_num}_{n_EVs}EVs_{n_chargers}chargers.json\")\n",
    "    print(data_file)\n",
    "    # env = ConstrainAction(data_file)\n",
    "    env = MultiAgentRoadCharging(data_file)\n",
    "    n_agents = env.n\n",
    "    trained_agents, rewards_history = maddpg_train(env, n_agents, n_episodes=50, m=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
