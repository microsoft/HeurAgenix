\subsection{Problem Statement}
We aim to schedule a fleet of for-hire EVs to charge during periods of surplus solar power. The goal is twofold: first, to reduce electricity costs for the fleet operator, and second, to flatten the net load profile by shifting charging to times of surplus solar generation. This can prevent the scenario where all EVs charge at night, when no solar power is available, thus reducing strain on the power grid.

\subsection{MDP Model}
We consider a for-hire EV fleet of size $N$, with a total number of $M$ chargers available for charging in the region ($N\gg M$), and a scheduling horizon of $T$.
The charging management problem for this fleet can be modeled as a discrete-time MDP with finite time steps \( t = 1, 2, \cdots, T\).
Each EV's state at time $t$ is $\mathbf{s}^i_t$ and we denote the aggregate state as $\bar{\mathbf{s}}_t=\{\mathbf{s}_t^i\}_{i=1}^N$.
At each time $t$, the dispatcher observes the system state $\bar{\mathbf{s}}_t$ and takes an aggregate action $\bar{\mathbf{a}}_t=\{a_t^i\}_{i=1}^N$ for all agents, deciding whether each agent should begin charging or continue charging. Since this is a centralized decision-making process, we use max-min fairness to ensure no EV earns too little in cumulative earnings under the aggregate policy.
After the action is taken, the system provides a reward $r_t$ and transitions to the next state $\bar{\mathbf{s}}_{t+1}$. 

\begin{itemize}
    \item[1)] \textit{State space}: The state of each agent (EV) at time \( t \) is represented by the tuple:
    \begin{align*}
        \mathbf{s}_t^i = (\alpha_t^i, \beta_t^i, \theta_t^i) \in \mathbb{Z}_{\geq 0} \times \{0,1\} \times \{0,1,2,\dots,100\}.
    \end{align*}
    The variable \( \alpha_t^i \) denotes the remaining trip length if vehicle $i$ has been assigned an order, and \( \alpha_t^i=0 \) if no order is assigned. The variable \( \beta_t^i \) denotes the charging status. Note that a ride trip can span multiple time steps, while a charging session lasts for one time step. The pair $(\alpha_t^i, \beta_t^i)$ indicates the operational status of vehicle $i$:
    \begin{align*}
        \begin{array}{cc}
          \alpha_t^i=0, \beta_t^i=0:   &  \text{Vehicle $i$ is idle}\\
          \alpha_t^i > 0, \beta_t^i=0:   &  \text{Vehicle $i$ is on a ride}\\
          \alpha_t^i=0, \beta_t^i>0:   &  \text{Vehicle $i$ is charging}
        \end{array}.
    \end{align*}
    The variable \( \theta_t^i \) represents the battery state of charge (SoC) as a percentage (e.g., \( \theta_t^i = 10 \) represents a 10\% SoC). 

\item[2)] \textit{Action space}: The action for each EV is \( a_t^i \in \{0,1\} \), where \( a_t^i = 1 \) indicates that EV \( i \) is scheduled to charge, and \( a_t^i = 0 \) means that EV \( i \) is unplugged or remains idle.

    \item[3)] \textit{State evolution}: 
    We assume that if a vehicle remains idle, i.e., takes action \( a_t^i = 0 \) in the state \( \alpha_t^i = 0, \beta_t^i = 0 \), then there is a probability \( \rho \) that it will be assigned an order. If assigned, the vehicle starts a trip with random length \( \tau_{\theta_t^i} \) and earns a payment of \( w_t \cdot \tau_{\theta_t^i} \). Here, \( \tau_{\theta_t^i} \) is a random positive integer drawn from a discrete distribution that depends on the vehicleâ€™s current battery SoC \( \theta_t^i \), i.e., \( \tau_{\theta_t^i} \sim f(\theta_t^i) \), and \( w_t \) represents the estimated earnings per unit of ride time for vehicle \( i \) at time \( t \) of the day.
    
    If a vehicle is charging at time $t$, it can return to an idle status in the next time step by taking the action $a_t^i=0$ to unplug the vehicle.
    
    Specifically, each EV $i$ with state $(\alpha_t^i, \beta_t^i, \theta_t^i)$ is transitioned to the following states under  action $a_t^i=0$:
    \begin{align*}
    \alpha_{t+1}^i, \beta_{t+1}^i, \theta_{t}^i~|~a_t^i=0 = \left\{
    \begin{array}{cc}
    \tau_{\theta_t^i}, \beta_{t}^i, \theta_{t}^i-\delta_i^{-}/C_i      & \text{w.p.~} \rho \text{~if~} \alpha_{t}^i=0, \beta_{t}^i=0 \\
    \alpha_{t}^i, \beta_{t}^i, \theta_{t}^i-\delta_i^{-}/C_i      & \text{w.p.~} 1-\rho \text{~if~} \alpha_{t}^i=0, \beta_{t}^i=0 \\
    \max(\alpha_{t}^i-1, 0), \beta_{t}^i, \theta_{t}^i-\delta_i^{-}/C_i      & \text{~if~} \alpha_{t}^i>0, \beta_{t}^i=0 \\
    \alpha_{t}^i, \beta_{t}^i-1, \theta_{t}^i-\delta_i^{-}/C_i      & \text{~if~} \alpha_{t}^i=0, \beta_{t}^i=1 
    \end{array}\right. .
    \end{align*}
    Under action $a_t^i=1$, the state of each agent transitions to:
    \begin{align*}
    \alpha_{t+1}^i, \beta_{t+1}^i, \theta_{t}^i~|~a_t^i=1 = \left\{
    \begin{array}{cc}
    \alpha_{t}^i, \beta_{t}^i+1, \theta_{t}^i+\delta_i^{+}/C_i      & \text{~if~} \alpha_{t}^i=0, \beta_{t}^i=0 \\
    \alpha_{t}^i, \beta_{t}^i, \theta_{t}^i+\delta_i^{+}/C_i      & \text{~if~} \alpha_{t}^i=0, \beta_{t}^i=1 
    \end{array}\right.
    \end{align*}
    where $\delta_i^{+}$ and $\delta_i^{-}$ are the charging and discharging rates of EV $i$, respectively.
    Note that if EV \( i \) is on a ride, the action space is restricted to \( \mathcal{A} = \{ 0 \} \), meaning the only available action is to remain idle (i.e., not charge).

\item[4)] \textit{Reward}: At time $t$, the reward received by each EV $i$ with action $a_t^i$ in state $\mathbf{s}_t^i=(\alpha_t^i, \beta_t^i, \theta_t^i)$ is:
    \begin{align*}
        r_t^i = w_t\cdot \tau_{\theta_t^i}\cdot (1-a_t^i)\mathbb{1}(\alpha_t^i=0)(1-\beta_t^i) - h_t\cdot a_t^i\cdot(1-\beta_t^i)-p_t\cdot \delta_t^{+}\cdot a_t^i
    \end{align*}
    where $\mathbb{1}(\cdot)$is the indicator function, $h_t$ is the base fare for plugging in to charge, and $p_t$ is per kWh charging fare.
    The first term represents the reward EV $i$ receives if it remains idle, the second term represents the cost for transitioning from being idle to charging, and the last term represents the charging cost. 

\item[5)] \textit{Objective function}: Given an initial system state $\bar{\mathbf{s}}_0$ and a policy $\pi$ that generates a sequence of actions $\bar{\mathbf{a}}_t$, $t=0,1,\cdots, T$, we aim to identify a policy that maximizes the expected accumulated rewards starting from $\bar{\mathbf{s}}_0$:
    \begin{align}\label{obj}
        \max_{\pi} ~\mathbb{E}\left[ \sum_{t=0}^{T-1}\sum_{i=1}^N r_i^t~|~\bar{\mathbf{s}}_0\right] + r_T
    \end{align}
    where $r_T$ is the minimum cumulative reward over all EVs:
    \begin{align*}
        r_T = \min_i \mathbb{E}\left[\sum_{t=0}^{T-1} r_i^t\right].
    \end{align*}
    By finding a policy that maximizes the objective in \eqref{obj}, we ensure both the maximal profits for the EV fleet and fairness among all EVs.

\item[6)] \textit{Global constraints}: The total number of EVs that are either charging or requesting to charge should not exceed the charging station capacity:
    \begin{align*}
        \sum_{i=1}^N \beta_t^i + \sum_{i=1}^N a_t^i \leq M,
    \end{align*}
    where the first term represents the EVs that are charging, and the second term represents the EVs that are requesting to charge.

\end{itemize}